{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Adaptive Retrieval for Enhanced RAG Systems\n",
    "\n",
    "In this notebook, I implement an Adaptive Retrieval system that dynamically selects the most appropriate retrieval strategy based on the type of query. This approach significantly enhances our RAG system's ability to provide accurate and relevant responses across a diverse range of questions.\n",
    "\n",
    "Different questions demand different retrieval strategies. Our system:\n",
    "\n",
    "1. Classifies the query type (Factual, Analytical, Opinion, or Contextual)\n",
    "2. Selects the appropriate retrieval strategy\n",
    "3. Executes specialized retrieval techniques\n",
    "4. Generates a tailored response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PA 211 Disaster Community Resources.pdf ---\n",
      "PA 211 Community Disaster and Human \n",
      "Services Resources in Pennsylvania \n",
      "Introduction \n",
      " \n",
      "Community Disaster and Human Services Resources in Pennsylvania \n",
      " \n",
      "Disasters, whether natural or man-made, have significant and far-reaching impacts on \n",
      "individuals, families, and communities. Pennsylvania, with its mix of urban, suburban, and \n",
      "rural regions, faces a diverse array of emergencies ranging from floods and severe storms to \n",
      "public health crises and housing instability. To ensure an effective res\n",
      "\n",
      "--- 211 RESPONDS TO URGENT NEEDS.pdf ---\n",
      "211 RESPONDS TO URGENT NEEDS \n",
      "FACT\n",
      "211 stood up a statewide text\n",
      "response to support employees\n",
      "impacted by the partial federal\n",
      "government shutdown who did\n",
      "not know when they would\n",
      "receive their next paycheck.\n",
      "211 assists in times of\n",
      "disaster and widespread\n",
      "need\n",
      "FACT\n",
      "FACT\n",
      "1\n",
      "PLEASE VOTE TO INCLUDE FUNDING FOR PENNSYLVANIA'S 211 SYSTEM IN THE STATE BUDGET TO\n",
      "SUPPORT 211'S CAPACITY TO HELP OUR COMMUNITIES IN TIMES OF DISASTER OR GREAT NEED.\n",
      "1 2-1-1 Data\n",
      "In January 2019, 187 individuals\n",
      "subscribed to\n",
      "\n",
      "--- PEMA.pdf ---\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page\n",
      "\n",
      "--- ready-gov_disaster-preparedness-guide-for-older-adults.pdf ---\n",
      "1\n",
      "TAKE  \n",
      "CONTROL IN\n",
      "1\n",
      "2\n",
      "3\n",
      "Disaster Preparedness Guide for Older Adults\n",
      "\n",
      " \n",
      " \n",
      "STEP 1 | ASSESS YOUR NEEDS\n",
      "First, know your risk. Then, understand your needs during emergencies. \n",
      "This section guides you through a self-assessment process to identify your \n",
      "specific needs so that you can create a personalized emergency plan.\n",
      "STEP 2 | MAKE A PLAN\n",
      "Develop a comprehensive emergency plan and emergency \n",
      "preparedness kit tailored to your unique needs. This section ensures \n",
      "you are well prepared to respond to\n",
      "\n",
      "--- Substantial Damages Toolkit.pdf ---\n",
      " \n",
      " \n",
      " \n",
      "Prepared for: \n",
      "Pennsylvania Emergency Management \n",
      "Agency \n",
      "Emergency Management, Mitigation, \n",
      "Insurance, and Resilient Communities \n",
      "(MIRC) Office \n",
      "1310 Elmerton Avenue  \n",
      "Harrisburg, Pennsylvania 17110 \n",
      "Prepared by: \n",
      "PG Environmental and ERG \n",
      "14555 Avion Parkway, Suite 125 \n",
      "Chantilly, VA 20151 \n",
      " \n",
      "Substantial Improvements /  \n",
      "Substantial Damages Toolkit \n",
      "APRIL 2023 \n",
      "\n",
      " \n",
      "ii \n",
      " \n",
      "Contents \n",
      "Introduction ................................................................................................\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_texts_from_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a folder (recursively).\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDFs.\n",
    "    Returns:\n",
    "        dict: {pdf_filename: extracted_text, ...}\n",
    "    \"\"\"\n",
    "    pdf_texts = {}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    pdf_texts[pdf_path] = extract_text_from_pdf(pdf_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {pdf_path}: {e}\")\n",
    "    return pdf_texts\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs\"\n",
    "pdf_texts = extract_texts_from_folder(folder_path)\n",
    "\n",
    "for pdf_file, text in pdf_texts.items():\n",
    "    print(f\"\\n--- {os.path.basename(pdf_file)} ---\")\n",
    "    print(text[:500])  # Print the first 500 characters to verify extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "We'll create a basic vector store to manage document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PA 211 Disaster Community Resources.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/211 RESPONDS TO URGENT NEEDS.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PEMA.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/ready-gov_disaster-preparedness-guide-for-older-adults.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/Substantial Damages Toolkit.pdf\n",
      "Vector store populated with Gemini embeddings from all PDFs.\n",
      "\n",
      "Searching for items similar to: 'how to make a plan for my family'\n",
      "\n",
      "Top 3 search results:\n",
      "  - Text: fore an emergency happens, sit down together and decide how you will get in  contact with each other, what mobility and/ or medication issues will nee...\n",
      "    Similarity: 0.6842\n",
      "    Metadata: {'source': 'PEMA.pdf', 'chunk_id': 65}\n",
      "  - Text:  your family and friends have a plan in case of an emergency. Fill  out these cards and give one to each of them to make sure they know who to call an...\n",
      "    Similarity: 0.6811\n",
      "    Metadata: {'source': 'PEMA.pdf', 'chunk_id': 67}\n",
      "  - Text: r needs, it is time to make a plan and  build a kit. Sometimes disasters strike with little to no warning, so it is  important to have a plan and be p...\n",
      "    Similarity: 0.6771\n",
      "    Metadata: {'source': 'ready-gov_disaster-preparedness-guide-for-older-adults.pdf', 'chunk_id': 5}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz  # pip install PyMuPDF\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []   # List to store embedding vectors\n",
    "        self.texts = []     # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "            norm_query = np.linalg.norm(query_vector)\n",
    "            norm_vector = np.linalg.norm(vector)\n",
    "            if norm_query == 0 or norm_vector == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (norm_query * norm_vector)\n",
    "            similarities.append((i, similarity))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        return results\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def create_gemini_embedding(text, model=\"models/embedding-001\"):\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return response['embedding']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load API key\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini API configuration: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Create the vector store\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # Folder containing PDFs\n",
    "    folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs/\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing PDF: {pdf_path}\")\n",
    "                try:\n",
    "                    text = extract_text_from_pdf(pdf_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {pdf_path}: {e}\")\n",
    "                    continue\n",
    "                chunks = chunk_text(text, chunk_size=1000, overlap=200)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if not chunk.strip():\n",
    "                        continue\n",
    "                    try:\n",
    "                        embedding = create_gemini_embedding(chunk)\n",
    "                        store.add_item(chunk, embedding, metadata={\"source\": file, \"chunk_id\": i})\n",
    "                    except Exception as e:\n",
    "                        print(f\"Embedding failed for {file} chunk {i}: {e}\")\n",
    "\n",
    "    print(\"Vector store populated with Gemini embeddings from all PDFs.\")\n",
    "\n",
    "    # Sample similarity search\n",
    "    query_text = \"how to make a plan for my family\"\n",
    "    print(f\"\\nSearching for items similar to: '{query_text}'\")\n",
    "    query_embedding = create_gemini_embedding(query_text)\n",
    "    search_results = store.similarity_search(query_embedding, k=3)\n",
    "\n",
    "    print(\"\\nTop 3 search results:\")\n",
    "    for result in search_results:\n",
    "        print(f\"  - Text: {result['text'][:150].replace('\\n',' ')}...\")\n",
    "        print(f\"    Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"    Metadata: {result['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called and 'client' is not used.\n",
    "\n",
    "def create_embeddings(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Defaults to \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    # Gemini's embed_content can handle both a single string or a list of strings\n",
    "    # in the 'content' parameter.\n",
    "    response = genai.embed_content(\n",
    "        model=model,\n",
    "        content=text\n",
    "    )\n",
    "\n",
    "    # If the original input was a single string, return just the first embedding vector.\n",
    "    if isinstance(text, str):\n",
    "        return response['embedding']\n",
    "\n",
    "    # Otherwise, return all embedding vectors as a list of lists.\n",
    "    return response['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_folder(folder_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process all PDFs in a folder for use with adaptive retrieval.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDF files.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "        chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: All document chunks and combined vector store.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"\\nExtracting text from PDF: {pdf_path}\")\n",
    "                extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "                if not extracted_text:\n",
    "                    print(f\"Failed to extract text from {pdf_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                print(\"Chunking text...\")\n",
    "                chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "                print(f\"Created {len(chunks)} text chunks for {file}\")\n",
    "\n",
    "                print(\"Creating embeddings for chunks...\")\n",
    "                chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "                if len(chunks) != len(chunk_embeddings):\n",
    "                    print(f\"Error: Mismatch between number of chunks and embeddings for {file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "                    store.add_item(\n",
    "                        text=chunk,\n",
    "                        embedding=embedding,\n",
    "                        metadata={\"index\": i, \"source\": file}\n",
    "                    )\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Added {len(chunks)} chunks from {file} to the vector store.\")\n",
    "\n",
    "    print(f\"\\nAll done! Processed {len(all_chunks)} chunks from all PDFs.\")\n",
    "    return all_chunks, store\n",
    "\n",
    "# Example usage:\n",
    "# folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs/\"\n",
    "# all_chunks, store = process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def classify_query(query, model=\"gemini-pro\"):\n",
    "    \"\"\"\n",
    "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        model (str): LLM model to use. Defaults to \"gemini-2.0-flash\".\n",
    "        \n",
    "    Returns:\n",
    "        str: Query category\n",
    "    \"\"\"\n",
    "    # Define the prompt to guide the AI's classification\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at classifying questions.\n",
    "    Classify the given query into exactly one of these categories:\n",
    "    - Factual: Queries seeking specific, verifiable information.\n",
    "    - Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "    - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "    - Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "    Return ONLY the category name, without any explanation or additional text.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Category:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate the classification response from the AI model\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0, # Low temperature for deterministic output\n",
    "                max_output_tokens=20 # Limit output to ensure it's just the category name\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and strip the category from the response\n",
    "        category = response.text.strip()\n",
    "    \n",
    "        # Define the list of valid categories\n",
    "        valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "        \n",
    "        # Ensure the returned category is a valid, single word\n",
    "        for valid in valid_categories:\n",
    "            if valid.lower() in category.lower():\n",
    "                return valid\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during query classification: {e}\")\n",
    "        # Default to \"Factual\" if classification fails\n",
    "        return \"Factual\"\n",
    "    \n",
    "    # Default to \"Factual\" if classification is not one of the valid categories\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Specialized Retrieval Strategies\n",
    "### 1. Factual Strategy - Focus on Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def call_gemini(prompt, model=\"gemini-2.0-flash\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for factual queries focusing on precision.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Use LLM to enhance the query for better precision\n",
    "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
    "    Your task is to reformulate the given factual query to make it more precise and\n",
    "    specific for information retrieval. Focus on key entities and their relationships.\n",
    "    Provide ONLY the enhanced query without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # Generate the enhanced query using the LLM\n",
    "    enhanced_query_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    enhanced_query = call_gemini(enhanced_query_prompt, model=\"gemini-2.0-flash\", temperature=0)\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # Create embeddings for the enhanced query\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # Perform initial similarity search to retrieve documents\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Initialize a list to store ranked results\n",
    "    ranked_results = []\n",
    "    \n",
    "    # Score and rank documents by relevance using LLM\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # Sort the results by relevance score in descending order\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return ranked_results[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analytical Strategy - Comprehensive Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "\n",
    "    # Define the prompt to guide the AI in generating sub-questions\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at breaking down complex questions.\n",
    "    Generate sub-questions that explore different aspects of the main analytical query.\n",
    "    These sub-questions should cover the breadth of the topic and help retrieve\n",
    "    comprehensive information.\n",
    "\n",
    "    Return a list of exactly 3 sub-questions, one per line.\n",
    "\n",
    "    Main query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    # Generate the sub-questions using the LLM\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=150 # A reasonable limit for 3 questions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the sub-questions\n",
    "        sub_queries = response.text.strip().split('\\n')\n",
    "        sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "        print(f\"Generated sub-queries: {sub_queries}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during sub-query generation: {e}\")\n",
    "        sub_queries = [query] # Fallback to the original query\n",
    "    \n",
    "    # Retrieve documents for each sub-query\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        # Create embeddings for the sub-query\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "        # Perform similarity search for the sub-query\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Ensure diversity by selecting from different sub-query results\n",
    "    # Remove duplicates (same text content)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # If we need more results to reach k, add more from initial results\n",
    "    if len(diverse_results) < k:\n",
    "        # Direct retrieval for the main query\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # Return the top k diverse results\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opinion Strategy - Diverse Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "# Also assume that `create_embeddings` and `SimpleVectorStore` are defined.\n",
    "\n",
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for opinion queries focusing on diverse perspectives.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the prompt to guide the AI in identifying different perspectives\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at identifying different perspectives on a topic.\n",
    "    For the given query about opinions or viewpoints, identify different perspectives\n",
    "    that people might have on this topic.\n",
    "\n",
    "    Return a list of exactly 3 different viewpoint angles, one per line.\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Generate the different perspectives using the LLM\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=150 # A reasonable limit for 3 short viewpoints\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the viewpoints\n",
    "        viewpoints = response.text.strip().split('\\n')\n",
    "        viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "        print(f\"Identified viewpoints: {viewpoints}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during viewpoint generation: {e}\")\n",
    "        # Fallback to a simple retrieval if viewpoint generation fails\n",
    "        viewpoint_embedding = create_embeddings(query)\n",
    "        return vector_store.similarity_search(viewpoint_embedding, k=k)\n",
    "    \n",
    "    # Retrieve documents representing each viewpoint\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Combine the main query with the viewpoint\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        # Create embeddings for the combined query\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        # Perform similarity search for the combined query\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # Mark results with the viewpoint they represent\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        # Add the results to the list of all results\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Select a diverse range of opinions\n",
    "    # Ensure we get at least one document from each viewpoint if possible\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Filter documents by viewpoint\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # Fill remaining slots with highest similarity docs\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # Sort remaining docs by similarity\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # Return the top k results\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contextual Strategy - User Context Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def call_gemini(prompt, model=\"gemini-pro\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for contextual queries integrating user context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        user_context (str): Additional user context\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # If no user context provided, try to infer it from the query\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "        For the given query, infer what contextual information might be relevant or implied\n",
    "        but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "        Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        inferred_context_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "        \n",
    "        try:\n",
    "            # Generate the inferred context using the LLM\n",
    "            user_context = call_gemini(inferred_context_prompt, model=\"gemini-2.0-flash\", temperature=0.1)\n",
    "            print(f\"Inferred context: {user_context}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inferring context: {e}\")\n",
    "            user_context = \"\" # Fallback to empty context\n",
    "    \n",
    "    # Reformulate the query to incorporate context\n",
    "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
    "    Given a query and some contextual information, create a more specific query that\n",
    "    incorporates the context to get more relevant information.\n",
    "\n",
    "    Return ONLY the reformulated query without explanation.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    Reformulate the query to incorporate this context:\"\"\"\n",
    "    \n",
    "    contextualized_query_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    \n",
    "    try:\n",
    "        # Generate the contextualized query using the LLM\n",
    "        contextualized_query = call_gemini(contextualized_query_prompt, model=\"gemini-2.0-flash\", temperature=0)\n",
    "        print(f\"Contextualized query: {contextualized_query}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reformulating query: {e}\")\n",
    "        contextualized_query = query # Fallback to original query\n",
    "    \n",
    "    # Retrieve documents based on the contextualized query\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Rank documents considering both relevance and user context\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # Score document relevance considering the context\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # Sort by context relevance and return top k results\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Document Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def score_document_relevance(query, document, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Score document relevance to a query using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        model (str): LLM model. Defaults to \"gemini-2.0-flash\".\n",
    "\n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
    "    Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
    "    0 = Completely irrelevant\n",
    "    10 = Perfectly addresses the query\n",
    "\n",
    "    Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    # Gemini models have higher context limits, but truncating is still good practice.\n",
    "    doc_preview = document[:4000] + \"...\" if len(document) > 4000 else document\n",
    "\n",
    "    # User prompt containing the query and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine the system and user prompts into a single prompt for Gemini\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate response from the model\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0, # Low temperature for a deterministic score\n",
    "                max_output_tokens=10 # Keep output short\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the model's response\n",
    "        score_text = response.text.strip()\n",
    "        \n",
    "        # Extract numeric score using regex\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            return min(10.0, max(0.0, score))  # Ensure score is within 0-10\n",
    "        else:\n",
    "            # Default score if extraction fails\n",
    "            return 5.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during relevance scoring: {e}\")\n",
    "        return 5.0 # Return a neutral score on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def score_document_context_relevance(query, context, document, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Score document relevance considering both query and context using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): User context\n",
    "        document (str): Document text\n",
    "        model (str): LLM model. Defaults to \"gemini-pro\".\n",
    "\n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance considering context\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
    "    Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
    "    when considering the provided context, where:\n",
    "    0 = Completely irrelevant\n",
    "    10 = Perfectly addresses the query in the given context\n",
    "\n",
    "    Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    # Gemini models have higher context limits, but truncating is still a good practice.\n",
    "    doc_preview = document[:4000] + \"...\" if len(document) > 4000 else document\n",
    "    \n",
    "    # User prompt containing the query, context, and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score considering context (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine the system and user prompts into a single prompt for Gemini\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate response from the model\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0, # Low temperature for a deterministic score\n",
    "                max_output_tokens=10 # Keep output short\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the model's response\n",
    "        score_text = response.text.strip()\n",
    "        \n",
    "        # Extract numeric score using regex\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            return min(10.0, max(0.0, score))  # Ensure score is within 0-10\n",
    "        else:\n",
    "            # Default score if extraction fails\n",
    "            return 5.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during relevance scoring: {e}\")\n",
    "        return 5.0 # Return a neutral score on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Adaptive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context for contextual queries\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    # Classify the query to determine its type\n",
    "    try:\n",
    "        query_type = classify_query(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying query. Falling back to Factual retrieval. Details: {e}\")\n",
    "        query_type = \"Factual\"\n",
    "        \n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "\n",
    "    # Select and execute the appropriate retrieval strategy based on the query type\n",
    "    if query_type == \"Factual\":\n",
    "        # Use the factual retrieval strategy for precise information\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # Use the analytical retrieval strategy for comprehensive coverage\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # Use the opinion retrieval strategy for diverse perspectives\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # Use the contextual retrieval strategy, incorporating user context\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # Default to factual retrieval strategy if classification fails\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "        \n",
    "    return results  # Return the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def generate_response(query, results, query_type, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on query, retrieved documents, and query type.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved documents\n",
    "        query_type (str): Type of query\n",
    "        model (str): LLM model. Defaults to \"gemini-2.0-flash\".\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Prepare context from retrieved documents by joining their texts with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "\n",
    "    # Create custom system prompt based on query type\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
    "        Answer the question based on the provided context. Focus on accuracy and precision.\n",
    "        If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
    "        Based on the provided context, offer a comprehensive analysis of the topic.\n",
    "        Cover different aspects and perspectives in your explanation.\n",
    "        If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
    "\n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
    "        Based on the provided context, present different perspectives on the topic.\n",
    "        Ensure fair representation of diverse opinions without showing bias.\n",
    "        Acknowledge where the context presents limited viewpoints.\"\"\"\n",
    "\n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
    "        Answer the question considering both the query and its context.\n",
    "        Make connections between the query context and the information in the provided documents.\n",
    "        If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # Create a single user prompt by combining the system prompt, context, and query\n",
    "    user_prompt = f\"\"\"\n",
    "    {system_prompt}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a helpful response based on the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate response using the Gemini API\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.2 # Temperature for some creativity\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Return the generated response content\n",
    "        return response.text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while generating the response: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Adaptive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with adaptive retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # Generate a response based on the query, retrieved documents, and query type\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # Compile the results into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rag_with_adaptive_retrieval_folder(folder_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with adaptive retrieval for all PDFs in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing PDF documents\n",
    "        query (str): User query\n",
    "        k (int): Number of documents/chunks to retrieve\n",
    "        user_context (str): Optional user context\n",
    "\n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Process all PDFs in the folder to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store = process_folder(folder_path)\n",
    "    \n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # Generate a response based on the query, retrieved documents, and query type\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # Compile the results into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare adaptive retrieval with standard retrieval on a set of test queries.\n",
    "    \n",
    "    This function processes a document, runs both standard and adaptive retrieval methods\n",
    "    on each test query, and compares their performance. If reference answers are provided,\n",
    "    it also evaluates the quality of responses against these references.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document to be processed as the knowledge source\n",
    "        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n",
    "        reference_answers (List[str], optional): Reference answers for evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results containing individual query results and overall comparison\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
    "    \n",
    "    # Process document to extract text, create chunks and build the vector store\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize collection for storing comparison results\n",
    "    results = []\n",
    "    \n",
    "    # Process each test query with both retrieval methods\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # --- Standard retrieval approach ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        # Create embedding for the query\n",
    "        query_embedding = create_embeddings(query)\n",
    "        # Retrieve documents using simple vector similarity\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        # Generate response using a generic approach\n",
    "        standard_response = generate_response(query, standard_docs, \"General\")\n",
    "        \n",
    "        # --- Adaptive retrieval approach ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
    "        query_type = classify_query(query)\n",
    "        # Retrieve documents using the strategy appropriate for this query type\n",
    "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "        # Generate a response tailored to the query type\n",
    "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
    "        \n",
    "        # Store complete results for this query\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add reference answer if available for this query\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result[\"reference_answer\"] = reference_answers[i]\n",
    "            \n",
    "        results.append(result)\n",
    "        \n",
    "        # Display preview of both responses for quick comparison\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {standard_response[:200]}...\")\n",
    "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
    "    \n",
    "    # Calculate comparative metrics if reference answers are available\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(comparison)\n",
    "    \n",
    "    # Return the complete evaluation results\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    Compare standard and adaptive responses against reference answers.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results containing both types of responses\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI in comparing responses\n",
    "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
    "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
    "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
    "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "    \n",
    "    # Initialize the comparison text with a header\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "    \n",
    "    # Iterate through each result to compare responses\n",
    "    for i, result in enumerate(results):\n",
    "        # Skip if there is no reference answer for the query\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "            \n",
    "        # Add query details to the comparison text\n",
    "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # Add standard retrieval response to the comparison text\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Add adaptive retrieval response to the comparison text\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Create the user prompt for the AI to compare the responses\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "        \n",
    "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
    "        \n",
    "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
    "        \n",
    "        Provide a detailed comparison of the two responses.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the comparison analysis using the OpenAI client\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # Add the AI's comparison analysis to the comparison text\n",
    "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
    "    \n",
    "    return comparison_text  # Return the complete comparison analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    Compare standard and adaptive responses against reference answers.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results containing both types of responses\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI in comparing responses\n",
    "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
    "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
    "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
    "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "    \n",
    "    # Initialize the comparison text with a header\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "    \n",
    "    # Iterate through each result to compare responses\n",
    "    for i, result in enumerate(results):\n",
    "        # Skip if there is no reference answer for the query\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "            \n",
    "        # Add query details to the comparison text\n",
    "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # Add standard retrieval response to the comparison text\n",
    "        standard_response = result['standard_retrieval'].get('response', 'No response found.')\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{standard_response}\\n\\n\"\n",
    "        \n",
    "        # Add adaptive retrieval response to the comparison text\n",
    "        adaptive_response = result['adaptive_retrieval'].get('response', 'No response found.')\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{adaptive_response}\\n\\n\"\n",
    "        \n",
    "        # Create the user prompt for the AI to compare the responses\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "        \n",
    "        Standard Retrieval Response: {standard_response}\n",
    "        \n",
    "        Adaptive Retrieval Response: {adaptive_response}\n",
    "        \n",
    "        Provide a detailed comparison of the two responses.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Combine system and user prompts into a single string for Gemini\n",
    "        full_prompt = f\"{comparison_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "        # Initialize the Gemini GenerativeModel\n",
    "        model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "        # Generate the comparison analysis using the Gemini API\n",
    "        try:\n",
    "            response = model_instance.generate_content(\n",
    "                full_prompt,\n",
    "                generation_config=genai.GenerationConfig(\n",
    "                    temperature=0.2 # Temperature for some creative analysis\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Add the AI's comparison analysis to the comparison text\n",
    "            comparison_text += f\"**Comparison Analysis:**\\n{response.text.strip()}\\n\\n\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            comparison_text += f\"**Comparison Analysis:**\\nAn error occurred during analysis: {e}\\n\\n\"\n",
    "    \n",
    "    return comparison_text # Return the complete comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Adaptive Retrieval System (Customized Queries)\n",
    "\n",
    "The final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running query: What does the Red Cross and PEMA shelter guide say about bringing pets to emergency shelters in Harrisburg? ---\n",
      "\n",
      "=== RAG WITH ADAPTIVE RETRIEVAL ===\n",
      "Query: What does the Red Cross and PEMA shelter guide say about bringing pets to emergency shelters in Harrisburg?\n",
      "\n",
      "Extracting text from PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PA 211 Disaster Community Resources.pdf\n",
      "Chunking text...\n",
      "Created 13 text chunks for PA 211 Disaster Community Resources.pdf\n",
      "Creating embeddings for chunks...\n",
      "Added 13 chunks from PA 211 Disaster Community Resources.pdf to the vector store.\n",
      "\n",
      "Extracting text from PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/211 RESPONDS TO URGENT NEEDS.pdf\n",
      "Chunking text...\n",
      "Created 7 text chunks for 211 RESPONDS TO URGENT NEEDS.pdf\n",
      "Creating embeddings for chunks...\n",
      "Added 7 chunks from 211 RESPONDS TO URGENT NEEDS.pdf to the vector store.\n",
      "\n",
      "Extracting text from PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PEMA.pdf\n",
      "Chunking text...\n",
      "Created 69 text chunks for PEMA.pdf\n",
      "Creating embeddings for chunks...\n",
      "Added 69 chunks from PEMA.pdf to the vector store.\n",
      "\n",
      "Extracting text from PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/ready-gov_disaster-preparedness-guide-for-older-adults.pdf\n",
      "Chunking text...\n",
      "Created 23 text chunks for ready-gov_disaster-preparedness-guide-for-older-adults.pdf\n",
      "Creating embeddings for chunks...\n",
      "Added 23 chunks from ready-gov_disaster-preparedness-guide-for-older-adults.pdf to the vector store.\n",
      "\n",
      "Extracting text from PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/Substantial Damages Toolkit.pdf\n",
      "Chunking text...\n",
      "Created 299 text chunks for Substantial Damages Toolkit.pdf\n",
      "Creating embeddings for chunks...\n",
      "Added 299 chunks from Substantial Damages Toolkit.pdf to the vector store.\n",
      "\n",
      "All done! Processed 411 chunks from all PDFs.\n",
      "An error occurred during query classification: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Query classified as: Factual\n",
      "An error occurred during query classification: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Query classified as: Factual\n",
      "Executing Factual retrieval strategy for: 'What does the Red Cross and PEMA shelter guide say about bringing pets to emergency shelters in Harrisburg?'\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 11\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query, ref_answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_queries, reference_answers):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Running query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[43mrag_with_adaptive_retrieval_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerated Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrag_with_adaptive_retrieval_folder\u001b[39m\u001b[34m(folder_path, query, k, user_context)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery classified as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Retrieve documents using the adaptive retrieval strategy based on the query type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m retrieved_docs = \u001b[43madaptive_retrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Generate a response based on the query, retrieved documents, and query type\u001b[39;00m\n\u001b[32m     30\u001b[39m response = generate_response(query, retrieved_docs, query_type)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36madaptive_retrieval\u001b[39m\u001b[34m(query, vector_store, k, user_context)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Select and execute the appropriate retrieval strategy based on the query type\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mFactual\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Use the factual retrieval strategy for precise information\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     results = \u001b[43mfactual_retrieval_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mAnalytical\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Use the analytical retrieval strategy for comprehensive coverage\u001b[39;00m\n\u001b[32m     29\u001b[39m     results = analytical_retrieval_strategy(query, vector_store, k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mfactual_retrieval_strategy\u001b[39m\u001b[34m(query, vector_store, k)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Generate the enhanced query using the LLM\u001b[39;00m\n\u001b[32m     38\u001b[39m enhanced_query_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m enhanced_query = \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43menhanced_query_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnhanced query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menhanced_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Create embeddings for the enhanced query\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcall_gemini\u001b[39m\u001b[34m(prompt, model, temperature)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A helper function to make a call to the Gemini API.\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m model_instance = genai.GenerativeModel(model)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/generativeai/generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG Google 2/rag_env/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 11\n}\n]"
     ]
    }
   ],
   "source": [
    "# Path to your knowledge source folder (with all your PDF files)\n",
    "folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs\"  # Update as needed\n",
    "\n",
    "# Define test queries covering different query types\n",
    "test_queries = [\n",
    "    \"What does the Red Cross and PEMA shelter guide say about bringing pets to emergency shelters in Harrisburg?\"                                           \n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "   \"The Red Cross and PEMA shelter guide indicates that select shelters, including those managed by the Red Cross in Harrisburg, permit pets in designated areas. Service animals are always allowed. PA 211 and shelter staff can provide details on which shelters are pet-friendly during each event.\" \n",
    "]\n",
    "\n",
    "# Example usage of your RAG function for PDFs in a folder:\n",
    "for query, ref_answer in zip(test_queries, reference_answers):\n",
    "    print(f\"\\n--- Running query: {query} ---\")\n",
    "    result = rag_with_adaptive_retrieval_folder(folder_path, query)\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(result[\"response\"])\n",
    "    print(\"\\nReference Answer:\")\n",
    "    print(ref_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(folder_path, test_queries, reference_answers):\n",
    "    # Process ALL PDFs in the folder, not just one!\n",
    "    all_chunks = []\n",
    "    vector_store = SimpleVectorStore()\n",
    "    import os\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing: {pdf_path}\")\n",
    "                # Extract, chunk, embed\n",
    "                extracted_text = extract_text_from_pdf(pdf_path)\n",
    "                chunks = chunk_text(extracted_text, 1000, 200)\n",
    "                chunk_embeddings = [create_embeddings(chunk) for chunk in chunks]\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "                    vector_store.add_item(chunk, embedding, metadata={\"index\": i, \"source\": pdf_path})\n",
    "                all_chunks.extend(chunks)\n",
    "    # Now proceed as before, using the combined vector_store\n",
    "\n",
    "    all_results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n===== Evaluating Query: '{query}' =====\")\n",
    "        query_type = classify_query(query)\n",
    "        \n",
    "        # --- Standard Retrieval ---\n",
    "        print(\"\\n--- Running Standard Retrieval ---\")\n",
    "        standard_embedding = create_embeddings(query)\n",
    "        standard_results = vector_store.similarity_search(standard_embedding, k=4)\n",
    "        standard_response = generate_response(query, standard_results, query_type)\n",
    "        \n",
    "        # --- Adaptive Retrieval ---\n",
    "        print(\"\\n--- Running Adaptive Retrieval ---\")\n",
    "        adaptive_results = adaptive_retrieval(query, vector_store, k=4)\n",
    "        adaptive_response = generate_response(query, adaptive_results, query_type)\n",
    "        \n",
    "        # --- Store Results ---\n",
    "        result_entry = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"reference_answer\": reference_answers[i] if i < len(reference_answers) else None,\n",
    "            \"standard_retrieval\": {\n",
    "                \"results\": standard_results,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"results\": adaptive_results,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        all_results.append(result_entry)\n",
    "        print(\"------------------------------------------\")\n",
    "        \n",
    "    comparison = compare_responses(all_results)\n",
    "    \n",
    "    return {\n",
    "        \"individual_results\": all_results,\n",
    "        \"comparison\": comparison\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import google.api_core.exceptions\n",
    "\n",
    "# --- Rate-limit aware wrapper ---\n",
    "def call_gemini(prompt, model=\"gemini-2.0-flash\", temperature=0, max_retries=6, base_sleep=8):\n",
    "    \"\"\"\n",
    "    Generate with retry/backoff when quota/rate limits are hit.\n",
    "    Tries to honor retry_delay from the error when present.\n",
    "    \"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = model_instance.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "            return (resp.text or \"\").strip()\n",
    "        except google.api_core.exceptions.ResourceExhausted as e:\n",
    "            # Try to parse a suggested retry seconds from the message\n",
    "            m = re.search(r\"retry_delay\\s*{\\s*seconds:\\s*(\\d+)\", str(e))\n",
    "            wait_s = int(m.group(1)) if m else base_sleep * (attempt + 1)\n",
    "            print(f\"[Rate limit] Waiting {wait_s}s (attempt {attempt+1}/{max_retries})...\")\n",
    "            time.sleep(wait_s)\n",
    "        except Exception as e:\n",
    "            # Non-quota error: surface and stop retrying\n",
    "            print(f\"[Gemini error] {e}\")\n",
    "            return \"ERROR: generation failed.\"\n",
    "    return \"ERROR: gave up after retries due to quota limits.\"\n",
    "\n",
    "def generate_response(query, results, query_type, model=\"gemini-2.0-flash\"):\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results]) if results else \"\"\n",
    "    system_prompt = (\n",
    "        f\"You are a helpful assistant. Answer the user's question based on the provided context. \"\n",
    "        f\"The query type is {query_type}. If you cannot answer, say so.\"\n",
    "    )\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    return call_gemini(f\"{system_prompt}\\n{user_prompt}\", model=model, temperature=0.2)\n",
    "\n",
    "# --- Evaluation Pipeline using an EXISTING vector store (no PDF processing) ---\n",
    "def evaluate_adaptive_vs_standard(vector_store, test_queries, reference_answers, k=4, pause_seconds=6):\n",
    "    \"\"\"\n",
    "    Evaluate standard vs adaptive retrieval using a prebuilt vector_store.\n",
    "    Adds pauses between API calls to avoid hitting per-minute quotas.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    print(\"Using existing vector store (no PDF processing).\")\n",
    "\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n===== Evaluating Query: '{query}' =====\")\n",
    "        query_type = classify_query(query) or \"Factual\"\n",
    "        time.sleep(pause_seconds)  # small pause between calls\n",
    "\n",
    "        # --- Standard Retrieval ---\n",
    "        print(\"\\n--- Running Standard Retrieval ---\")\n",
    "        standard_embedding = create_embeddings(query)  # 1 embed call\n",
    "        standard_results = vector_store.similarity_search(standard_embedding, k=k)\n",
    "        standard_response = generate_response(query, standard_results, query_type)  # 1 generate call\n",
    "        time.sleep(pause_seconds)\n",
    "\n",
    "        # --- Adaptive Retrieval ---\n",
    "        print(\"\\n--- Running Adaptive Retrieval ---\")\n",
    "        adaptive_results = adaptive_retrieval(query, vector_store, k=k)  # internally uses a few generates/embeds\n",
    "        adaptive_response = generate_response(query, adaptive_results, query_type)  # 1 generate call\n",
    "        time.sleep(pause_seconds)\n",
    "\n",
    "        # --- Store Results ---\n",
    "        result_entry = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"reference_answer\": reference_answers[i] if i < len(reference_answers) else None,\n",
    "            \"standard_retrieval\": {\n",
    "                \"results\": standard_results,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"results\": adaptive_results,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        all_results.append(result_entry)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "    comparison = compare_responses(all_results)  # this will call Gemini a few times; keep the pauses inside compare if needed\n",
    "    return {\n",
    "        \"individual_results\": all_results,\n",
    "        \"comparison\": comparison\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== vector_store_loader.py ======\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional\n",
    "\n",
    "# Optional: only import LangChain bits if present\n",
    "def _lazy_import_faiss():\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "    return FAISS, Embeddings\n",
    "\n",
    "def _lazy_import_chroma():\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    return Chroma\n",
    "\n",
    "# ---------- SimpleVectorStore (your custom one) ----------\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        self.np = np\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(self.np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = self.np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "            nq = self.np.linalg.norm(query_vector)\n",
    "            nv = self.np.linalg.norm(vector)\n",
    "            score = 0.0 if nq == 0 or nv == 0 else float(self.np.dot(query_vector, vector) / (nq * nv))\n",
    "            similarities.append((i, score))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        out = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            out.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "        return out\n",
    "\n",
    "# ---------- Adapters so your code can call .similarity_search(...) uniformly ----------\n",
    "class VectorStoreAdapter:\n",
    "    \"\"\"Uniform interface: .similarity_search(query_embedding, k) -> List[dict(text, metadata, similarity)].\"\"\"\n",
    "    def __init__(self, kind: str, store, name: Optional[str] = None):\n",
    "        self.kind = kind\n",
    "        self.store = store\n",
    "        self.name = name or kind\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=4):\n",
    "        if self.kind == \"simple\":\n",
    "            return self.store.similarity_search(query_embedding, k=k)\n",
    "\n",
    "        if self.kind in (\"faiss\", \"chroma\"):\n",
    "            # LangChain vectorstores return Documents; use by-vector search to avoid re-embedding\n",
    "            docs = self.store.similarity_search_by_vector(query_embedding, k=k)\n",
    "            # Some implementations expose scores via similarity_search_with_score_by_vector\n",
    "            # Try to get scores if available; fall back to None\n",
    "            try:\n",
    "                docs_scores = self.store.similarity_search_with_score_by_vector(query_embedding, k=k)\n",
    "                score_map = {id(d): s for d, s in docs_scores}\n",
    "            except Exception:\n",
    "                score_map = {}\n",
    "\n",
    "            results = []\n",
    "            for d in docs:\n",
    "                # LC Document: .page_content and .metadata\n",
    "                sim = score_map.get(id(d))\n",
    "                # If scores are distances, you might want to invert/normalize — leaving as-is here.\n",
    "                results.append({\n",
    "                    \"text\": getattr(d, \"page_content\", \"\"),\n",
    "                    \"metadata\": getattr(d, \"metadata\", {}),\n",
    "                    \"similarity\": sim if sim is not None else 0.0\n",
    "                })\n",
    "            return results\n",
    "\n",
    "        raise ValueError(f\"Unknown adapter kind: {self.kind}\")\n",
    "\n",
    "# ---------- Loaders ----------\n",
    "def load_simple_pickle(pkl_path: str) -> VectorStoreAdapter:\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    # If it’s raw dicts, you could reconstruct a SimpleVectorStore here.\n",
    "    # We assume it’s your SimpleVectorStore instance.\n",
    "    return VectorStoreAdapter(\"simple\", store, name=os.path.basename(pkl_path))\n",
    "\n",
    "def load_faiss(dir_path: str, embeddings=None) -> VectorStoreAdapter:\n",
    "    FAISS, Embeddings = _lazy_import_faiss()\n",
    "\n",
    "    # A tiny dummy embeddings class so FAISS.load_local won’t complain if you only do by-vector search.\n",
    "    class _DummyEmbeddings(Embeddings):\n",
    "        def embed_documents(self, texts): raise NotImplementedError(\"Not used\")\n",
    "        def embed_query(self, text): raise NotImplementedError(\"Not used\")\n",
    "\n",
    "    if embeddings is None:\n",
    "        embeddings = _DummyEmbeddings()\n",
    "\n",
    "    store = FAISS.load_local(dir_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return VectorStoreAdapter(\"faiss\", store, name=os.path.basename(dir_path))\n",
    "\n",
    "def load_chroma(dir_path: str) -> VectorStoreAdapter:\n",
    "    Chroma = _lazy_import_chroma()\n",
    "    store = Chroma(persist_directory=dir_path)  # embedding_function not needed for by-vector calls\n",
    "    return VectorStoreAdapter(\"chroma\", store, name=os.path.basename(dir_path))\n",
    "\n",
    "def detect_backend(path: str) -> str:\n",
    "    if os.path.isfile(path) and path.endswith(\".pkl\"):\n",
    "        return \"simple\"\n",
    "    if os.path.isdir(path):\n",
    "        # FAISS typical files\n",
    "        faiss_files = {\"index.faiss\", \"index.pkl\"}\n",
    "        if faiss_files.issubset(set(os.listdir(path))):\n",
    "            return \"faiss\"\n",
    "        # Chroma typical files\n",
    "        chroma_markers = {\"chroma-collections.parquet\", \"chroma-embeddings.parquet\", \"chroma.sqlite3\"}\n",
    "        if any(fname in os.listdir(path) for fname in chroma_markers):\n",
    "            return \"chroma\"\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot detect vector store backend at path. \"\n",
    "        \"Expected a .pkl file (SimpleVectorStore) or a FAISS/Chroma directory.\"\n",
    "    )\n",
    "\n",
    "def load_vector_store(path: str, backend: Optional[str] = None, embeddings=None) -> VectorStoreAdapter:\n",
    "    \"\"\"\n",
    "    Auto-detect and load a vector store (SimpleVectorStore .pkl, FAISS dir, or Chroma dir).\n",
    "    For FAISS, you can pass an 'embeddings' instance used at build time; otherwise a dummy is used.\n",
    "    \"\"\"\n",
    "    kind = backend or detect_backend(path)\n",
    "    if kind == \"simple\":\n",
    "        return load_simple_pickle(path)\n",
    "    if kind == \"faiss\":\n",
    "        return load_faiss(path, embeddings=embeddings)\n",
    "    if kind == \"chroma\":\n",
    "        return load_chroma(path)\n",
    "    raise ValueError(f\"Unsupported backend: {kind}\")\n",
    "\n",
    "# ---------- Optional helpers to save/load SimpleVectorStore ----------\n",
    "def save_simple_vector_store(store: SimpleVectorStore, pkl_path: str):\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(store, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inline vector-store loader & adapter ===\n",
    "import os, pickle\n",
    "from typing import Optional\n",
    "\n",
    "# --- Your SimpleVectorStore shape (only needed if you saved a pickle of it) ---\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        self.np = np\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(self.np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        q = self.np.array(query_embedding)\n",
    "        nq = self.np.linalg.norm(q)\n",
    "        sims = []\n",
    "        for i, v in enumerate(self.vectors):\n",
    "            nv = self.np.linalg.norm(v)\n",
    "            score = 0.0 if nq == 0 or nv == 0 else float(self.np.dot(q, v) / (nq * nv))\n",
    "            if (not filter_func) or filter_func(self.metadata[i]):\n",
    "                sims.append((i, score))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        out = []\n",
    "        for i in range(min(k, len(sims))):\n",
    "            idx, score = sims[i]\n",
    "            out.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "        return out\n",
    "\n",
    "# --- Uniform adapter so your evaluator can call .similarity_search(query_embedding, k) ---\n",
    "class VectorStoreAdapter:\n",
    "    def __init__(self, kind: str, store, name: Optional[str] = None):\n",
    "        self.kind = kind\n",
    "        self.store = store\n",
    "        self.name = name or kind\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=4):\n",
    "        if self.kind == \"simple\":\n",
    "            return self.store.similarity_search(query_embedding, k=k)\n",
    "\n",
    "        if self.kind in (\"faiss\", \"chroma\"):\n",
    "            # by-vector search (no re-embedding)\n",
    "            try:\n",
    "                docs_scores = self.store.similarity_search_with_score_by_vector(query_embedding, k=k)\n",
    "                results = []\n",
    "                for d, s in docs_scores:\n",
    "                    results.append({\n",
    "                        \"text\": getattr(d, \"page_content\", \"\"),\n",
    "                        \"metadata\": getattr(d, \"metadata\", {}),\n",
    "                        \"similarity\": float(s) if s is not None else 0.0\n",
    "                    })\n",
    "                return results\n",
    "            except Exception:\n",
    "                docs = self.store.similarity_search_by_vector(query_embedding, k=k)\n",
    "                return [{\n",
    "                    \"text\": getattr(d, \"page_content\", \"\"),\n",
    "                    \"metadata\": getattr(d, \"metadata\", {}),\n",
    "                    \"similarity\": 0.0\n",
    "                } for d in docs]\n",
    "\n",
    "        raise ValueError(f\"Unknown adapter kind: {self.kind}\")\n",
    "\n",
    "# --- Backends ---\n",
    "def _lazy_import_faiss():\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "    return FAISS, Embeddings\n",
    "\n",
    "def _lazy_import_chroma():\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    return Chroma\n",
    "\n",
    "def load_simple_pickle(pkl_path: str) -> VectorStoreAdapter:\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    # If you saved raw dicts, reconstruct here; assuming you pickled the object.\n",
    "    return VectorStoreAdapter(\"simple\", store, name=os.path.basename(pkl_path))\n",
    "\n",
    "def load_faiss(dir_path: str, embeddings=None) -> VectorStoreAdapter:\n",
    "    FAISS, Embeddings = _lazy_import_faiss()\n",
    "    class _DummyEmb(Embeddings):\n",
    "        def embed_documents(self, texts): raise NotImplementedError\n",
    "        def embed_query(self, text): raise NotImplementedError\n",
    "    if embeddings is None:\n",
    "        embeddings = _DummyEmb()\n",
    "    store = FAISS.load_local(dir_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return VectorStoreAdapter(\"faiss\", store, name=os.path.basename(dir_path))\n",
    "\n",
    "def load_chroma(dir_path: str) -> VectorStoreAdapter:\n",
    "    Chroma = _lazy_import_chroma()\n",
    "    store = Chroma(persist_directory=dir_path)\n",
    "    return VectorStoreAdapter(\"chroma\", store, name=os.path.basename(dir_path))\n",
    "\n",
    "def detect_backend(path: str) -> str:\n",
    "    if os.path.isfile(path) and path.endswith(\".pkl\"):\n",
    "        return \"simple\"\n",
    "    if os.path.isdir(path):\n",
    "        names = set(os.listdir(path))\n",
    "        if {\"index.faiss\", \"index.pkl\"}.issubset(names):\n",
    "            return \"faiss\"\n",
    "        if any(n in names for n in {\"chroma.sqlite3\", \"chroma-collections.parquet\", \"chroma-embeddings.parquet\"}):\n",
    "            return \"chroma\"\n",
    "    raise FileNotFoundError(\"Could not detect vector store backend at given path.\")\n",
    "\n",
    "def load_vector_store(path: str, backend: Optional[str] = None, embeddings=None) -> VectorStoreAdapter:\n",
    "    kind = backend or detect_backend(path)\n",
    "    if kind == \"simple\":\n",
    "        return load_simple_pickle(path)\n",
    "    if kind == \"faiss\":\n",
    "        return load_faiss(path, embeddings=embeddings)\n",
    "    if kind == \"chroma\":\n",
    "        return load_chroma(path)\n",
    "    raise ValueError(f\"Unsupported backend: {kind}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
