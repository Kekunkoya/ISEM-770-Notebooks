{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Relevant Segment Extraction (RSE) for Enhanced RAG\n",
    "\n",
    "In this notebook, we implement a Relevant Segment Extraction (RSE) technique to improve the context quality in our RAG system. Rather than simply retrieving a collection of isolated chunks, we identify and reconstruct continuous segments of text that provide better context to our language model.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "Relevant chunks tend to be clustered together within documents. By identifying these clusters and preserving their continuity, we provide more coherent context for the LLM to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Use a context manager to automatically close the document\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page to extract text\n",
    "            for page in mypdf:\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=0):\n",
    "    \"\"\"\n",
    "    Split text into non-overlapping chunks.\n",
    "    For RSE, we typically want non-overlapping chunks so we can reconstruct segments properly.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Simple character-based chunking\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A lightweight vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=1536):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \n",
    "        Args:\n",
    "            dimension (int): Dimension of embeddings\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, vectors=None, metadata=None):\n",
    "        \"\"\"\n",
    "        Add documents to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): List of document chunks\n",
    "            vectors (List[List[float]], optional): List of embedding vectors\n",
    "            metadata (List[Dict], optional): List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata.append(meta)\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (List[float]): Query embedding vector\n",
    "            top_k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of results with documents, scores, and metadata\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Convert query vector to numpy array\n",
    "        query_array = np.array(query_vector)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "                # Compute cosine similarity\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-k results\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"document\": self.documents[i],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings with Gemini...\n",
      "\n",
      "Embeddings created successfully.\n",
      "Number of embeddings: 3\n",
      "Embedding dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(texts: List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using the Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of texts to embed\n",
    "        model (str): Embedding model to use\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: List of embedding vectors\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        # The Gemini API's `embed_content` can handle a list of texts directly,\n",
    "        # so explicit batching loops are not required for most use cases.\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=texts\n",
    "        )\n",
    "        \n",
    "        # The embedding list is directly under the 'embedding' key\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a list of text chunks\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"Social factors like family breakdown can also lead to homelessness.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Creating embeddings with Gemini...\")\n",
    "    # Create embeddings for the text chunks\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"\\nEmbeddings created successfully.\")\n",
    "        print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"Embedding dimensions: {len(embeddings[0])}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with RSE\n",
    "Now let's implement the core RSE functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 65 chunks\n",
      "Generating embeddings for chunks...\n",
      "\n",
      "Document '/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf' processed successfully.\n",
      "Vector store contains 65 items.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper Functions (Gemini-compatible) ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Chunks the given text into segments.\"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for i in range(0, len(text), step):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(texts: List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"Creates embeddings for a list of texts using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        if not texts:\n",
    "            return []\n",
    "        # Gemini's embed_content handles batching, no need for a manual loop\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"A simple vector store implementation using NumPy.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_documents(self, documents: List[str], vectors: List[Any], metadata: List[Dict]):\n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(np.array(vec, dtype=np.float32))\n",
    "            self.metadata.append(meta)\n",
    "\n",
    "# --- 3. The main processing function (revised for Gemini) ---\n",
    "def process_document(pdf_path: str, chunk_size: int = 800) -> Tuple[List[str], SimpleVectorStore, Dict]:\n",
    "    \"\"\"\n",
    "    Process a document for use with RSE.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from document...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text into non-overlapping segments...\")\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"Generating embeddings for chunks...\")\n",
    "    # Use the Gemini-compatible embedding function\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    if not chunk_embeddings or len(chunks) != len(chunk_embeddings):\n",
    "        raise RuntimeError(\"Failed to create embeddings or embedding count does not match chunk count.\")\n",
    "\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info\n",
    "\n",
    "# --- 4. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = '/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf'\n",
    "    \n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_file_path}'\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        chunks, store, doc_info = process_document(pdf_file_path, chunk_size=800)\n",
    "        print(f\"\\nDocument '{doc_info['source']}' processed successfully.\")\n",
    "        print(f\"Vector store contains {len(store.documents)} items.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during document processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE Core Algorithm: Computing Chunk Values and Finding Best Segments\n",
    "Now that we have the necessary functions to process a document and generate embeddings for its chunks, we can implement the core algorithm for RSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating chunk values for the query...\n",
      "\n",
      "Calculated Chunk Values:\n",
      "Chunk 0 ('Homelessness is a complex soci...') Value: 0.8000\n",
      "Chunk 1 ('A lack of affordable housing i...') Value: 0.8000\n",
      "Chunk 2 ('The sun is the center of our s...') Value: -1.2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper Functions (Assumed to be defined and configured for Gemini) ---\n",
    "def create_embeddings(texts: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text(s) using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"A placeholder for your vector store class.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        if not self.vectors: return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if np.linalg.norm(query_vector) == 0 or np.linalg.norm(vector) == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append({\"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "# --- 3. The main function (revised for Gemini) ---\n",
    "def calculate_chunk_values(query: str, chunks: List[str], vector_store: SimpleVectorStore, irrelevant_chunk_penalty: float = 0.2) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate chunk values by combining relevance and position.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query text\n",
    "        chunks (List[str]): List of document chunks\n",
    "        vector_store (SimpleVectorStore): Vector store containing the chunks\n",
    "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: List of chunk values\n",
    "    \"\"\"\n",
    "    # Create query embedding using the Gemini-compatible function\n",
    "    query_embedding_list = create_embeddings(query)\n",
    "    if not query_embedding_list:\n",
    "        return [0.0] * len(chunks)\n",
    "    query_embedding = query_embedding_list[0]\n",
    "    \n",
    "    # Get all chunks with similarity scores\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # Create a mapping of chunk_index to relevance score\n",
    "    relevance_scores = {result[\"metadata\"][\"index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # Calculate chunk values (relevance score minus penalty)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values\n",
    "\n",
    "# --- 4. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a vector store with chunks and embeddings\n",
    "    store = SimpleVectorStore()\n",
    "    chunks = [\n",
    "        \"Homelessness is a complex social problem. It is a state of not having a place to live.\",\n",
    "        \"A lack of affordable housing is a key contributing factor. This is a big problem in many cities.\",\n",
    "        \"The sun is the center of our solar system. The earth orbits the sun.\" # Irrelevant chunk\n",
    "    ]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = create_embeddings(chunk)\n",
    "        if embedding:\n",
    "            store.add_item(chunk, embedding[0], {\"index\": i})\n",
    "    \n",
    "    query = \"What causes homelessness?\"\n",
    "    \n",
    "    print(\"Calculating chunk values for the query...\")\n",
    "    values = calculate_chunk_values(query, chunks, store)\n",
    "    \n",
    "    print(\"\\nCalculated Chunk Values:\")\n",
    "    for i, value in enumerate(values):\n",
    "        print(f\"Chunk {i} ('{chunks[i][:30]}...') Value: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    Find the best segments using a variant of the maximum sum subarray algorithm.\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): Values for each chunk\n",
    "        max_segment_length (int): Maximum length of a single segment\n",
    "        total_max_length (int): Maximum total length across all segments\n",
    "        min_segment_value (float): Minimum value for a segment to be considered\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: List of (start, end) indices for best segments\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal continuous text segments...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # Keep finding segments until we hit our limits\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # Minimum threshold for a segment\n",
    "        best_segment = None\n",
    "        \n",
    "        # Try each possible starting position\n",
    "        for start in range(len(chunk_values)):\n",
    "            # Skip if this start position is already in a selected segment\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # Try each possible segment length\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # Skip if end position is already in a selected segment\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate segment value as sum of chunk values\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # Update best segment if this one is better\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # If we found a good segment, add it\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
    "        else:\n",
    "            # No more good segments to find\n",
    "            break\n",
    "    \n",
    "    # Sort segments by their starting position for readability\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing and Using Segments for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    Reconstruct text segments based on chunk indices.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): List of all document chunks\n",
    "        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of reconstructed text segments\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # Join the chunks in this segment to form the complete segment text\n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        # Append the segment text and its range to the reconstructed_segments list\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # Return the list of reconstructed text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    Format segments into a context string for the LLM.\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): List of segment dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted context text\n",
    "    \"\"\"\n",
    "    context = []  # Initialize an empty list to store the formatted context\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # Create a header for each segment with its index and chunk range\n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # Add the segment header to the context list\n",
    "        context.append(segment['text'])  # Add the segment text to the context list\n",
    "        context.append(\"-\" * 80)  # Add a separator line for readability\n",
    "    \n",
    "    # Join all elements in the context list with double newlines and return the result\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Responses with RSE Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, homelessness is a complex issue stemming from a combination of economic, social, and personal factors.  A significant contributing factor is the lack of affordable housing, particularly impacting low-income individuals and families.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text from relevant segments\n",
    "        model (str): LLM model to use\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    print(\"Generating response using relevant segments as context...\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "The context consists of document segments that have been retrieved as relevant to the user's query.\n",
    "Use the information from these segments to provide a comprehensive and accurate answer.\n",
    "If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a helpful answer based on the context provided.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        \n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and context from a previous step\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    context = \"\"\"\n",
    "    SEGMENT 1:\n",
    "    Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\n",
    "    \n",
    "    SEGMENT 2:\n",
    "    A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, context)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RSE Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with Relevant Segment Extraction.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        query (str): User query\n",
    "        chunk_size (int): Size of chunks\n",
    "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Result with query, segments, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # Calculate relevance scores and chunk values based on the query\n",
    "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    # Find the best segments of text based on chunk values\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    # Reconstruct text segments from the best chunks\n",
    "    print(\"\\nReconstructing text segments from chunks...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    # Format the segments into a context string for the language model\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    # Generate a response from the language model using the context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Compile the result into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Standard Retrieval\n",
    "Let's implement a standard retrieval approach to compare with RSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
    "    \"\"\"\n",
    "    Standard RAG with top-k retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        query (str): User query\n",
    "        k (int): Number of chunks to retrieve\n",
    "        chunk_size (int): Size of chunks\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Result with query, chunks, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    print(\"Creating query embedding and retrieving chunks...\")\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # Retrieve the top-k most relevant chunks based on the query embedding\n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"document\"] for result in results]\n",
    "    \n",
    "    # Format the retrieved chunks into a context string\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"CHUNK {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # Generate a response from the language model using the context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Compile the result into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of RSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare RSE with standard top-k retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        query (str): User query\n",
    "        reference_answer (str, optional): Reference answer for evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n========= EVALUATION =========\\n\")\n",
    "    \n",
    "    # Run the RAG with Relevant Segment Extraction (RSE) method\n",
    "    rse_result = rag_with_rse(pdf_path, query)\n",
    "    \n",
    "    # Run the standard top-k retrieval method\n",
    "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
    "    \n",
    "    # If a reference answer is provided, evaluate the responses\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== COMPARING RESULTS ===\")\n",
    "        \n",
    "        # Create an evaluation prompt to compare the responses against the reference answer\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "\n",
    "            Reference Answer:\n",
    "            {reference_answer}\n",
    "\n",
    "            Response from Standard Retrieval:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            Response from Relevant Segment Extraction:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            Compare these two responses against the reference answer. Which one is:\n",
    "            1. More accurate and comprehensive\n",
    "            2. Better at addressing the user's query\n",
    "            3. Less likely to include irrelevant information\n",
    "\n",
    "            Explain your reasoning for each point.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Evaluating responses against reference answer...\")\n",
    "        \n",
    "        # Generate the evaluation using the specified model\n",
    "        evaluation = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Print the evaluation results\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "    \n",
    "    # Return the results of both methods\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "The provided RAG system results are completely unfaithful and irrelevant to the reference answer regarding the ETHOS typology.  Neither the standard nor reranked results mention any aspect of the ETHOS typology's definition, structure, or purpose. Instead, they offer a list of common contributing factors to homelessness.\n",
      "\n",
      "Let's break down the evaluation criteria:\n",
      "\n",
      "* **Faithfulness:**  The results are entirely unfaithful.  They don't contain any information from the provided PDF about the ETHOS typology.  The reference answer clearly defines the typology as a specific framework with categories and subcategories, while the RAG system responses offer unrelated factors.\n",
      "\n",
      "* **Relevance:** The results are irrelevant. The question explicitly asks about the ETHOS typology.  While the listed factors (job loss, lack of affordable housing, mental health issues) are related to the broader topic of homelessness, they do not answer the question asked.  They represent a different level of analysis—causes of homelessness rather than a description of a specific classification system.\n",
      "\n",
      "* **Ordering:** The ordering is inconsequential because the results are completely irrelevant. The minor change in order between the standard and reranked results has no impact given the fundamental flaw in the response.\n",
      "\n",
      "\n",
      "**Specific Examples of Weaknesses:**\n",
      "\n",
      "* **Complete Absence of Typology Information:** The results lack any mention of the four conceptual categories (rooflessness, houselessness, insecure housing, inadequate housing), the thirteen operational categories, or the twenty-four living situations specified in the reference answer and presumably present in the PDF.\n",
      "\n",
      "* **Irrelevant Information Provided:** The response gives factors contributing to homelessness, which is a different topic than the definition of a specific typology.  This shows a failure to understand the nuance of the question.\n",
      "\n",
      "\n",
      "**Strengths:** (There are none in this case.)  The RAG system completely fails to provide a relevant or faithful answer to the query.  The system didn't even identify keywords like \"ETHOS\" or \"Typology\" within the PDF to retrieve the relevant section.\n",
      "\n",
      "\n",
      "**In summary:** The RAG system's performance on this task is extremely poor. It demonstrates a failure to accurately retrieve and process information relevant to the question from the given PDF.  The system needs significant improvement in its ability to understand the semantics of the question and to accurately extract relevant information from the document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 0) Initialize Gemini client\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1) Load your validation JSON\n",
    "with open('/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "query = data[0]['question']\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# 2) Here's the revised evaluate_methods function for Gemini\n",
    "def evaluate_methods(pdf_path: str, query: str, reference_answer: str, standard_results: List[str], reranked_results: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates reranking results using Gemini.\n",
    "    \n",
    "    Args:\n",
    "    pdf_path (str): Path to the document.\n",
    "    query (str): The user's question.\n",
    "    reference_answer (str): The ideal answer.\n",
    "    standard_results (List[str]): Results before reranking.\n",
    "    reranked_results (List[str]): Results after reranking.\n",
    "    \n",
    "    Returns:\n",
    "    str: A detailed analysis of the results.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an objective evaluator of RAG system responses. \"\n",
    "        \"Provide a detailed analysis of how the two result sets compare against the reference answer, \"\n",
    "        \"with specific examples.\"\n",
    "    )\n",
    "    \n",
    "    comparison_text = (\n",
    "        f\"Based on the RAG pipeline’s answers to:\\n\"\n",
    "        f\"  • PDF: {pdf_path}\\n\"\n",
    "        f\"  • Question: {query}\\n\\n\"\n",
    "        f\"And comparing against the reference:\\n\"\n",
    "        f\"  {reference_answer}\\n\\n\"\n",
    "        f\"Standard Results:\\n{standard_results}\\n\\n\"\n",
    "        f\"Reranked Results:\\n{reranked_results}\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"{comparison_text}\\n\\n\"\n",
    "        \"Please compare faithfulness, relevance, and ordering—point out specific strengths and weaknesses.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
    "        resp = gemini_model.generate_content(user_prompt)\n",
    "        return resp.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        return \"Evaluation failed due to an error.\"\n",
    "\n",
    "# 3) Simulate RAG pipeline outputs\n",
    "standard_results = [\"lack of affordable housing\", \"job loss\", \"mental health issues\"]\n",
    "llm_results = [\"job loss\", \"lack of affordable housing\", \"mental health issues\"]\n",
    "\n",
    "# 4) Call the function\n",
    "results = evaluate_methods(\n",
    "    pdf_path=pdf_path,\n",
    "    query=query,\n",
    "    reference_answer=reference_answer,\n",
    "    standard_results=standard_results,\n",
    "    reranked_results=llm_results\n",
    ")\n",
    "\n",
    "# 5) Print\n",
    "print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
