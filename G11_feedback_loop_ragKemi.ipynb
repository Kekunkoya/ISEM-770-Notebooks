{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Feedback Loop in RAG\n",
    "\n",
    "In this notebook, I implement a RAG system with a feedback loop mechanism that continuously improves over time. By collecting and incorporating user feedback, our system learns to provide more relevant and higher-quality responses with each interaction.\n",
    "\n",
    "Traditional RAG systems are static - they retrieve information based solely on embedding similarity. With a feedback loop, we create a dynamic system that:\n",
    "\n",
    "- Remembers what worked (and what didn't)\n",
    "- Adjusts document relevance scores over time\n",
    "- Incorporates successful Q&A pairs into its knowledge base\n",
    "- Gets smarter with each user interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import _datetime\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Use a context manager to automatically close the document\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page to extract text\n",
    "            for page in mypdf:\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "We'll create a basic vector store to manage document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \n",
    "    This class provides an in-memory storage and retrieval system for \n",
    "    embedding vectors and their corresponding text chunks and metadata.\n",
    "    It supports basic similarity search functionality using cosine similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store with empty lists for vectors, texts, and metadata.\n",
    "        \n",
    "        The vector store maintains three parallel lists:\n",
    "        - vectors: NumPy arrays of embedding vectors\n",
    "        - texts: Original text chunks corresponding to each vector\n",
    "        - metadata: Optional metadata dictionaries for each item\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []    # List to store original text chunks\n",
    "        self.metadata = [] # List to store metadata for each text chunk\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "            text (str): The original text chunk to store.\n",
    "            embedding (List[float]): The embedding vector representing the text.\n",
    "            metadata (dict, optional): Additional metadata for the text chunk,\n",
    "                                      such as source, timestamp, or relevance scores.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert and store the embedding\n",
    "        self.texts.append(text)                   # Store the original text\n",
    "        self.metadata.append(metadata or {})      # Store metadata (empty dict if None)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector to compare against stored vectors.\n",
    "            k (int): Number of most similar results to return.\n",
    "            filter_func (callable, optional): Function to filter results based on metadata.\n",
    "                                             Takes metadata dict as input and returns boolean.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items, each containing:\n",
    "                - text: The original text\n",
    "                - metadata: Associated metadata\n",
    "                - similarity: Raw cosine similarity score\n",
    "                - relevance_score: Either metadata-based relevance or calculated similarity\n",
    "                \n",
    "        Note: Returns empty list if no vectors are stored or none pass the filter.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if vector store is empty\n",
    "        \n",
    "        # Convert query embedding to numpy array for vector operations\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate cosine similarity between query and each stored vector\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Skip items that don't pass the filter criteria\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity: dot product / (norm1 * norm2)\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Store index and similarity score\n",
    "        \n",
    "        # Sort results by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Construct result dictionaries for the top k matches\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score,\n",
    "                # Use pre-existing relevance score from metadata if available, otherwise use similarity\n",
    "                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for single text (first 5 values): [0.052571062, -0.03685706, -0.06520665, -0.04034025, 0.038206574]\n",
      "\n",
      "Number of embeddings for list: 2\n",
      "First embedding in list (first 5 values): [0.07521696, -0.034325134, -0.039195377, -0.008227663, 0.10222888]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text or list of texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API can handle both single strings and lists of strings\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=text\n",
    "        )\n",
    "        \n",
    "        # If the input was a single string, the response has a single embedding.\n",
    "        # If the input was a list, the response is a list of embeddings.\n",
    "        return response['embedding']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Create an embedding for a single string\n",
    "    single_text = \"Homelessness is a complex social issue.\"\n",
    "    embedding = create_embeddings(single_text)\n",
    "    print(f\"Embedding for single text (first 5 values): {embedding[:5]}\")\n",
    "    \n",
    "    # Example 2: Create embeddings for a list of strings\n",
    "    list_of_texts = [\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"Social factors also play a role in homelessness.\"\n",
    "    ]\n",
    "    embeddings_list = create_embeddings(list_of_texts)\n",
    "    print(f\"\\nNumber of embeddings for list: {len(embeddings_list)}\")\n",
    "    print(f\"First embedding in list (first 5 values): {embeddings_list[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback System Functions\n",
    "Now we'll implement the core feedback system components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    \"\"\"\n",
    "    Format user feedback in a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's query\n",
    "        response (str): System's response\n",
    "        relevance (int): Relevance score (1-5)\n",
    "        quality (int): Quality score (1-5)\n",
    "        comments (str): Optional feedback comments\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Formatted feedback\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance),\n",
    "        \"quality\": int(quality),\n",
    "        \"comments\": comments,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    Store feedback in a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        feedback (Dict): Feedback data\n",
    "        feedback_file (str): Path to feedback file\n",
    "    \"\"\"\n",
    "    with open(feedback_file, \"a\") as f:\n",
    "        json.dump(feedback, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    Load feedback data from file.\n",
    "    \n",
    "    Args:\n",
    "        feedback_file (str): Path to feedback file\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of feedback entries\n",
    "    \"\"\"\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(feedback_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    \n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing with Feedback Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG (Retrieval Augmented Generation) with feedback loop.\n",
    "    This function handles the complete document processing pipeline:\n",
    "    1. Text extraction from PDF\n",
    "    2. Text chunking with overlap\n",
    "    3. Embedding creation for chunks\n",
    "    4. Storage in vector database with metadata\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file to process.\n",
    "    chunk_size (int): Size of each text chunk in characters.\n",
    "    chunk_overlap (int): Number of overlapping characters between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: A tuple containing:\n",
    "        - List of document chunks\n",
    "        - Populated vector store with embeddings and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Extract raw text content from the PDF document\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Step 2: Split text into manageable, overlapping chunks for better context preservation\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Step 3: Generate vector embeddings for each text chunk\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Step 4: Initialize the vector database to store chunks and their embeddings\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Step 5: Add each chunk with its embedding to the vector store\n",
    "    # Include metadata for feedback-based improvements\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                # Position in original document\n",
    "                \"source\": pdf_path,        # Source document path\n",
    "                \"relevance_score\": 1.0,    # Initial relevance score (will be updated with feedback)\n",
    "                \"feedback_count\": 0        # Counter for feedback received on this chunk\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Adjustment Based on Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feedback_relevance(query, doc_text, feedback):\n",
    "    \"\"\"\n",
    "    Use LLM to assess if a past feedback entry is relevant to the current query and document.\n",
    "    \n",
    "    This function helps determine which past feedback should influence the current retrieval\n",
    "    by sending the current query, past query+feedback, and document content to an LLM\n",
    "    for relevance assessment.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Current user query that needs information retrieval\n",
    "        doc_text (str): Text content of the document being evaluated\n",
    "        feedback (Dict): Previous feedback data containing 'query' and 'response' keys\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the feedback is deemed relevant to current query/document, False otherwise\n",
    "    \"\"\"\n",
    "    # Define system prompt instructing the LLM to make binary relevance judgments only\n",
    "    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n",
    "    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n",
    "\n",
    "    # Construct user prompt with current query, past feedback data, and truncated document content\n",
    "    user_prompt = f\"\"\"\n",
    "    Current query: {query}\n",
    "    Past query that received feedback: {feedback['query']}\n",
    "    Document content: {doc_text[:500]}... [truncated]\n",
    "    Past response that received feedback: {feedback['response'][:500]}... [truncated]\n",
    "\n",
    "    Is this past feedback relevant to the current query and document? (yes/no)\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM API with zero temperature for deterministic output\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  # Use temperature=0 for consistent, deterministic responses\n",
    "    )\n",
    "    \n",
    "    # Extract and normalize the response to determine relevance\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    return 'yes' in answer  # Return True if the answer contains 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_relevance_scores(query, results, feedback_data):\n",
    "    \"\"\"\n",
    "    Adjust document relevance scores based on historical feedback to improve retrieval quality.\n",
    "    \n",
    "    This function analyzes past user feedback to dynamically adjust the relevance scores of \n",
    "    retrieved documents. It identifies feedback that is relevant to the current query context,\n",
    "    calculates score modifiers based on relevance ratings, and re-ranks the results accordingly.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Current user query\n",
    "        results (List[Dict]): Retrieved documents with their original similarity scores\n",
    "        feedback_data (List[Dict]): Historical feedback containing user ratings\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Results with adjusted relevance scores, sorted by the new scores\n",
    "    \"\"\"\n",
    "    # If no feedback data available, return original results unchanged\n",
    "    if not feedback_data:\n",
    "        return results\n",
    "    \n",
    "    print(\"Adjusting relevance scores based on feedback history...\")\n",
    "    \n",
    "    # Process each retrieved document\n",
    "    for i, result in enumerate(results):\n",
    "        document_text = result[\"text\"]\n",
    "        relevant_feedback = []\n",
    "        \n",
    "        # Find relevant feedback for this specific document and query combination\n",
    "        # by querying the LLM to assess relevance of each historical feedback item\n",
    "        for feedback in feedback_data:\n",
    "            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n",
    "            if is_relevant:\n",
    "                relevant_feedback.append(feedback)\n",
    "        \n",
    "        # Apply score adjustments if relevant feedback exists\n",
    "        if relevant_feedback:\n",
    "            # Calculate average relevance rating from all applicable feedback entries\n",
    "            # Feedback relevance is on a 1-5 scale (1=not relevant, 5=highly relevant)\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            \n",
    "            # Convert the average relevance to a score modifier in range 0.5-1.5\n",
    "            # - Scores below 3/5 will reduce the original similarity (modifier < 1.0)\n",
    "            # - Scores above 3/5 will increase the original similarity (modifier > 1.0)\n",
    "            modifier = 0.5 + (avg_relevance / 5.0)\n",
    "            \n",
    "            # Apply the modifier to the original similarity score\n",
    "            original_score = result[\"similarity\"]\n",
    "            adjusted_score = original_score * modifier\n",
    "            \n",
    "            # Update the result dictionary with new scores and feedback metadata\n",
    "            result[\"original_similarity\"] = original_score  # Preserve the original score\n",
    "            result[\"similarity\"] = adjusted_score           # Update the primary score\n",
    "            result[\"relevance_score\"] = adjusted_score      # Update the relevance score\n",
    "            result[\"feedback_applied\"] = True               # Flag that feedback was applied\n",
    "            result[\"feedback_count\"] = len(relevant_feedback)  # Number of feedback entries used\n",
    "            \n",
    "            # Log the adjustment details\n",
    "            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n",
    "    \n",
    "    # Re-sort results by adjusted scores to ensure higher quality matches appear first\n",
    "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Our Index with Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(current_store, chunks, feedback_data):\n",
    "    \"\"\"\n",
    "    Enhance vector store with high-quality feedback to improve retrieval quality over time.\n",
    "    \n",
    "    This function implements a continuous learning process by:\n",
    "    1. Identifying high-quality feedback (highly rated Q&A pairs)\n",
    "    2. Creating new retrieval items from successful interactions\n",
    "    3. Adding these to the vector store with boosted relevance weights\n",
    "    \n",
    "    Args:\n",
    "        current_store (SimpleVectorStore): Current vector store containing original document chunks\n",
    "        chunks (List[str]): Original document text chunks \n",
    "        feedback_data (List[Dict]): Historical user feedback with relevance and quality ratings\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: Enhanced vector store containing both original chunks and feedback-derived content\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning index with high-quality feedback...\")\n",
    "    \n",
    "    # Filter for only high-quality responses (both relevance and quality rated 4 or 5)\n",
    "    # This ensures we only learn from the most successful interactions\n",
    "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    if not good_feedback:\n",
    "        print(\"No high-quality feedback found for fine-tuning.\")\n",
    "        return current_store  # Return original store unchanged if no good feedback exists\n",
    "    \n",
    "    # Initialize new store that will contain both original and enhanced content\n",
    "    new_store = SimpleVectorStore()\n",
    "    \n",
    "    # First transfer all original document chunks with their existing metadata\n",
    "    for i in range(len(current_store.texts)):\n",
    "        new_store.add_item(\n",
    "            text=current_store.texts[i],\n",
    "            embedding=current_store.vectors[i],\n",
    "            metadata=current_store.metadata[i].copy()  # Use copy to prevent reference issues\n",
    "        )\n",
    "    \n",
    "    # Create and add enhanced content from good feedback\n",
    "    for feedback in good_feedback:\n",
    "        # Format a new document that combines the question and its high-quality answer\n",
    "        # This creates retrievable content that directly addresses user queries\n",
    "        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n",
    "        \n",
    "        # Generate embedding vector for this new synthetic document\n",
    "        embedding = create_embeddings(enhanced_text)\n",
    "        \n",
    "        # Add to vector store with special metadata that identifies its origin and importance\n",
    "        new_store.add_item(\n",
    "            text=enhanced_text,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"type\": \"feedback_enhanced\",  # Mark as derived from feedback\n",
    "                \"query\": feedback[\"query\"],   # Store original query for reference\n",
    "                \"relevance_score\": 1.2,       # Boost initial relevance to prioritize these items\n",
    "                \"feedback_count\": 1,          # Track feedback incorporation\n",
    "                \"original_feedback\": feedback # Preserve complete feedback record\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n",
    "    \n",
    "    # Log summary statistics about the enhancement\n",
    "    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n",
    "    return new_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, a key factor contributing to homelessness is the lack of affordable housing.  This disproportionately impacts low-income families and individuals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text from retrieved documents\n",
    "        model (str): LLM model to use\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based only on the context above.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        \n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and context from a previous step\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    context = \"Homelessness is a complex social problem. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, context)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing query with feedback-enhanced RAG ===\n",
      "Query: What causes homelessness?\n",
      "Generating response...\n",
      "\n",
      "=== Response ===\n",
      "Based on the provided text, a key factor in homelessness is the lack of affordable housing, which disproportionately affects low-income families.  The text also states that homelessness is a complex problem with economic, social, and personal factors, but it does not elaborate on those factors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_embeddings(text: str, model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"Creates an embedding for text using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"Generate a response based on the query and context using Gemini.\"\"\"\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If the context doesn't contain the answer, say so clearly.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"Placeholder vector store class for a runnable example.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if np.linalg.norm(query_vector) == 0 or np.linalg.norm(vector) == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append({\"text\": self.texts[i], \"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "def adjust_relevance_scores(query, results, feedback_data):\n",
    "    \"\"\"\n",
    "    Placeholder function to adjust relevance scores.\n",
    "    In a real scenario, this would use feedback data to boost or penalize scores.\n",
    "    \"\"\"\n",
    "    return results\n",
    "\n",
    "# --- 2. Your original `rag_with_feedback_loop` function (revised) ---\n",
    "def rag_with_feedback_loop(query: str, vector_store: SimpleVectorStore, feedback_data: List[Dict], k: int = 5, model: str = \"gemini-1.5-flash\") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline incorporating feedback loop.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    query_embedding = create_embeddings(query)\n",
    "    if not query_embedding:\n",
    "        return {\"error\": \"Failed to create query embedding.\"}\n",
    "    \n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
    "    \n",
    "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": adjusted_results,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Response ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 3. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a vector store with chunks and embeddings\n",
    "    store = SimpleVectorStore()\n",
    "    docs = [\n",
    "        \"Homelessness is a complex social problem with economic, social, and personal factors.\",\n",
    "        \"A key factor is the lack of affordable housing, which disproportionately affects low-income families.\",\n",
    "        \"The sun is the star at the center of our solar system.\"\n",
    "    ]\n",
    "    for i, doc in enumerate(docs):\n",
    "        embedding = create_embeddings(doc)\n",
    "        if embedding:\n",
    "            store.add_item(doc, embedding, {\"id\": i})\n",
    "\n",
    "    # Simulate feedback data\n",
    "    feedback = [\n",
    "        {\"query\": \"affordable housing\", \"doc_id\": 1, \"relevance_feedback\": \"positive\"},\n",
    "        {\"query\": \"sun\", \"doc_id\": 2, \"relevance_feedback\": \"negative\"}\n",
    "    ]\n",
    "    \n",
    "    user_query = \"What causes homelessness?\"\n",
    "    rag_with_feedback_loop(user_query, store, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow: From Initial Setup to Feedback Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"valh.json\", fine_tune=False):\n",
    "    \"\"\"\n",
    "    Execute a complete RAG workflow with feedback integration for continuous improvement.\n",
    "    \n",
    "    This function orchestrates the entire Retrieval-Augmented Generation process:\n",
    "    1. Load historical feedback data\n",
    "    2. Process and chunk the document\n",
    "    3. Optionally fine-tune the vector index with prior feedback\n",
    "    4. Perform retrieval and generation with feedback-adjusted relevance scores\n",
    "    5. Collect new user feedback for future improvement\n",
    "    6. Store feedback to enable system learning over time\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document to be processed\n",
    "        query (str): User's natural language query\n",
    "        feedback_data (List[Dict], optional): Pre-loaded feedback data, loads from file if None\n",
    "        feedback_file (str): Path to the JSON file storing feedback history\n",
    "        fine_tune (bool): Whether to enhance the index with successful past Q&A pairs\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results containing the response and retrieval metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Load historical feedback for relevance adjustment if not explicitly provided\n",
    "    if feedback_data is None:\n",
    "        feedback_data = load_feedback_data(feedback_file)\n",
    "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
    "    \n",
    "    # Step 2: Process document through extraction, chunking and embedding pipeline\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Step 3: Fine-tune the vector index by incorporating high-quality past interactions\n",
    "    # This creates enhanced retrievable content from successful Q&A pairs\n",
    "    if fine_tune and feedback_data:\n",
    "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    # Step 4: Execute core RAG with feedback-aware retrieval\n",
    "    # Note: This depends on the rag_with_feedback_loop function which should be defined elsewhere\n",
    "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "    \n",
    "    # Step 5: Collect user feedback to improve future performance\n",
    "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
    "    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n",
    "    relevance = input()\n",
    "    \n",
    "    print(\"Rate quality (1-5, with 5 being highest quality):\")\n",
    "    quality = input()\n",
    "    \n",
    "    print(\"Any comments? (optional, press Enter to skip)\")\n",
    "    comments = input()\n",
    "    \n",
    "    # Step 6: Format feedback into structured data\n",
    "    feedback = get_user_feedback(\n",
    "        query=query,\n",
    "        response=result[\"response\"],\n",
    "        relevance=int(relevance),\n",
    "        quality=int(quality),\n",
    "        comments=comments\n",
    "    )\n",
    "    \n",
    "    # Step 7: Persist feedback to enable continuous system learning\n",
    "    store_feedback(feedback, feedback_file)\n",
    "    print(\"Feedback recorded. Thank you!\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Full RAG Workflow ===\n",
      "Loaded 0 feedback entries from feedback_data.json\n",
      "\n",
      "=== Would you like to provide feedback on this response? ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration and Helper Functions ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def create_embeddings(texts: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"Creates embeddings for text using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append({\"text\": self.texts[i], \"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "def load_feedback_data(feedback_file: str) -> List[Dict]:\n",
    "    \"\"\"Loads feedback data from a JSON file.\"\"\"\n",
    "    if os.path.exists(feedback_file):\n",
    "        with open(feedback_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def store_feedback(feedback: Dict, feedback_file: str):\n",
    "    \"\"\"Stores a new feedback entry to a JSON file.\"\"\"\n",
    "    feedback_data = load_feedback_data(feedback_file)\n",
    "    feedback_data.append(feedback)\n",
    "    with open(feedback_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(feedback_data, f, indent=2)\n",
    "\n",
    "def process_document(pdf_path: str) -> Tuple[List[str], SimpleVectorStore]:\n",
    "    \"\"\"A placeholder for your PDF processing and vector store creation.\"\"\"\n",
    "    chunks = [\n",
    "        \"Homelessness is a complex social problem. It has multiple causes.\",\n",
    "        \"A lack of affordable housing is a major contributing factor.\",\n",
    "        \"Past successful interventions involved providing supportive services.\",\n",
    "        \"User feedback is crucial for improving RAG systems over time.\"\n",
    "    ]\n",
    "    vector_store = SimpleVectorStore()\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = create_embeddings(chunk)\n",
    "        if embedding:\n",
    "            vector_store.add_item(chunk, embedding, metadata={\"index\": i})\n",
    "    return chunks, vector_store\n",
    "\n",
    "def fine_tune_index(vector_store, chunks, feedback_data):\n",
    "    \"\"\"A placeholder for fine-tuning the index based on feedback.\"\"\"\n",
    "    print(\"Fine-tuning index with feedback data...\")\n",
    "    return vector_store\n",
    "\n",
    "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model=\"gemini-1.5-flash\"):\n",
    "    \"\"\"\n",
    "    A placeholder RAG pipeline using Gemini and a simulated feedback adjustment.\n",
    "    \"\"\"\n",
    "    query_embedding = create_embeddings(query)\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_texts = [result[\"text\"] for result in results]\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Generate response using Gemini\n",
    "    system_prompt = \"You are a helpful assistant. Answer the user's question based only on the provided context.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return {\"response\": response.text, \"retrieved_documents\": results}\n",
    "    except Exception as e:\n",
    "        print(f\"Response generation error: {e}\")\n",
    "        return {\"response\": \"Error\", \"retrieved_documents\": results}\n",
    "\n",
    "def get_user_feedback(query, response, relevance, quality, comments):\n",
    "    \"\"\"Placeholder to format feedback.\"\"\"\n",
    "    return {\"query\": query, \"response\": response, \"relevance\": relevance, \"quality\": quality, \"comments\": comments}\n",
    "\n",
    "# --- 2. Your original `full_rag_workflow` function ---\n",
    "def full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
    "    \"\"\"\n",
    "    Execute a complete RAG workflow with feedback integration for continuous improvement.\n",
    "    \"\"\"\n",
    "    print(\"=== Starting Full RAG Workflow ===\")\n",
    "    \n",
    "    # Step 1: Load historical feedback data\n",
    "    if feedback_data is None:\n",
    "        feedback_data = load_feedback_data(feedback_file)\n",
    "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
    "    \n",
    "    # Step 2: Process document\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Step 3: Fine-tune the vector index\n",
    "    if fine_tune and feedback_data:\n",
    "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    # Step 4: Execute core RAG with feedback-aware retrieval\n",
    "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "    \n",
    "    # Step 5: Collect user feedback\n",
    "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
    "    relevance = input(\"Rate relevance (1-5, with 5 being most relevant): \")\n",
    "    quality = input(\"Rate quality (1-5, with 5 being highest quality): \")\n",
    "    comments = input(\"Any comments? (optional, press Enter to skip): \")\n",
    "    \n",
    "    # Step 6: Format feedback into structured data\n",
    "    feedback = get_user_feedback(\n",
    "        query=query,\n",
    "        response=result[\"response\"],\n",
    "        relevance=int(relevance),\n",
    "        quality=int(quality),\n",
    "        comments=comments\n",
    "    )\n",
    "    \n",
    "    # Step 7: Persist feedback\n",
    "    \n",
    "    store_feedback(feedback, feedback_file)\n",
    "    print(\"Feedback recorded. Thank you!\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 3. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf\"\n",
    "    user_query = \"What are the key factors of homelessness?\"\n",
    "    \n",
    "    full_rag_workflow(pdf_file_path, user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Evaluate the impact of feedback loop on RAG quality by comparing performance before and after feedback integration.\n",
    "    \n",
    "    This function runs a controlled experiment to measure how incorporating feedback affects retrieval and generation:\n",
    "    1. First round: Run all test queries with no feedback\n",
    "    2. Generate synthetic feedback based on reference answers (if provided)\n",
    "    3. Second round: Run the same queries with feedback-enhanced retrieval\n",
    "    4. Compare results between rounds to quantify feedback impact\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document used as the knowledge base\n",
    "        test_queries (List[str]): List of test queries to evaluate system performance\n",
    "        reference_answers (List[str], optional): Reference/gold standard answers for evaluation\n",
    "                                                and synthetic feedback generation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results containing:\n",
    "            - round1_results: Results without feedback\n",
    "            - round2_results: Results with feedback\n",
    "            - comparison: Quantitative comparison metrics between rounds\n",
    "    \"\"\"\n",
    "    print(\"=== Evaluating Feedback Loop Impact ===\")\n",
    "    \n",
    "    # Create a temporary feedback file for this evaluation session only\n",
    "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
    "    \n",
    "    # Initialize feedback collection (empty at the start)\n",
    "    feedback_data = []\n",
    "    \n",
    "    # ----------------------- FIRST EVALUATION ROUND -----------------------\n",
    "    # Run all queries without any feedback influence to establish baseline performance\n",
    "    print(\"\\n=== ROUND 1: NO FEEDBACK ===\")\n",
    "    round1_results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # Process document to create initial vector store\n",
    "        chunks, vector_store = process_document(pdf_path)\n",
    "        \n",
    "        # Execute RAG without feedback influence (empty feedback list)\n",
    "        result = rag_with_feedback_loop(query, vector_store, [])\n",
    "        round1_results.append(result)\n",
    "        \n",
    "        # Generate synthetic feedback if reference answers are available\n",
    "        # This simulates user feedback for training the system\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            # Calculate synthetic feedback scores based on similarity to reference answer\n",
    "            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n",
    "            # Convert similarity (0-1) to rating scale (1-5)\n",
    "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            \n",
    "            # Create structured feedback entry\n",
    "            feedback = get_user_feedback(\n",
    "                query=query,\n",
    "                response=result[\"response\"],\n",
    "                relevance=relevance,\n",
    "                quality=quality,\n",
    "                comments=f\"Synthetic feedback based on reference similarity: {similarity_to_ref:.2f}\"\n",
    "            )\n",
    "            \n",
    "            # Add to in-memory collection and persist to temporary file\n",
    "            feedback_data.append(feedback)\n",
    "            store_feedback(feedback, temp_feedback_file)\n",
    "    \n",
    "    # ----------------------- SECOND EVALUATION ROUND -----------------------\n",
    "    # Run the same queries with feedback incorporation to measure improvement\n",
    "    print(\"\\n=== ROUND 2: WITH FEEDBACK ===\")\n",
    "    round2_results = []\n",
    "    \n",
    "    # Process document and enhance with feedback-derived content\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # Execute RAG with feedback influence\n",
    "        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "        round2_results.append(result)\n",
    "    \n",
    "    # ----------------------- RESULTS ANALYSIS -----------------------\n",
    "    # Compare performance metrics between the two rounds\n",
    "    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n",
    "    \n",
    "    # Clean up temporary evaluation artifacts\n",
    "    if os.path.exists(temp_feedback_file):\n",
    "        os.remove(temp_feedback_file)\n",
    "    \n",
    "    return {\n",
    "        \"round1_results\": round1_results,\n",
    "        \"round2_results\": round2_results,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between two texts using embeddings.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): First text\n",
    "        text2 (str): Second text\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Generate embeddings for both texts\n",
    "    embedding1 = create_embeddings(text1)\n",
    "    embedding2 = create_embeddings(text2)\n",
    "    \n",
    "    # Convert embeddings to numpy arrays\n",
    "    vec1 = np.array(embedding1)\n",
    "    vec2 = np.array(embedding2)\n",
    "    \n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare results from two rounds of RAG.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): Test queries\n",
    "        round1_results (List[Dict]): Results from round 1\n",
    "        round2_results (List[Dict]): Results from round 2\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPARING RESULTS ===\")\n",
    "    \n",
    "    # System prompt to guide the AI's evaluation behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from two versions:\n",
    "        1. Standard RAG: No feedback used\n",
    "        2. Feedback-enhanced RAG: Uses a feedback loop to improve retrieval\n",
    "\n",
    "        Analyze which version provides better responses in terms of:\n",
    "        - Relevance to the query\n",
    "        - Accuracy of information\n",
    "        - Completeness\n",
    "        - Clarity and conciseness\n",
    "    \"\"\"\n",
    "\n",
    "    comparisons = []\n",
    "    \n",
    "    # Iterate over each query and its corresponding results from both rounds\n",
    "    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n",
    "        # Create a prompt for comparing the responses\n",
    "        comparison_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Standard RAG Response:\n",
    "        {r1[\"response\"]}\n",
    "\n",
    "        Feedback-enhanced RAG Response:\n",
    "        {r2[\"response\"]}\n",
    "        \"\"\"\n",
    "\n",
    "        # Include reference answer if available\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            comparison_prompt += f\"\"\"\n",
    "            Reference Answer:\n",
    "            {reference_answers[i]}\n",
    "            \"\"\"\n",
    "\n",
    "        comparison_prompt += \"\"\"\n",
    "        Compare these responses and explain which one is better and why.\n",
    "        Focus specifically on how the feedback loop has (or hasn't) improved the response quality.\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the OpenAI API to generate a comparison analysis\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": comparison_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Append the comparison analysis to the results\n",
    "        comparisons.append({\n",
    "            \"query\": query,\n",
    "            \"analysis\": response.choices[0].message.content\n",
    "        })\n",
    "        \n",
    "        # Print a snippet of the analysis for each query\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        print(f\"Analysis: {response.choices[0].message.content[:200]}...\")\n",
    "    \n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the feedback loop (Custom Validation Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document to create initial vector store...\n",
      "\n",
      "Query 1: Why is a single number inadequate for understanding homelessness?\n",
      "\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import fitz\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Helper functions (must be defined before being called) ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    all_text = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for i in range(0, len(text), step):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(texts: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append({\"text\": self.texts[i], \"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    system_prompt = \"You are a helpful assistant. Answer based only on the provided context.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Response generation error: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def rag_with_feedback_loop(query: str, vector_store: SimpleVectorStore, feedback_data: List[Dict]) -> Dict:\n",
    "    query_embedding = create_embeddings(query)\n",
    "    results = vector_store.similarity_search(query_embedding, k=5)\n",
    "    retrieved_texts = [result[\"text\"] for result in results]\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    response = generate_response(query, context)\n",
    "    return {\"response\": response}\n",
    "\n",
    "def process_document(pdf_path: str, chunk_size: int = 800) -> Tuple[List[str], SimpleVectorStore]:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text, chunk_size, 0)\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    vector_store = SimpleVectorStore()\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        vector_store.add_item(chunk, embedding, metadata={\"id\": i})\n",
    "    return chunks, vector_store\n",
    "\n",
    "def evaluate_feedback_loop(pdf_path: str, test_queries: List[str], reference_answers: List[str]) -> List[Dict]:\n",
    "    evaluation_results = []\n",
    "    print(\"Processing document to create initial vector store...\")\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        result = rag_with_feedback_loop(query, vector_store, [])\n",
    "        evaluation_results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": result[\"response\"],\n",
    "            \"reference_answer\": reference_answers[i] if i < len(reference_answers) else \"N/A\"\n",
    "        })\n",
    "    return evaluation_results\n",
    "\n",
    "# --- Main Logic ---\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "test_queries = [\"Why is a single number inadequate for understanding homelessness?\"]\n",
    "reference_answers = [\"Section How many homeless people are there? and discussion of stock, flow, and prevalence figures\"]\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = evaluate_feedback_loop(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feedback Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SELECTIVE EVALUATION ---\n",
      "The RAG answer is completely unfaithful and irrelevant to the provided context.  The context is an abstract discussing the standardization of homelessness definitions across Europe, not the policy objectives themselves.  The RAG answer fabricates policy objectives (\"subsidized housing in newspapers,\" \"predictive AI for prevention,\" etc.) that are not mentioned or implied in the abstract.\n",
      "\n",
      "**Faithfulness:**  0/5 - The answer hallucinates information entirely unrelated to the provided text.\n",
      "\n",
      "**Relevance:** 0/5 - The answer does not address the question based on the provided context.  The context offers no information on policy objectives, and the answer does not attempt to connect to the limited information provided.\n",
      "\n",
      "The provided \"Reference Answer\" is also a fabrication and should not be used as a benchmark for evaluating the RAG response.  A proper reference answer would acknowledge that the provided text does not contain information on policy objectives and state that explicitly.\n",
      "\n",
      "\n",
      "--- SUMMARY EVALUATION ---\n",
      "**Faithfulness:** The RAG answer is completely unfaithful.  It provides a \"dummy\" answer and does not reflect any information present in the provided context. The context discusses homelessness prevalence in the US and highlights the need for further research in Europe, offering no insight into European homelessness policies or their objectives.\n",
      "\n",
      "**Relevance:** The RAG answer is irrelevant.  While the question asks about European homelessness strategies, the provided context is about US homelessness data and the need for European research.  The dummy answer provides unrelated examples (subsidized housing, AI prediction, supportive programs), which are plausible policy objectives but have no basis in the given context.  The RAG system failed to appropriately use the context and instead generated fabricated content.\n",
      "\n",
      "**Overall:** The RAG output is completely unsatisfactory.  It demonstrates a failure to extract relevant information and a lack of understanding that the context provided is insufficient to answer the query.  The generation of a \"dummy\" answer is unacceptable, as it lacks transparency and misrepresents the system's capability.\n",
      "\n",
      "\n",
      "--- EXTRACTION EVALUATION ---\n",
      "The RAG answer is unfaithful and irrelevant to the provided context.  The context consists of two documents: a UNECE/Eurostat recommendation for population and housing censuses, and a research paper on measuring homelessness within the Polish prison system.  Neither document provides information on typical policy objectives in European homelessness strategies.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "* **Faithfulness:** The RAG answer is completely unfaithful. It hallucinates information about policy objectives (subsidized housing, predictive AI, supportive programs) that are not present in the source documents.\n",
      "\n",
      "* **Relevance:** The RAG answer is irrelevant because the provided source documents do not contain the relevant information.  The answer is essentially fabricated.  A relevant answer would have stated that the provided context does not contain information to answer the question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "import fitz # For the PDF extraction stub\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Helper functions (revised for Gemini-compatibility) ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def compress_text(full_text: str, method: str) -> str:\n",
    "    \"\"\"Stub for text compression.\"\"\"\n",
    "    length = len(full_text)\n",
    "    if method == \"selective\": return full_text[:500]\n",
    "    elif method == \"summary\": return full_text[max(0, (length - 500) // 2):max(0, (length - 500) // 2)+500]\n",
    "    elif method == \"extraction\": return full_text[-500:]\n",
    "    return full_text\n",
    "\n",
    "def run_rag_pipeline(context: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Stub for RAG pipeline that returns a dummy answer.\n",
    "    \"\"\"\n",
    "    return f\"Dummy RAG answer for query: '{query}' based on provided context.\"\n",
    "\n",
    "# --- The main evaluation function (revised for Gemini) ---\n",
    "def evaluate_compression(pdf_path: str, query: str, reference_answer: str, compression_types: list[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Applies each compression method, runs RAG, and evaluates against a reference using Gemini.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Bug fix: Use the pdf_path argument instead of a hardcoded path\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    for ctype in compression_types:\n",
    "        compressed_ctx = compress_text(full_text, method=ctype)\n",
    "        rag_answer = run_rag_pipeline(compressed_ctx, query)\n",
    "\n",
    "        system_prompt = \"You are an objective evaluator of RAG outputs.\"\n",
    "        user_prompt = (\n",
    "            f\"Compression type: {ctype}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Context:\\n{compressed_ctx}\\n\\n\"\n",
    "            f\"RAG Answer: {rag_answer}\\n\\n\"\n",
    "            f\"Reference Answer: {reference_answer}\\n\\n\"\n",
    "            \"Evaluate for faithfulness and relevance. Provide details.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Create a Gemini model instance with the system prompt\n",
    "            resp = genai.GenerativeModel(\n",
    "                \"gemini-1.5-flash\",\n",
    "                system_instruction=system_prompt\n",
    "            ).generate_content(user_prompt)\n",
    "            results[ctype] = resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation for {ctype}: {e}\")\n",
    "            results[ctype] = \"Evaluation failed due to an error.\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'\")\n",
    "        exit()\n",
    "\n",
    "    query = \"What are typical policy objectives in European homelessness strategies?\"\n",
    "    reference_answer = \"\"\"\n",
    "Section Setting concrete targets\n",
    "listing of subsidized housing in the newspapers\n",
    "prevention reduction of homelessness through predictive AI\n",
    "supportive programs through workplace, hospitals and churches\n",
    "\"\"\"\n",
    "    compression_types = [\"selective\", \"summary\", \"extraction\"]\n",
    "\n",
    "    results = evaluate_compression(\n",
    "        pdf_path=pdf_path,\n",
    "        query=query,\n",
    "        reference_answer=reference_answer,\n",
    "        compression_types=compression_types\n",
    "    )\n",
    "\n",
    "    for ctype, eval_text in results.items():\n",
    "        print(f\"\\n--- {ctype.upper()} EVALUATION ---\\n{eval_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEEDBACK IMPACT ANALYSIS ===\n",
      "\n",
      "Query 1: What are the main causes of homelessness?\n",
      "\n",
      "Analysis of feedback impact:\n",
      "The RAG pipeline in round 2 provided a more complete answer by including economic issues, showing the impact of the feedback loop.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response length comparison (proxy for completeness):\n",
      "Round 1: 55.0 chars\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random # Used for simulating data\n",
    "\n",
    "# Simulate the output of a Gemini-powered RAG pipeline\n",
    "# In a real scenario, this dictionary would be generated by your main evaluation function\n",
    "evaluation_results = {\n",
    "    'round1_results': [\n",
    "        {'response': 'Homelessness is a complex social problem with various contributing factors.'},\n",
    "        {'response': 'Affordable housing is a key factor.'}\n",
    "    ],\n",
    "    'round2_results': [\n",
    "        {'response': 'Homelessness is a complex social problem caused by a lack of affordable housing and job loss.'},\n",
    "        {'response': 'A lack of affordable housing and economic issues are key factors.'}\n",
    "    ],\n",
    "    'comparison': [\n",
    "        {\n",
    "            'query': 'What are the main causes of homelessness?',\n",
    "            'analysis': \"The RAG pipeline in round 2 provided a more complete answer by including economic issues, showing the impact of the feedback loop.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add some dummy responses to simulate a real scenario\n",
    "avg_len_round1 = sum(len(r['response']) for r in evaluation_results['round1_results']) / len(evaluation_results['round1_results'])\n",
    "avg_len_round2 = sum(len(r['response']) for r in evaluation_results['round2_results']) / len(evaluation_results['round2_results'])\n",
    "evaluation_results['round1_results'][0]['response_length'] = len(evaluation_results['round1_results'][0]['response'])\n",
    "evaluation_results['round1_results'][1]['response_length'] = len(evaluation_results['round1_results'][1]['response'])\n",
    "evaluation_results['round2_results'][0]['response_length'] = len(evaluation_results['round2_results'][0]['response'])\n",
    "evaluation_results['round2_results'][1]['response_length'] = len(evaluation_results['round2_results'][1]['response'])\n",
    "\n",
    "\n",
    "# Extract the comparison data which contains the analysis of feedback impact\n",
    "comparisons = evaluation_results['comparison']\n",
    "\n",
    "# Print out the analysis results to visualize feedback impact\n",
    "print(\"\\n=== FEEDBACK IMPACT ANALYSIS ===\\n\")\n",
    "for i, comparison in enumerate(comparisons):\n",
    "    print(f\"Query {i+1}: {comparison['query']}\")\n",
    "    print(f\"\\nAnalysis of feedback impact:\")\n",
    "    print(comparison['analysis'])\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Additionally, we can compare some metrics between rounds\n",
    "round_responses = [evaluation_results[f'round{round_num}_results'] for round_num in range(1, len(evaluation_results) - 1)]\n",
    "response_lengths = [[len(r[\"response\"]) for r in round] for round in round_responses]\n",
    "\n",
    "print(\"\\nResponse length comparison (proxy for completeness):\")\n",
    "avg_lengths = [sum(lengths) / len(lengths) for lengths in response_lengths]\n",
    "for round_num, avg_len in enumerate(avg_lengths, start=1):\n",
    "    print(f\"Round {round_num}: {avg_len:.1f} chars\")\n",
    "\n",
    "if len(avg_lengths) > 1:\n",
    "    changes = [(avg_lengths[i] - avg_lengths[i-1]) / avg_lengths[i-1] * 100 for i in range(1, len(avg_lengths))]\n",
    "    for round_num, change in enumerate(changes, start=2):\n",
    "        print(f\"Change from Round {round_num-1} to Round {round_num}: {change:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing relevant feedback...\n",
      "Is feedback 1 relevant? True\n",
      "\n",
      "Assessing irrelevant feedback...\n",
      "Is feedback 2 relevant? False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main assessment function (revised for Gemini) ---\n",
    "def assess_feedback_relevance(query: str, doc_text: str, feedback: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the given feedback entry is relevant to the current query and document excerpt.\n",
    "    Handles different feedback field names gracefully.\n",
    "    \"\"\"\n",
    "    # Safely pull out the feedback text, trying multiple possible keys\n",
    "    feedback_content = (\n",
    "        feedback.get(\"feedback_text\")\n",
    "        or feedback.get(\"feedback\")\n",
    "        or feedback.get(\"comment\")\n",
    "        or str(feedback)\n",
    "    )\n",
    "\n",
    "    system_prompt = \"You are an objective evaluator. Answer 'yes' or 'no' only.\"\n",
    "    user_prompt = (\n",
    "        f\"Current query: {query}\\n\"\n",
    "        f\"Document excerpt (first 200 chars): {doc_text[:200]}...\\n\"\n",
    "        f\"Past feedback: {feedback_content}\\n\"\n",
    "        \"Is this past feedback relevant to the current query and document? (yes/no)\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create a Gemini model instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response\n",
    "        resp = gemini_model.generate_content(user_prompt)\n",
    "        answer = resp.text.strip().lower()\n",
    "        return answer.startswith(\"yes\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during relevance assessment: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query, document excerpt, and feedback\n",
    "    sample_query = \"What are the main causes of homelessness?\"\n",
    "    sample_doc_text = \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "    \n",
    "    # Simulate relevant feedback\n",
    "    relevant_feedback = {\"comment\": \"The response should mention economic factors more clearly.\"}\n",
    "    \n",
    "    # Simulate irrelevant feedback\n",
    "    irrelevant_feedback = {\"feedback\": \"The AI provided a good summary of the solar system.\"}\n",
    "\n",
    "    print(\"Assessing relevant feedback...\")\n",
    "    is_relevant_1 = assess_feedback_relevance(sample_query, sample_doc_text, relevant_feedback)\n",
    "    print(f\"Is feedback 1 relevant? {is_relevant_1}\")\n",
    "    \n",
    "    print(\"\\nAssessing irrelevant feedback...\")\n",
    "    is_relevant_2 = assess_feedback_relevance(sample_query, sample_doc_text, irrelevant_feedback)\n",
    "    print(f\"Is feedback 2 relevant? {is_relevant_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, a key factor contributing to homelessness is the lack of affordable housing.  This disproportionately impacts low-income families and individuals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text from retrieved documents\n",
    "        model (str): LLM model to use\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based only on the context above.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        \n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and context from a previous step\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    context = \"Homelessness is a complex social problem. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, context)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing RAG results for two rounds with Gemini...\n",
      "\n",
      "--- Analysis for Query: What are the main causes of homelessness? ---\n",
      "Round 2 is significantly better than Round 1.  The feedback loop has demonstrably improved the response quality.\n",
      "\n",
      "**Round 1's shortcomings:**\n",
      "\n",
      "* **Vague and lacks detail:**  \"Homelessness is a social problem\" is a tautology and offers no actionable insight.  The mention of affordable housing is correct but lacks the specificity and breadth of the reference answer.\n",
      "\n",
      "* **Doesn't fully answer the query:** The response is too simplistic and doesn't address the multiple contributing factors.\n",
      "\n",
      "**Round 2's improvements:**\n",
      "\n",
      "* **More comprehensive and accurate:**  It correctly identifies multiple key causes of homelessness: lack of affordable housing, economic issues, and personal crises (which encompasses job loss).  This aligns directly with the reference answer's key points.\n",
      "\n",
      "* **More specific and informative:**  It moves beyond general statements to provide more concrete factors contributing to homelessness.\n",
      "\n",
      "\n",
      "**Feedback Loop Effectiveness:**\n",
      "\n",
      "The feedback loop clearly worked.  The initial response was too general and superficial. The second response directly incorporates the missing information (economic issues and personal crises) suggested (implicitly or explicitly) by the reference answer, which presumably informed the feedback loop.  This demonstrates that the feedback mechanism successfully addressed the deficiencies in the first response, resulting in a much more complete and accurate answer in Round 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the comparison function for Gemini ---\n",
    "def compare_results(queries: List[str], round1_results: List[Dict], round2_results: List[Dict], reference_answers: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Compares two rounds of RAG outputs using a Gemini LLM.\n",
    "    \n",
    "    Args:\n",
    "    queries (List[str]): List of user queries.\n",
    "    round1_results (List[Dict]): Results from the first RAG round.\n",
    "    round2_results (List[Dict]): Results from the second RAG round.\n",
    "    reference_answers (List[str]): Ideal answers for each query.\n",
    "    \n",
    "    Returns:\n",
    "    List[Dict]: List of comparison analyses.\n",
    "    \"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    system_prompt = \"You are an objective analyst comparing two rounds of RAG outputs.\"\n",
    "    \n",
    "    try:\n",
    "        # Create a Gemini model instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Gemini model: {e}\")\n",
    "        return []\n",
    "    \n",
    "    for query, r1, r2, ref in zip(queries, round1_results, round2_results, reference_answers):\n",
    "        comparison_prompt = (\n",
    "            f\"Query: {query}\\n\\n\"\n",
    "            f\"Round 1 Answer: {r1['response']}\\n\\n\"\n",
    "            f\"Round 2 Answer: {r2['response']}\\n\\n\"\n",
    "            f\"Reference Answer: {ref}\\n\\n\"\n",
    "            \"Compare these responses and explain which one is better and why. \"\n",
    "            \"Focus specifically on how the feedback loop has (or hasn't) improved response quality.\"\n",
    "        )\n",
    "        \n",
    "        # Generate the comparison using the Gemini API\n",
    "        try:\n",
    "            resp = gemini_model.generate_content(comparison_prompt)\n",
    "            comparisons.append({\n",
    "                \"query\": query,\n",
    "                \"analysis\": resp.text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during comparison for query '{query}': {e}\")\n",
    "            comparisons.append({\n",
    "                \"query\": query,\n",
    "                \"analysis\": \"Comparison failed due to an error.\"\n",
    "            })\n",
    "            \n",
    "    return comparisons\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a set of queries and RAG results for two rounds\n",
    "    queries = [\"What are the main causes of homelessness?\"]\n",
    "    reference_answers = [\"A lack of affordable housing, economic issues, and personal crises.\"]\n",
    "    \n",
    "    round1_results = [{\"response\": \"Homelessness is a social problem. A lack of affordable housing is a key factor.\"}]\n",
    "    round2_results = [{\"response\": \"Homelessness is caused by a lack of affordable housing, economic issues, and personal crises like job loss.\"}]\n",
    "    \n",
    "    print(\"Comparing RAG results for two rounds with Gemini...\")\n",
    "    comparison_analysis = compare_results(queries, round1_results, round2_results, reference_answers)\n",
    "    \n",
    "    for comp in comparison_analysis:\n",
    "        print(f\"\\n--- Analysis for Query: {comp['query']} ---\")\n",
    "        print(comp['analysis'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
