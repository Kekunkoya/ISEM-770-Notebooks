{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814afd85",
   "metadata": {},
   "source": [
    "# Hierarchical Indices for RAG\n",
    "\n",
    "In this notebook, I implement a hierarchical indexing approach for RAG systems. This technique improves retrieval by using a two-tier search method: first identifying relevant document sections through summaries, then retrieving specific details from those sections.\n",
    "\n",
    "Traditional RAG approaches treat all text chunks equally, which can lead to:\n",
    "\n",
    "- Lost context when chunks are too small\n",
    "- Irrelevant results when the document collection is large\n",
    "- Inefficient searches across the entire corpus\n",
    "\n",
    "Hierarchical retrieval solves these problems by:\n",
    "\n",
    "- Creating concise summaries for larger document sections\n",
    "- First searching these summaries to identify relevant sections\n",
    "- Then retrieving detailed information only from those sections\n",
    "- Maintaining context while preserving specific details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfe6c7",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e8e3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup & Helpers ===\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# If you plan to use PDF extraction here:\n",
    "try:\n",
    "    from PyPDF2 import PdfReader\n",
    "except Exception as e:\n",
    "    print(\"PyPDF2 not available; please install it if you need PDF text extraction.\")\n",
    "    PdfReader = None\n",
    "\n",
    "# Gemini setup (correct usage: choose model at construction time)\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if GEMINI_API_KEY:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "except Exception as e:\n",
    "    print(\"google-generativeai not available; Gemini calls will fail unless installed and key set.\")\n",
    "    genai = None\n",
    "\n",
    "def gemini_text(prompt: str, model_name: str = \"gemini-1.5-pro\") -> str:\n",
    "    \"\"\"Correct Gemini usage: pick model in constructor, no model= kwarg in generate_content().\"\"\"\n",
    "    if genai is None:\n",
    "        return \"[Gemini not configured] \" + prompt[:200]\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    resp = model.generate_content(prompt)\n",
    "    return getattr(resp, \"text\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0deb8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Embeddings (stub) ===\n",
    "# Replace this with your real embedder (OpenAI, Gemini Embeddings, etc.).\n",
    "def create_embeddings(texts, model: str = None):\n",
    "    \"\"\"Accepts str or List[str]; returns List[np.ndarray] with fixed dim.\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    dim = 768\n",
    "    rng = np.random.default_rng(42)  # deterministic demo\n",
    "    return [rng.random(dim, dtype=float) for _ in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c4b26",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30ac9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Vector Store ===\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.embeddings: List[np.ndarray] = []\n",
    "        self.documents: List[Dict[str, Any]] = []  # each: {\"text\": str, \"metadata\": {...}}\n",
    "\n",
    "    def _as_unit_vector(self, x):\n",
    "        v = np.asarray(x, dtype=float).reshape(-1)\n",
    "        n = np.linalg.norm(v)\n",
    "        return v if n == 0 else v / n\n",
    "\n",
    "    def add_item(self, *, text: str, embedding: Union[np.ndarray, List[float]], metadata: Optional[Dict[str, Any]] = None):\n",
    "        emb = self._as_unit_vector(embedding)\n",
    "        self.embeddings.append(emb)\n",
    "        self.documents.append({\"text\": text, \"metadata\": (metadata or {})})\n",
    "\n",
    "    def similarity_search(self, query_embedding, k: int = 5, filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None, return_scores: bool = False):\n",
    "        if not self.embeddings:\n",
    "            return [] if not return_scores else []\n",
    "        q = self._as_unit_vector(query_embedding)\n",
    "        sims = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            doc = self.documents[i]\n",
    "            if filter_func is not None and not filter_func(doc):\n",
    "                continue\n",
    "            sim = float(np.dot(q, emb))   # cosine because normalized\n",
    "            sims.append((i, sim))\n",
    "        if not sims:\n",
    "            return [] if not return_scores else []\n",
    "        sims.sort(key=lambda t: t[1], reverse=True)\n",
    "        top = sims[:max(1, int(k))]\n",
    "        return [(self.documents[i], score) for i, score in top] if return_scores else [self.documents[i] for i, _ in top]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741da9e9",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cd44c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === PDF Processing ===\n",
    "def extract_text_pages(pdf_path: str) -> List[str]:\n",
    "    if PdfReader is None:\n",
    "        raise RuntimeError(\"PyPDF2 is not installed. Please install it to extract PDF text.\")\n",
    "    pages = []\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        for p in reader.pages:\n",
    "            pages.append(p.extract_text() or \"\")\n",
    "    return pages\n",
    "\n",
    "def chunk_text(text: str, chunk_size=1000, overlap=200) -> List[Tuple[str, int]]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append((text[start:end], start))\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(end - overlap, start + 1)\n",
    "    return chunks\n",
    "\n",
    "def process_document(pdf_path: str, chunk_size=1000, chunk_overlap=200) -> SimpleVectorStore:\n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    pages = extract_text_pages(pdf_path)\n",
    "    store = SimpleVectorStore()\n",
    "    all_chunks = []\n",
    "    for page_idx, page_text in enumerate(pages):\n",
    "        page_text = page_text.strip()\n",
    "        if not page_text:\n",
    "            continue\n",
    "        for chunk, start_idx in chunk_text(page_text, chunk_size, chunk_overlap):\n",
    "            all_chunks.append({\"text\": chunk, \"metadata\": {\"page\": page_idx, \"char_start\": start_idx}})\n",
    "    print(f\"Created {len(all_chunks)} text chunks\")\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings([c[\"text\"] for c in all_chunks], model=None)\n",
    "    for i, c in enumerate(all_chunks):\n",
    "        store.add_item(text=c[\"text\"], embedding=chunk_embeddings[i], metadata=c[\"metadata\"])\n",
    "    print(f\"Vector store created with {len(store.documents)} chunks\\n\")\n",
    "    return store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3676b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Generate Response (robust to strings or list of docs) ===\n",
    "def _normalize_sources(sources):\n",
    "    out, seen = [], set()\n",
    "    for s in (sources or []):\n",
    "        if isinstance(s, str):\n",
    "            v = s.strip()\n",
    "        elif isinstance(s, dict):\n",
    "            title = s.get(\"title\")\n",
    "            page = s.get(\"page\") or (s.get(\"metadata\") or {}).get(\"page\")\n",
    "            url  = s.get(\"url\")\n",
    "            bits = [b for b in [title, f\"p.{page}\" if page is not None else None, url] if b]\n",
    "            v = \" \".join(bits) if bits else \"source\"\n",
    "        else:\n",
    "            v = str(s)\n",
    "        if v and v not in seen:\n",
    "            seen.add(v)\n",
    "            out.append(v)\n",
    "    return out\n",
    "\n",
    "def generate_response(query: str, retrieved_docs, sources: list = None, llm_call=None, max_ctx_chars: int = 1500):\n",
    "    # Build context\n",
    "    if isinstance(retrieved_docs, str):\n",
    "        context = retrieved_docs[:max_ctx_chars]\n",
    "    else:\n",
    "        context = \"\"\n",
    "        for d in (retrieved_docs or []):\n",
    "            t = d.get(\"text\", \"\") if isinstance(d, dict) else str(d)\n",
    "            if not t:\n",
    "                continue\n",
    "            space_left = max_ctx_chars - len(context)\n",
    "            if space_left <= 0:\n",
    "                break\n",
    "            context += t[:space_left] + \"\\n\"\n",
    "\n",
    "    # Infer sources if not provided\n",
    "    if sources is None and not isinstance(retrieved_docs, str):\n",
    "        inferred = []\n",
    "        for d in (retrieved_docs or []):\n",
    "            if isinstance(d, dict):\n",
    "                meta = d.get(\"metadata\") or {}\n",
    "                page = meta.get(\"page\")\n",
    "                if page is not None:\n",
    "                    inferred.append(f\"p.{page}\")\n",
    "        sources = inferred\n",
    "\n",
    "    prompt = f\"\"\"Answer the user's question using ONLY the context.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{context.strip()}\n",
    "\n",
    "If the context is thin, say what's missing and answer conservatively. End with a 'Sources:' line when available.\n",
    "\"\"\"\n",
    "\n",
    "    text = (llm_call(prompt).strip() if llm_call else \"Here’s what the document indicates:\\n\" + context.strip())\n",
    "    src_list = _normalize_sources(sources)\n",
    "    if src_list:\n",
    "        text += \"\\n\\nSources: \" + \", \".join(src_list)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bd90f",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Simple CRAG & Standard RAG (demo versions) ===\n",
    "def rag_retrieve(store: SimpleVectorStore, query: str, k: int = 8):\n",
    "    q_emb = create_embeddings(query)[0]\n",
    "    return store.similarity_search(q_emb, k=k, return_scores=False)\n",
    "\n",
    "def crag_retrieve(store: SimpleVectorStore, query: str, k_initial: int = 12, k_final: int = 6):\n",
    "    # initial retrieve\n",
    "    q_emb = create_embeddings(query)[0]\n",
    "    initial_docs = store.similarity_search(q_emb, k=k_initial, return_scores=True)\n",
    "\n",
    "    # naive re-rank: prefer shorter chunks slightly (toy heuristic)\n",
    "    rescored = []\n",
    "    for doc, score in initial_docs:\n",
    "        length_penalty = 0.05 * (len(doc.get(\"text\", \"\")) / 1000.0)\n",
    "        rescored.append((doc, score - length_penalty))\n",
    "    rescored.sort(key=lambda t: t[1], reverse=True)\n",
    "    return [d for d, _ in rescored[:k_final]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "200a7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Grading & Comparison helpers (Gemini) ===\n",
    "def grade_answer(query: str, answer: str, reference: str = \"\") -> str:\n",
    "    prompt = f\"\"\"You are grading an answer.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Reference (optional):\n",
    "{reference}\n",
    "\n",
    "Give a brief justification and a 0-100 score as 'Score: NN'.\n",
    "\"\"\"\n",
    "    return gemini_text(prompt)\n",
    "\n",
    "def compare_responses(query: str, crag_answer: str, rag_answer: str) -> str:\n",
    "    prompt = f\"\"\"Compare two answers and decide which is better.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer A (CRAG):\n",
    "{crag_answer}\n",
    "\n",
    "Answer B (Standard RAG):\n",
    "{rag_answer}\n",
    "\n",
    "Assess correctness, grounding, completeness, and clarity.\n",
    "Return rationale and a final line exactly: 'Winner: A', 'Winner: B', or 'Winner: Tie'.\n",
    "\"\"\"\n",
    "    return gemini_text(prompt)\n",
    "\n",
    "def _extract_score(text: str) -> float:\n",
    "    m = re.search(r\"Score:\\s*([0-9]+(?:\\.[0-9]+)?)\", text or \"\", flags=re.I)\n",
    "    return float(m.group(1)) if m else 0.0\n",
    "\n",
    "def _extract_winner(text: str) -> str:\n",
    "    t = (text or \"\").strip().lower()\n",
    "    if \"winner: a\" in t:\n",
    "        return \"A\"\n",
    "    if \"winner: b\" in t:\n",
    "        return \"B\"\n",
    "    return \"Tie\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4688e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Full evaluation runner ===\n",
    "def run_crag_evaluation(pdf_path: str, test_queries: List[str], reference_answers: Optional[List[str]] = None):\n",
    "    store = process_document(pdf_path)\n",
    "\n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n===== Evaluating Query {i+1}/{len(test_queries)} =====\\nQuery: {query}\\n\")\n",
    "\n",
    "        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else \"\"\n",
    "\n",
    "        print(\"=== Running CRAG ===\")\n",
    "        crag_docs = crag_retrieve(store, query)\n",
    "        crag_answer = generate_response(query, crag_docs, llm_call=lambda p: gemini_text(p, \"gemini-1.5-pro\"))\n",
    "\n",
    "        print(\"\\n=== Running standard RAG ===\")\n",
    "        rag_docs = rag_retrieve(store, query)\n",
    "        rag_answer = generate_response(query, rag_docs, llm_call=lambda p: gemini_text(p, \"gemini-1.5-pro\"))\n",
    "\n",
    "        print(\"\\n=== Evaluating CRAG response ===\")\n",
    "        crag_grade = grade_answer(query, crag_answer, reference)\n",
    "        print(crag_grade)\n",
    "\n",
    "        print(\"\\n=== Evaluating standard RAG response ===\")\n",
    "        rag_grade = grade_answer(query, rag_answer, reference)\n",
    "        print(rag_grade)\n",
    "\n",
    "        print(\"\\n=== Comparing approaches ===\")\n",
    "        cmp_txt = compare_responses(query, crag_answer, rag_answer)\n",
    "        print(cmp_txt)\n",
    "\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"crag_answer\": crag_answer,\n",
    "            \"rag_answer\": rag_answer,\n",
    "            \"crag_score\": _extract_score(crag_grade),\n",
    "            \"rag_score\": _extract_score(rag_grade),\n",
    "            \"winner\": _extract_winner(cmp_txt)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Summarize\n",
    "    avg_crag = float(np.mean([r[\"crag_score\"] for r in results])) if results else 0.0\n",
    "    avg_rag  = float(np.mean([r[\"rag_score\"]  for r in results])) if results else 0.0\n",
    "    verdict  = \"CRAG\" if avg_crag > avg_rag else (\"Standard RAG\" if avg_rag > avg_crag else \"Tie\")\n",
    "\n",
    "    overall_analysis = f\"Avg(CRAG)={avg_crag:.3f}, Avg(RAG)={avg_rag:.3f} → {verdict}\"\n",
    "    return {\"per_query\": results, \"overall_analysis\": overall_analysis}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb4afa",
   "metadata": {},
   "source": [
    "## Evaluation of Hierarchical and Standard RAG Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b575a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: define create_embeddings if missing ---\n",
    "import numpy as np\n",
    "\n",
    "def create_embeddings(texts, model: str = None):\n",
    "    \"\"\"\n",
    "    Accepts a string or list of strings and returns a list of 1D numpy arrays.\n",
    "    Keep the 'model' param for compatibility with upstream calls.\n",
    "    Replace this with your real embedder when ready.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    dim = 768\n",
    "    rng = np.random.default_rng(42)  # deterministic for testing\n",
    "    return [rng.random(dim, dtype=float) for _ in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbea9b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /Users/kekunkoya/Desktop/770 Google /AI_Information.pdf...\n",
      "Created 46 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Vector store created with 46 chunks\n",
      "\n",
      "\n",
      "===== Evaluating Query 1/1 =====\n",
      "Query: How does machine learning differ from traditional programming?\n",
      "\n",
      "=== Running CRAG ===\n",
      "\n",
      "=== Running standard RAG ===\n",
      "\n",
      "=== Evaluating CRAG response ===\n",
      "Justification: The answer admits to not answering the question and instead points out what information is missing from the provided source.  It does not attempt to answer the question based on general knowledge.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "=== Evaluating standard RAG response ===\n",
      "Justification: The answer claims the provided source doesn't contain relevant information and fails to answer the question.  It even lists specific pages as sources, implying they were consulted, but then states they lack the necessary information.  This suggests either the sources *were* relevant but the answerer didn't understand them, or the answerer didn't actually consult the sources.  Either way, the provided answer is completely inadequate.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "=== Comparing approaches ===\n",
      "* **Correctness:** Both answers correctly state that the provided document doesn't contain the necessary information to answer the question.  Neither answer hallucinates.\n",
      "\n",
      "* **Grounding:** Both answers explicitly state that the information is missing from the source document.  The page numbers provided don't help in evaluation as we don't have the document itself.  However, both present the sources they used.\n",
      "\n",
      "* **Completeness:** Both answers provide the necessary information - acknowledging the inability to answer the question due to missing information.  Answer A phrases it slightly better by saying \"Information about machine learning, traditional programming, and a comparison of the two is missing,\" directly addressing all parts of the question. Answer B is close, but slightly less precise.\n",
      "\n",
      "* **Clarity:** Both answers are clearly written and easy to understand.  Answer A's structure is marginally more logical, flowing from stating what the document *does* provide to what it's *missing*.\n",
      "\n",
      "\n",
      "Rationale: Answer A is slightly better due to being slightly more complete and clearly structured. Both correctly identify the lack of information, but A is more precise in explaining what's missing.\n",
      "\n",
      "Winner: A\n",
      "\n",
      "=== Overall Analysis of CRAG vs Standard RAG ===\n",
      "Avg(CRAG)=0.000, Avg(RAG)=0.000 → Tie\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\n",
    "print(\"\\n=== Overall Analysis of CRAG vs Standard RAG ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f870e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /Users/kekunkoya/Desktop/770 Google /AI_Information.pdf...\n",
      "Created 46 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Vector store created with 46 chunks\n",
      "\n",
      "\n",
      "===== Evaluating Query 1/1 =====\n",
      "Query: How does machine learning differ from traditional programming?\n",
      "\n",
      "=== Running CRAG ===\n",
      "\n",
      "=== Running standard RAG ===\n",
      "\n",
      "=== Evaluating CRAG response ===\n",
      "Justification: The answer admits it doesn't address the question and the provided \"sources\" are meaningless without context.  While the optional reference provides the correct distinction, the answer itself fails to convey this information.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "=== Evaluating standard RAG response ===\n",
      "Justification: The answer acknowledges the inability to answer the question based on the provided document and correctly states that more information is needed. However, it lists source page numbers that are irrelevant and do not seem to connect to the non-existent explanation.  While acknowledging limitations is good, citing irrelevant sources detracts from the response. The existence of a reference section suggests the user *could* have answered the question correctly.\n",
      "\n",
      "Score: 10\n",
      "\n",
      "=== Comparing approaches ===\n",
      "Both answers correctly state that the provided context is insufficient to answer the question.  They both also correctly identify the need for more information on machine learning and traditional programming.\n",
      "\n",
      "* **Correctness:** Both are correct.\n",
      "* **Grounding:** Both appropriately cite sources, although it's unclear what these page numbers refer to without seeing the document. The ordering of sources doesn't impact the assessment in this case.\n",
      "* **Completeness:** Both answers provide a complete response given the limitations of the source material.\n",
      "* **Clarity:** Answer B is slightly clearer due to its more concise and direct phrasing. \"I cannot answer your question\" is more helpful than \"More information is needed to answer your question.\"  Both adequately explain *why* they can't answer.\n",
      "\n",
      "Rationale: Both answers acknowledge the lack of information and correctly state the need for more context.  However, Answer B's phrasing is slightly clearer and more helpful to the user.\n",
      "\n",
      "Winner: B\n",
      "\n",
      "=== Overall Analysis of CRAG vs Standard RAG ===\n",
      "Avg(CRAG)=0.000, Avg(RAG)=10.000 → Standard RAG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Demo ---\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/770 Google /AI_Information.pdf\"\n",
    "\n",
    "test_queries = [\n",
    "    \"How does machine learning differ from traditional programming?\",\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"Machine learning differs from traditional programming by having computers learn patterns from data rather than following explicit instructions. In traditional programming, developers write rules; in ML, models learn from examples.\"\n",
    "]\n",
    "\n",
    "evaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\n",
    "print(\"\\n=== Overall Analysis of CRAG vs Standard RAG ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
