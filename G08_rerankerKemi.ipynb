{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Reranking for Enhanced RAG Systems\n",
    "\n",
    "This notebook implements reranking techniques to improve retrieval quality in RAG systems. Reranking acts as a second filtering step after initial retrieval to ensure the most relevant content is used for response generation.\n",
    "\n",
    "## Key Concepts of Reranking\n",
    "\n",
    "1. **Initial Retrieval**: First pass using basic similarity search (less accurate but faster)\n",
    "2. **Document Scoring**: Evaluating each retrieved document's relevance to the query\n",
    "3. **Reordering**: Sorting documents by their relevance scores\n",
    "4. **Selection**: Using only the most relevant documents for response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Use a context manager to automatically close the document\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page to extract text\n",
    "            for page in mypdf:\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return all_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Text extraction complete.\n",
      "\n",
      "Extracted Text (first 500 characters):\n",
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">> Abstract_ Substantial progress has been made at EU level on defining home-\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this \n",
      "typology. The development and some of th...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Your PDF text extraction function ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        # Use a context manager to ensure the file is closed properly\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            # Iterate through each page in the PDF\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# --- 2. Gemini API Configuration (for a complete workflow) ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 3. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \"/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf\"\n",
    "\n",
    "    # Verify that the PDF file exists before proceeding\n",
    "    if not os.path.exists(pdf_file):\n",
    "        print(f\"Error: PDF file not found at '{pdf_file}'\")\n",
    "        exit()\n",
    "\n",
    "    # Step A: Extract text from the PDF\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    print(\"Text extraction complete.\")\n",
    "\n",
    "    # Step B: Print the extracted text\n",
    "    if text:\n",
    "        # Print the first 500 characters to verify\n",
    "        print(\"\\nExtracted Text (first 500 characters):\")\n",
    "        print(text[:500] + \"...\")\n",
    "    else:\n",
    "        print(\"No text was extracted from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how reranking integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for single text (first 5 values): [0.052571062, -0.03685706, -0.06520665, -0.04034025, 0.038206574]\n",
      "\n",
      "Number of embeddings for list: 2\n",
      "First embedding in list (first 5 values): [0.07521696, -0.034325134, -0.039195377, -0.008227663, 0.10222888]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text or list of texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: A list of embedding vectors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API can handle both single strings and lists of strings\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=text\n",
    "        )\n",
    "        \n",
    "        # The API returns a dictionary with a single key 'embedding'\n",
    "        # The value is a list of lists (for multiple texts) or a single list (for one text)\n",
    "        if isinstance(text, str):\n",
    "            # If the input was a single string, return the single embedding vector\n",
    "            return response['embedding']\n",
    "        else:\n",
    "            # If the input was a list, return the list of embedding vectors\n",
    "            return response['embedding']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Create an embedding for a single string\n",
    "    single_text = \"Homelessness is a complex social issue.\"\n",
    "    embedding = create_embeddings(single_text)\n",
    "    print(f\"Embedding for single text (first 5 values): {embedding[:5]}\")\n",
    "    \n",
    "    # Example 2: Create embeddings for a list of strings\n",
    "    list_of_texts = [\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"Social factors also play a role in homelessness.\"\n",
    "    ]\n",
    "    embeddings_list = create_embeddings(list_of_texts)\n",
    "    print(f\"\\nNumber of embeddings for list: {len(embeddings_list)}\")\n",
    "    print(f\"First embedding in list (first 5 values): {embeddings_list[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline\n",
    "Now that we have defined the necessary functions and classes, we can proceed to define the document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf('/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf')\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize a simple vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LLM-based Reranking\n",
    "Let's implement the LLM-based reranking function using the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking 5 documents...\n",
      "Scoring document 1/5...\n",
      "\n",
      "Final Reranked Results (Top 2):\n",
      "Rank 1 (Score: 6.0): A key factor is the lack of affordable housing.\n",
      "Rank 2 (Score: 6.0): Economic factors like job loss contribute to homelessness.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the reranking function for Gemini ---\n",
    "def rerank_with_llm(query: str, results: List[Dict], top_n: int = 3, model: str = \"gemini-1.5-flash\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Reranks search results using LLM relevance scoring.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        model (str): Model to use for scoring\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    print(f\"Reranking {len(results)} documents...\")\n",
    "    \n",
    "    scored_results = []\n",
    "    \n",
    "    # Define the system prompt for the LLM\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\n",
    "Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
    "\n",
    "Guidelines:\n",
    "- Score 0-2: Document is completely irrelevant\n",
    "- Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
    "- Score 6-8: Document is relevant and partially answers the query\n",
    "- Score 9-10: Document is highly relevant and directly answers the query\n",
    "\n",
    "You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n",
    "    \n",
    "    # Create the Gemini model instance once with the system prompt\n",
    "    try:\n",
    "        llm_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Gemini model: {e}\")\n",
    "        return []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
    "        \n",
    "        # Define the user prompt for the LLM\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Document:\n",
    "{result['text']}\n",
    "\n",
    "Rate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Get the LLM response\n",
    "            response = llm_model.generate_content(user_prompt)\n",
    "            \n",
    "            # Extract the score from the LLM response\n",
    "            score_text = response.text.strip()\n",
    "            score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
    "            \n",
    "            if score_match:\n",
    "                score = float(score_match.group(1))\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
    "                score = result.get(\"similarity\", 0) * 10\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scoring API call for document {i+1}: {e}\")\n",
    "            score = result.get(\"similarity\", 0) * 10 # Fallback in case of API error\n",
    "        \n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result.get(\"similarity\", 0),\n",
    "            \"relevance_score\": score\n",
    "        })\n",
    "    \n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    return reranked_results[:top_n]\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate initial search results\n",
    "    initial_results = [\n",
    "        {\"text\": \"A key factor is the lack of affordable housing.\", \"metadata\": {\"id\": 1}, \"similarity\": 0.85},\n",
    "        {\"text\": \"The sun is the center of our solar system.\", \"metadata\": {\"id\": 2}, \"similarity\": 0.70},\n",
    "        {\"text\": \"Economic factors like job loss contribute to homelessness.\", \"metadata\": {\"id\": 3}, \"similarity\": 0.90},\n",
    "        {\"text\": \"Jupiter is the largest planet.\", \"metadata\": {\"id\": 4}, \"similarity\": 0.65},\n",
    "        {\"text\": \"Government organizations provide shelters and support.\", \"metadata\": {\"id\": 5}, \"similarity\": 0.75}\n",
    "    ]\n",
    "    query = \"What are the economic causes of homelessness?\"\n",
    "\n",
    "    # Rerank the results\n",
    "    final_results = rerank_with_llm(query, initial_results, top_n=2)\n",
    "\n",
    "    # Print the reranked results\n",
    "    print(\"\\nFinal Reranked Results (Top 2):\")\n",
    "    for i, res in enumerate(final_results):\n",
    "        print(f\"Rank {i+1} (Score: {res['relevance_score']}): {res['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Keyword-based Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_keywords(query, results, top_n=3):\n",
    "    \"\"\"\n",
    "    A simple alternative reranking method based on keyword matching and position.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    # Extract important keywords from the query\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    \n",
    "    scored_results = []  # Initialize a list to store scored results\n",
    "    \n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n",
    "        \n",
    "        # Base score starts with vector similarity\n",
    "        base_score = result[\"similarity\"] * 0.5\n",
    "        \n",
    "        # Initialize keyword score\n",
    "        keyword_score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in document_text:\n",
    "                # Add points for each keyword found\n",
    "                keyword_score += 0.1\n",
    "                \n",
    "                # Add more points if keyword appears near the beginning\n",
    "                first_position = document_text.find(keyword)\n",
    "                if first_position < len(document_text) / 4:  # In the first quarter of the text\n",
    "                    keyword_score += 0.1\n",
    "                \n",
    "                # Add points for keyword frequency\n",
    "                frequency = document_text.count(keyword)\n",
    "                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n",
    "        \n",
    "        # Calculate the final score by combining base score and keyword score\n",
    "        final_score = base_score + keyword_score\n",
    "        \n",
    "        # Append the scored result to the list\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # Sort results by final relevance score in descending order\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top_n results\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, a key factor contributing to homelessness is the lack of affordable housing.  This disproportionately impacts low-income families and individuals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): Model to use for response generation\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based only on the context above.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and context from a previous step\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    context = \"Homelessness is a complex social problem. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, context)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RAG Pipeline with Reranking\n",
    "So far, we have implemented the core components of the RAG pipeline, including document processing, question answering, and reranking. Now, we will combine these components to create a full RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Helper Functions (Assumed to be defined and configured for Gemini) ---\n",
    "# These functions replace your original OpenAI-based helpers.\n",
    "def create_embeddings(text: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        # Gemini returns a list of embeddings, even for a single text.\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "def rerank_with_llm(query: str, results: List[Dict], top_n: int = 3, model: str = \"gemini-1.5-flash\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Reranks search results using LLM relevance scoring from Gemini.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an expert at evaluating document relevance for search queries. Your task is to rate documents on a scale from 0 to 10. You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\n",
    "    \n",
    "    scored_results = []\n",
    "    \n",
    "    try:\n",
    "        llm_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        for result in results:\n",
    "            user_prompt = f\"Query: {query}\\n\\nDocument:\\n{result['text']}\\n\\nRate this document's relevance to the query on a scale from 0 to 10:\"\n",
    "            response = llm_model.generate_content(user_prompt)\n",
    "            score = float(response.text.strip())\n",
    "            scored_results.append({**result, \"relevance_score\": score})\n",
    "    except Exception as e:\n",
    "        print(f\"Reranking error: {e}\")\n",
    "        return []\n",
    "\n",
    "    return sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)[:top_n]\n",
    "\n",
    "def rerank_with_keywords(query: str, results: List[Dict], top_n: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    A simple keyword-based reranking method.\n",
    "    \"\"\"\n",
    "    # This function is already API-agnostic, no changes needed.\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    scored_results = []\n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()\n",
    "        keyword_score = sum([0.1 for keyword in keywords if keyword in document_text])\n",
    "        final_score = result.get(\"similarity\", 0) * 0.5 + keyword_score\n",
    "        scored_results.append({**result, \"relevance_score\": final_score})\n",
    "    return sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)[:top_n]\n",
    "\n",
    "\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context using Gemini.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nPlease provide a comprehensive answer based only on the context above.\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Response generation error: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Your main RAG pipeline function (revised) ---\n",
    "def rag_with_reranking(query: str, vector_store, reranking_method: str = \"llm\", top_n: int = 3, model: str = \"gemini-1.5-flash\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline incorporating reranking.\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding_list = create_embeddings(query)\n",
    "    if not query_embedding_list:\n",
    "        return {\"error\": \"Failed to create query embedding.\"}\n",
    "    query_embedding = query_embedding_list[0] # The first item is the single embedding\n",
    "\n",
    "    # Initial retrieval (get more than we need for reranking)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
    "    \n",
    "    # Apply reranking\n",
    "    if reranking_method == \"llm\":\n",
    "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n, model=model)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
    "    else:\n",
    "        reranked_results = initial_results[:top_n]\n",
    "    \n",
    "    # Combine context from reranked results\n",
    "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
    "    \n",
    "    # Generate response based on context\n",
    "    response = generate_response(query, context, model=model)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"reranking_method\": reranking_method,\n",
    "        \"initial_results\": initial_results[:top_n],\n",
    "        \"reranked_results\": reranked_results,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Reranking Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Load the validation data from a JSON file ---\n",
    "with open('/Users/kekunkoya/Desktop/770 Google /valh.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Extract the first query and reference answer ---\n",
    "query = data[0]['question']\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# --- Path to the PDF file for processing ---\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf\"\n",
    "\n",
    "# --- Example: For Google Gemini usage ---\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure your Google API Key\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# You can now pass `query`, `reference_answer`, and `pdf_path`\n",
    "# into your Gemini RAG or embedding pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import google.generativeai as genai\n",
    "\n",
    "GEMINI_MODEL = \"gemini-1.5-pro\"\n",
    "EMBED_MODEL  = \"text-embedding-004\"\n",
    "\n",
    "# ---- Tunables to prevent OOM ----\n",
    "MAX_PAGES     = 150        # hard cap pages read (raise if needed)\n",
    "CHUNK_SIZE    = 1000       # chars per chunk\n",
    "CHUNK_OVERLAP = 120\n",
    "CHUNK_LIMIT   = 1200       # hard cap number of chunks sent to embed\n",
    "BATCH_SIZE    = 8          # how many chunks to embed at a time\n",
    "RETRY_MAX     = 4\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, max_pages: int = MAX_PAGES) -> str:\n",
    "    out = []\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        total = min(len(reader.pages), max_pages)\n",
    "        for i in range(total):\n",
    "            t = reader.pages[i].extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                out.append(t)\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):\n",
    "    chunks, n = [], len(text)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        j = min(n, i + chunk_size)\n",
    "        chunk = text[i:j].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        # move forward with overlap\n",
    "        i = j - overlap\n",
    "        if i < 0:\n",
    "            i = 0\n",
    "        if i >= j:  # safety\n",
    "            i = j\n",
    "    return chunks\n",
    "\n",
    "def _embed_batch(batch_texts):\n",
    "    # robust single-call batch embed with retries\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            # embed_content supports a list via \"contents\"\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=batch_texts)\n",
    "            # response may be a dict with \"embedding\" per item or a list — normalize:\n",
    "            if isinstance(resp, dict) and \"embeddings\" in resp:\n",
    "                vecs = [e[\"values\"] for e in resp[\"embeddings\"]]\n",
    "            elif isinstance(resp, list):\n",
    "                vecs = [r[\"embedding\"][\"values\"] for r in resp]\n",
    "            else:\n",
    "                # sometimes single returns dict with \"embedding\"\n",
    "                if \"embedding\" in resp:\n",
    "                    vecs = [resp[\"embedding\"][\"values\"]]\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected embed response shape\")\n",
    "            return vecs\n",
    "        except Exception as e:\n",
    "            if attempt == RETRY_MAX:\n",
    "                raise\n",
    "            time.sleep(1.5 * attempt)\n",
    "\n",
    "def embed_texts_gemini(texts):\n",
    "    \"\"\"Memory-friendly, batched embedding with padding.\"\"\"\n",
    "    all_vecs = []\n",
    "    for start in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[start:start + BATCH_SIZE]\n",
    "        try:\n",
    "            vecs = _embed_batch(batch)\n",
    "        except Exception as e:\n",
    "            # fallback: zero-vectors for this batch length\n",
    "            vecs = None\n",
    "            print(f\"Embedding batch failed at {start}: {e}\")\n",
    "        if vecs is None:\n",
    "            # guess dim later; store placeholder\n",
    "            all_vecs.extend([[] for _ in batch])\n",
    "        else:\n",
    "            all_vecs.extend(vecs)\n",
    "\n",
    "    # find max dim and pad\n",
    "    maxd = max((len(v) for v in all_vecs), default=0)\n",
    "    if maxd == 0:\n",
    "        raise ValueError(\"All embeddings failed or returned empty.\")\n",
    "    padded = [(v + [0.0]*(maxd - len(v))) if len(v) < maxd else v for v in all_vecs]\n",
    "    return np.asarray(padded, dtype=np.float32)\n",
    "\n",
    "def process_document_gemini(pdf_path: str,\n",
    "                            chunk_size=CHUNK_SIZE,\n",
    "                            overlap=CHUNK_OVERLAP,\n",
    "                            chunk_limit=CHUNK_LIMIT):\n",
    "    text = extract_text_from_pdf(pdf_path, max_pages=MAX_PAGES)\n",
    "    if not text:\n",
    "        raise ValueError(\"No extractable text found in PDF (scanned image-only PDF?).\")\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "    if not chunks:\n",
    "        raise ValueError(\"Chunking produced no text.\")\n",
    "\n",
    "    # Hard cap to avoid OOM\n",
    "    if len(chunks) > chunk_limit:\n",
    "        print(f\"⚠️ Truncating chunks {len(chunks)} → {chunk_limit} to avoid OOM.\")\n",
    "        chunks = chunks[:chunk_limit]\n",
    "\n",
    "    # Embed in memory-safe batches\n",
    "    embeddings = embed_texts_gemini(chunks)\n",
    "\n",
    "    return {\"chunks\": chunks, \"embeddings\": embeddings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document with Gemini embeddings...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pip install google-generativeai PyPDF2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "GEMINI_MODEL = \"gemini-1.5-pro\"\n",
    "EMBED_MODEL  = \"text-embedding-004\"\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = []\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        for p in reader.pages:\n",
    "            txt = p.extract_text() or \"\"\n",
    "            text.append(txt)\n",
    "    # Basic clean\n",
    "    return re.sub(r\"[ \\t]+\", \" \", \"\\n\".join(text)).strip()\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150):\n",
    "    \"\"\"\n",
    "    Simple token-ish chunking by characters to avoid extra deps.\n",
    "    Tune sizes for your PDFs and model context window.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        j = min(n, i + chunk_size)\n",
    "        chunks.append(text[i:j])\n",
    "        i = j - overlap\n",
    "        if i < 0: i = 0\n",
    "    return [c.strip() for c in chunks if c.strip()]\n",
    "\n",
    "def embed_texts_gemini(texts):\n",
    "    # Google API accepts one item at a time for embed_content\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        try:\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=t)\n",
    "            vecs.append(resp[\"embedding\"][\"value\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding failed for a chunk: {e}\")\n",
    "            vecs.append([0.0])  # keep shape consistent later\n",
    "    # Pad to same length if anything went wrong\n",
    "    maxd = max(len(v) for v in vecs)\n",
    "    padded = [v + [0.0]*(maxd - len(v)) for v in vecs]\n",
    "    return np.array(padded, dtype=np.float32)\n",
    "\n",
    "def cosine_sim_matrix(a, b):\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "    return a_norm @ b_norm.T\n",
    "\n",
    "def keyword_overlap_score(query, text):\n",
    "    qs = set(re.findall(r\"\\b\\w+\\b\", query.lower()))\n",
    "    ts = set(re.findall(r\"\\b\\w+\\b\", text.lower()))\n",
    "    if not qs: return 0.0\n",
    "    return len(qs & ts) / math.sqrt(len(qs) * (len(ts) + 1e-9))\n",
    "\n",
    "# -----------------------------\n",
    "# Core: document processing\n",
    "# -----------------------------\n",
    "def process_document_gemini(pdf_path: str, chunk_size=1200, overlap=150):\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=overlap)\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No text extracted from PDF.\")\n",
    "    embeddings = embed_texts_gemini(chunks)\n",
    "    return {\n",
    "        \"chunks\": chunks,\n",
    "        \"embeddings\": embeddings,  # np.ndarray [num_chunks, dim]\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Reranking helpers\n",
    "# -----------------------------\n",
    "def retrieve_top_k(query, vector_store, k=6):\n",
    "    qvec = embed_texts_gemini([query])  # [1, dim]\n",
    "    sims = cosine_sim_matrix(qvec, vector_store[\"embeddings\"])[0]  # [num_chunks]\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "    return [(int(i), float(sims[i]), vector_store[\"chunks\"][i]) for i in top_idx]\n",
    "\n",
    "def llm_rerank_gemini(query, candidates):\n",
    "    \"\"\"\n",
    "    Ask Gemini to rank the candidate contexts by usefulness; returns same list re-ordered.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    numbered = \"\\n\\n\".join([f\"[{i}] {c[:1200]}\" for i, (_, _, c) in enumerate(candidates)])\n",
    "    prompt = f\"\"\"\n",
    "You are ranking context passages for answering a question. \n",
    "Question: {query}\n",
    "\n",
    "Passages:\n",
    "{numbered}\n",
    "\n",
    "Return a JSON list of passage indices in best-to-worst order, for example: [2,0,1,...]\n",
    "Only return the JSON array.\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = model.generate_content(prompt)\n",
    "        txt = resp.text.strip()\n",
    "        order = json.loads(txt)\n",
    "        # Validate indices\n",
    "        order = [o for o in order if isinstance(o, int) and 0 <= o < len(candidates)]\n",
    "        if len(order) != len(candidates):\n",
    "            # Fallback: keep originals if weird output\n",
    "            return candidates\n",
    "        return [candidates[i] for i in order]\n",
    "    except Exception as e:\n",
    "        print(f\"LLM rerank failed: {e}\")\n",
    "        return candidates\n",
    "\n",
    "def keyword_rerank(query, candidates):\n",
    "    scored = [(kw := keyword_overlap_score(query, c[2]), c) for c in candidates]\n",
    "    scored.sort(key=lambda x: -x[0])\n",
    "    return [c for _, c in scored]\n",
    "\n",
    "# -----------------------------\n",
    "# RAG pipeline\n",
    "# -----------------------------\n",
    "def rag_with_reranking_gemini(query, vector_store, reranking_method=\"none\", top_k=6, max_ctx_chars=3500):\n",
    "    # 1) retrieve by embeddings\n",
    "    candidates = retrieve_top_k(query, vector_store, k=top_k)\n",
    "\n",
    "    # 2) rerank if requested\n",
    "    if reranking_method == \"llm\":\n",
    "        candidates = llm_rerank_gemini(query, candidates)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        candidates = keyword_rerank(query, candidates)\n",
    "    # else \"none\" → leave as is\n",
    "\n",
    "    # 3) build context\n",
    "    context_parts = []\n",
    "    total = 0\n",
    "    for _, _, chunk in candidates:\n",
    "        if total + len(chunk) > max_ctx_chars: break\n",
    "        context_parts.append(chunk)\n",
    "        total += len(chunk)\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4) ask Gemini\n",
    "    system_msg = (\n",
    "        \"You are a careful research assistant. \"\n",
    "        \"Answer using ONLY the provided context. If the answer is not in the context, say you cannot find it.\"\n",
    "    )\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    try:\n",
    "        resp = model.generate_content([system_msg, user_prompt])\n",
    "        answer = resp.text.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"[Generation failed: {e}]\"\n",
    "\n",
    "    return {\n",
    "        \"response\": answer,\n",
    "        \"used_chunks\": [c[0] for c in candidates],\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage (drop-in)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "    print(\"Processing document with Gemini embeddings...\")\n",
    "    vector_store = process_document_gemini(pdf_path)\n",
    "\n",
    "    query = \"What measurement approaches are commonly used to collect data on homelessness in Europe?\"\n",
    "    print(\"Comparing retrieval methods...\")\n",
    "\n",
    "    print(\"\\n=== STANDARD RETRIEVAL ===\")\n",
    "    standard_results = rag_with_reranking_gemini(query, vector_store, reranking_method=\"none\")\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"\\nResponse:\\n{standard_results['response']}\")\n",
    "\n",
    "    print(\"\\n=== LLM-BASED RERANKING ===\")\n",
    "    llm_results = rag_with_reranking_gemini(query, vector_store, reranking_method=\"llm\")\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"\\nResponse:\\n{llm_results['response']}\")\n",
    "\n",
    "    print(\"\\n=== KEYWORD-BASED RERANKING ===\")\n",
    "    keyword_results = rag_with_reranking_gemini(query, vector_store, reranking_method=\"keywords\")\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"\\nResponse:\\n{keyword_results['response']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of reranked results compared to standard results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        standard_results (Dict): Results from standard retrieval\n",
    "        reranked_results (Dict): Results from reranked retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation output\n",
    "    \"\"\"\n",
    "    # Define the system prompt for the AI evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
    "    Compare the retrieved contexts and responses from two different retrieval methods.\n",
    "    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with truncated contexts and responses\n",
    "    comparison_text = f\"\"\"Query: {query}\n",
    "\n",
    "Standard Retrieval Context:\n",
    "{standard_results['context'][:1000]}... [truncated]\n",
    "\n",
    "Standard Retrieval Answer:\n",
    "{standard_results['response']}\n",
    "\n",
    "Reranked Retrieval Context:\n",
    "{reranked_results['context'][:1000]}... [truncated]\n",
    "\n",
    "Reranked Retrieval Answer:\n",
    "{reranked_results['response']}\"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the comparison text\n",
    "    if reference_answer:\n",
    "        comparison_text += f\"\"\"\n",
    "        \n",
    "Reference Answer:\n",
    "{reference_answer}\"\"\"\n",
    "\n",
    "    # Create the user prompt for the AI evaluator\n",
    "    user_prompt = f\"\"\"\n",
    "{comparison_text}\n",
    "\n",
    "Please evaluate which retrieval method provided:\n",
    "1. More relevant context\n",
    "2. More accurate answer\n",
    "3. More comprehensive answer\n",
    "4. Better overall performance\n",
    "\n",
    "Provide a detailed analysis with specific examples.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the evaluation output\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking_gemini(query, standard_results, reranked_results, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluates reranked vs standard results using Google Gemini.\n",
    "    Expects results dicts to contain 'context' and 'response'.\n",
    "    \"\"\"\n",
    "    # System + user prompts\n",
    "    system_prompt = (\n",
    "        \"You are an expert evaluator of RAG systems. \"\n",
    "        \"Compare the retrieved contexts and responses from two different retrieval methods. \"\n",
    "        \"Assess which one provides better context and a more accurate, comprehensive answer.\"\n",
    "    )\n",
    "\n",
    "    # Guard for missing keys\n",
    "    std_ctx = (standard_results.get(\"context\") or \"\")[:1000]\n",
    "    std_ans = standard_results.get(\"response\") or \"\"\n",
    "    rr_ctx  = (reranked_results.get(\"context\") or \"\")[:1000]\n",
    "    rr_ans  = reranked_results.get(\"response\") or \"\"\n",
    "\n",
    "    comparison_text = f\"\"\"Query: {query}\n",
    "\n",
    "Standard Retrieval Context:\n",
    "{std_ctx}... [truncated]\n",
    "\n",
    "Standard Retrieval Answer:\n",
    "{std_ans}\n",
    "\n",
    "Reranked Retrieval Context:\n",
    "{rr_ctx}... [truncated]\n",
    "\n",
    "Reranked Retrieval Answer:\n",
    "{rr_ans}\"\"\"\n",
    "\n",
    "    if reference_answer:\n",
    "        comparison_text += f\"\"\"\n",
    "\n",
    "Reference Answer:\n",
    "{reference_answer}\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"{comparison_text}\n",
    "\n",
    "Please evaluate which retrieval method provided:\n",
    "1. More relevant context\n",
    "2. More accurate answer\n",
    "3. More comprehensive answer\n",
    "4. Better overall performance\n",
    "\n",
    "Provide a detailed analysis with specific examples.\"\"\"\n",
    "\n",
    "    # Call Gemini\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            [{\"role\": \"user\", \"parts\": [system_prompt + \"\\n\\n\" + user_prompt]}],\n",
    "            generation_config={\"temperature\": 0}\n",
    "        )\n",
    "        return (resp.text or \"\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Evaluation failed: {e}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking_gemini(query, vector_store, reranking_method=\"none\", top_k=6, max_ctx_chars=3500):\n",
    "    # 1) retrieve by embeddings\n",
    "    candidates = retrieve_top_k(query, vector_store, k=top_k)\n",
    "\n",
    "    # 2) rerank if requested\n",
    "    if reranking_method == \"llm\":\n",
    "        candidates = llm_rerank_gemini(query, candidates)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        candidates = keyword_rerank(query, candidates)\n",
    "\n",
    "    # 3) build context\n",
    "    context_parts = []\n",
    "    total = 0\n",
    "    for _, _, chunk in candidates:\n",
    "        if total + len(chunk) > max_ctx_chars:\n",
    "            break\n",
    "        context_parts.append(chunk)\n",
    "        total += len(chunk)\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4) ask Gemini\n",
    "    system_msg = (\n",
    "        \"You are a careful research assistant. \"\n",
    "        \"Answer using ONLY the provided context. If the answer is not in the context, say you cannot find it.\"\n",
    "    )\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    try:\n",
    "        resp = model.generate_content([system_msg, user_prompt])\n",
    "        answer = resp.text.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"[Generation failed: {e}]\"\n",
    "\n",
    "    # ✅ Proper return inside the function\n",
    "    return {\n",
    "        \"response\": answer,\n",
    "        \"used_chunks\": [c[0] for c in candidates],\n",
    "        \"context\": context  # added so evaluate_reranking_gemini works\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "## Analysis of Result Sets Compared to Reference Answer\n",
      "\n",
      "Both the standard and reranked result sets perform similarly well against the reference answer in terms of faithfulness and relevance, but differ slightly in ordering.  Let's break down each aspect:\n",
      "\n",
      "**Faithfulness:**\n",
      "\n",
      "* **Standard Results:**  Faithfully reflects the key points in the reference answer.  \"Lack of affordable housing,\" \"job loss,\" and \"mental health issues\" are all directly mentioned or implied as significant contributing factors in the reference.\n",
      "* **Reranked Results:** Equally faithful. The same three causes are present, just in a different order.\n",
      "\n",
      "**Relevance:**\n",
      "\n",
      "* **Standard Results:** All three items are highly relevant to the causes of homelessness.  They represent significant contributing factors supported by extensive research.\n",
      "* **Reranked Results:**  Same as above. The relevance of each item remains unchanged by the reordering.\n",
      "\n",
      "**Ordering:**\n",
      "\n",
      "* **Standard Results:** The order (\"lack of affordable housing\", \"job loss\", \"mental health issues\") is a reasonable ordering, although arguably debatable.  It prioritizes a systemic factor (housing) before individual factors (job loss, mental health).\n",
      "* **Reranked Results:** The order (\"job loss\", \"lack of affordable housing\", \"mental health issues\") presents a different prioritization.  It leads with a more immediate and potentially more widely understood cause for many individuals (job loss), before moving to the broader systemic issue of housing and then the individual crisis of mental health.\n",
      "\n",
      "**Specific Strengths and Weaknesses:**\n",
      "\n",
      "* **Strength (both sets):** Both capture the core causes of homelessness accurately, aligning well with the reference answer's description of a complex issue with multiple contributing factors.  They avoid extraneous or irrelevant information.\n",
      "* **Weakness (both sets):**  The lists are somewhat simplistic.  The reference answer describes \"economic instability\" which is broader than just \"job loss.\" Similarly, \"personal crises\" encompasses a wider array of issues than just \"mental health issues.\" The result sets lack the nuance of the reference's explanation that homelessness is multifaceted and stems from an interaction of various factors.\n",
      "* **Weakness (ordering):**  Neither ordering is definitively \"better.\" The optimal order depends on the intended audience and purpose.  A systemic approach might favor prioritizing housing, while an individual-focused approach might favor job loss.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Both the standard and reranked results provide largely accurate and relevant information regarding the causes of homelessness.  The difference in ordering is not a significant issue, as both are justifiable, and neither is superior without further context on the desired presentation.  However, both could be improved by expanding beyond these three to include other significant contributing factors, like substance abuse, domestic violence, and lack of social support systems,  to better reflect the complexity outlined in the reference answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- 0) Initialize Gemini client ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Define the evaluation function for Gemini ---\n",
    "def evaluate_reranking(query: str, standard_results: Any, reranked_results: Any, reference_answer: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Evaluates reranking results using Gemini.\n",
    "    \n",
    "    Args:\n",
    "    query (str): User's question.\n",
    "    standard_results (Any): Results before reranking.\n",
    "    reranked_results (Any): Results after reranking.\n",
    "    reference_answer (str): The ideal answer.\n",
    "    model (str): Model to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    str: A detailed analysis of the results.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an objective evaluator. Provide a detailed analysis of how the two result sets compare \"\n",
    "        \"against the reference answer, with specific examples.\"\n",
    "    )\n",
    "    \n",
    "    comparison_text = (\n",
    "        f\"Query: {query}\\n\\n\"\n",
    "        f\"Standard Results:\\n{standard_results}\\n\\n\"\n",
    "        f\"Reranked Results:\\n{reranked_results}\\n\\n\"\n",
    "        f\"Reference Answer:\\n{reference_answer}\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"{comparison_text}\\n\\n\"\n",
    "        \"Please compare faithfulness, relevance, and ordering—point out specific strengths and weaknesses.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        resp = gemini_model.generate_content(user_prompt)\n",
    "        return resp.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        return \"Evaluation failed due to an error.\"\n",
    "\n",
    "# --- Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate data for a runnable example\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    standard_results = [\"lack of affordable housing\", \"job loss\", \"mental health issues\"]\n",
    "    llm_results = [\"job loss\", \"lack of affordable housing\", \"mental health issues\"]\n",
    "    reference_answer = \"Homelessness is a complex issue driven by a lack of affordable housing, economic instability, and personal crises like job loss.\"\n",
    "\n",
    "    # 1) Then call it as before:\n",
    "    evaluation = evaluate_reranking(\n",
    "        query=query,\n",
    "        standard_results=standard_results,\n",
    "        reranked_results=llm_results,\n",
    "        reference_answer=reference_answer\n",
    "    )\n",
    "    \n",
    "    # 2) Print\n",
    "    print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "    print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
