{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Evaluating Chunk Sizes in Simple RAG\n",
    "\n",
    "Choosing the right chunk size is crucial for improving retrieval accuracy in a Retrieval-Augmented Generation (RAG) pipeline. The goal is to balance retrieval performance with response quality.\n",
    "\n",
    "This section evaluates different chunk sizes by:\n",
    "\n",
    "1. Extracting text from a PDF.\n",
    "2. Splitting text into chunks of varying sizes.\n",
    "3. Creating embeddings for each chunk.\n",
    "4. Retrieving relevant chunks for a query.\n",
    "5. Generating a response using retrieved chunks.\n",
    "6. Evaluating faithfulness and relevancy.\n",
    "7. Comparing results for different chunk sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from the PDF\n",
    "First, we will extract text from the `AI_Information.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from the PDF.\n",
      "First 500 characters:\n",
      "\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. PDF Text Extraction Function ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file using the provided path\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page in the PDF\n",
    "            for page in mypdf:\n",
    "                # Extract text from the current page\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Return the extracted text, stripped of leading/trailing whitespace\n",
    "    return all_text.strip()\n",
    "\n",
    "# --- 3. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the PDF file\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/770 Google /AI_Information.pdf\"\n",
    "    \n",
    "    # Check if the file exists before trying to extract text\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'\")\n",
    "        exit()\n",
    "\n",
    "    # Extract text from the PDF file\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if extracted_text:\n",
    "        # Print the first 500 characters of the extracted text\n",
    "        print(\"Successfully extracted text from the PDF.\")\n",
    "        print(\"First 500 characters:\\n\")\n",
    "        print(extracted_text[:500])\n",
    "    else:\n",
    "        print(\"Failed to extract text.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "To improve retrieval, we split the extracted text into overlapping chunks of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128, Number of Chunks: 6\n",
      "Chunk Size: 256, Number of Chunks: 3\n",
      "Chunk Size: 512, Number of Chunks: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. Text Chunking Function ---\n",
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): Number of characters per chunk.\n",
    "    overlap (int): Overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # The step size for the loop is the chunk size minus the overlap\n",
    "    step_size = n - overlap\n",
    "    # Iterate through the text with the specified step size\n",
    "    for i in range(0, len(text), step_size):\n",
    "        # Append a chunk of text from the current index to the index + chunk size\n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# --- 2. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate extracted text from a PDF or other source\n",
    "    extracted_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural\n",
    "    intelligence displayed by humans and animals. AI research has been defined as the field of study\n",
    "    of intelligent agents, which refers to any device that perceives its environment and takes\n",
    "    actions that maximize its chance of successfully achieving its goals. The term \"artificial\n",
    "    intelligence\" had previously been used to describe machines that mimic and display \"human\"\n",
    "    cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Define different chunk sizes to evaluate\n",
    "    chunk_sizes = [128, 256, 512]\n",
    "\n",
    "    # Create a dictionary to store text chunks for each chunk size\n",
    "    # Overlap is set to 20% of the chunk size\n",
    "    text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n",
    "\n",
    "    # Print the number of chunks created for each chunk size\n",
    "    for size, chunks in text_chunks_dict.items():\n",
    "        print(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings convert text into numerical representations for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size: 128, Number of Embeddings: 3\n",
      "First embedding shape: (768,)\n",
      "\n",
      "Chunk Size: 256, Number of Embeddings: 2\n",
      "First embedding shape: (768,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(texts: List[str], model: str = \"models/embedding-001\") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    texts (List[str]): List of input texts.\n",
    "    model (str): Embedding model.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of numerical embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embeddings for a list of texts\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        # The API returns a single embedding list for the entire batch\n",
    "        return [np.array(emb) for emb in response['embedding']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a dictionary of text chunks\n",
    "    # This would come from a previous text chunking step\n",
    "    text_chunks_dict = {\n",
    "        128: [\"This is chunk one.\", \"This is chunk two.\", \"This is chunk three.\"],\n",
    "        256: [\"This is a longer chunk one.\", \"This is a longer chunk two.\"]\n",
    "    }\n",
    "\n",
    "    # Generate embeddings for each chunk size\n",
    "    # The tqdm progress bar will work as before\n",
    "    chunk_embeddings_dict = {}\n",
    "    for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\"):\n",
    "        chunk_embeddings_dict[size] = create_embeddings(chunks)\n",
    "\n",
    "    # Print the shape of the first embedding for verification\n",
    "    for size, embeddings in chunk_embeddings_dict.items():\n",
    "        if embeddings:\n",
    "            print(f\"\\nChunk Size: {size}, Number of Embeddings: {len(embeddings)}\")\n",
    "            print(f\"First embedding shape: {embeddings[0].shape}\")\n",
    "        else:\n",
    "            print(f\"\\nChunk Size: {size}, Failed to generate embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We use cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Comparing 'The cat sat on the mat.' and 'A feline rested on the rug.'...\n",
      "Cosine Similarity: 0.8778\n",
      "\n",
      "Comparing 'The cat sat on the mat.' and 'The car drove on the highway.'...\n",
      "Cosine Similarity: 0.6581\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Set your GOOGLE_API_KEY as an environment variable\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper function to get embeddings from Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for a given text using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- 3. Your original cosine_similarity function ---\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    # Handle the case where a norm is zero to prevent division by zero\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- 4. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define two sentences with similar meaning\n",
    "    sentence1 = \"The cat sat on the mat.\"\n",
    "    sentence2 = \"A feline rested on the rug.\"\n",
    "    \n",
    "    # Define a sentence with a different meaning\n",
    "    sentence3 = \"The car drove on the highway.\"\n",
    "\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    vec1 = get_embedding(sentence1)\n",
    "    vec2 = get_embedding(sentence2)\n",
    "    vec3 = get_embedding(sentence3)\n",
    "\n",
    "    if vec1.size > 0 and vec2.size > 0 and vec3.size > 0:\n",
    "        # Calculate and print the cosine similarity between the similar sentences\n",
    "        print(f\"\\nComparing '{sentence1}' and '{sentence2}'...\")\n",
    "        similarity_1_2 = cosine_similarity(vec1, vec2)\n",
    "        print(f\"Cosine Similarity: {similarity_1_2:.4f}\")\n",
    "\n",
    "        # Calculate and print the cosine similarity between the dissimilar sentences\n",
    "        print(f\"\\nComparing '{sentence1}' and '{sentence3}'...\")\n",
    "        similarity_1_3 = cosine_similarity(vec1, vec3)\n",
    "        print(f\"Cosine Similarity: {similarity_1_3:.4f}\")\n",
    "\n",
    "        # Expected output: similarity_1_2 should be a high value (close to 1), \n",
    "        # and similarity_1_3 should be a low value (closer to 0).\n",
    "    else:\n",
    "        print(\"\\nFailed to generate embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunk embeddings...\n",
      "\n",
      "Searching for chunks relevant to: 'What is the relationship between machine learning and AI?'...\n",
      "\n",
      "Top 2 most relevant chunks:\n",
      "[1] Machine learning is a subfield of AI that provides systems the ability to automatically learn and improve from experience.\n",
      "[2] Artificial intelligence is a branch of computer science that deals with creating intelligent machines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. Define helper functions for Gemini ---\n",
    "def get_embedding(text: str or List[str], model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates embeddings for a given text or list of texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str or List[str]): The text(s) to embed.\n",
    "        model (str): The Gemini embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector(s) as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        # Handle single and multiple text inputs\n",
    "        if isinstance(text, str):\n",
    "            return np.array(response['embedding'], dtype=np.float32)\n",
    "        else:\n",
    "            return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two NumPy vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- 3. Your core retrieval function, adapted for Gemini ---\n",
    "def retrieve_relevant_chunks(query: str, text_chunks: List[str], chunk_embeddings: List[np.ndarray], k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant text chunks.\n",
    "\n",
    "    Args:\n",
    "    query (str): User query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): Embeddings of text chunks.\n",
    "    k (int): Number of top chunks to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Most relevant text chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query using the Gemini-specific helper function\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]\n",
    "\n",
    "# --- 4. Main Logic Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate text chunks and their embeddings\n",
    "    text_chunks = [\n",
    "        \"Artificial intelligence is a branch of computer science that deals with creating intelligent machines.\",\n",
    "        \"Quantum computing harnesses quantum phenomena to perform computations beyond classical computers.\",\n",
    "        \"Machine learning is a subfield of AI that provides systems the ability to automatically learn and improve from experience.\",\n",
    "        \"The sun is the star at the center of the Solar System.\",\n",
    "        \"Robotics is an interdisciplinary field that integrates computer science and engineering.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Creating chunk embeddings...\")\n",
    "    chunk_embeddings = get_embedding(text_chunks)\n",
    "    \n",
    "    if chunk_embeddings.size > 0:\n",
    "        user_query = \"What is the relationship between machine learning and AI?\"\n",
    "        print(f\"\\nSearching for chunks relevant to: '{user_query}'...\")\n",
    "        \n",
    "        top_results = retrieve_relevant_chunks(user_query, text_chunks, chunk_embeddings, k=2)\n",
    "        \n",
    "        print(\"\\nTop 2 most relevant chunks:\")\n",
    "        for i, result in enumerate(top_results):\n",
    "            print(f\"[{i+1}] {result}\")\n",
    "    else:\n",
    "        print(\"Failed to create embeddings, cannot perform search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for chunk_size=256:\n",
      "['chunk1 for size 256', 'chunk2 for size 256']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1) Load your validation data ---\n",
    "val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/val.json'\n",
    "if not os.path.isfile(val_path):\n",
    "    raise FileNotFoundError(f\"Could not find validation file at: {val_path!r}\")\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "query = data[3]['question']\n",
    "\n",
    "# --- 2) Define your chunk sizes and sample text/embeddings dicts ---\n",
    "#    Replace these with your actual chunks and embeddings.\n",
    "chunk_sizes = [128, 256, 512]\n",
    "text_chunks_dict = {\n",
    "    size: [\"chunk1 for size \"+str(size), \"chunk2 for size \"+str(size)]  # ← REPLACE\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "# here we simulate embeddings as random vectors; replace with your real embeddings\n",
    "chunk_embeddings_dict = {\n",
    "    size: np.random.rand(len(text_chunks_dict[size]), 768)\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# --- 3) Define retrieval function inline ---\n",
    "def retrieve_relevant_chunks(query: str,\n",
    "                             chunks: list[str],\n",
    "                             embeddings: np.ndarray,\n",
    "                             top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Example using cosine similarity. Replace `embed_query`\n",
    "    with however you turn your query into a vector.\n",
    "    \"\"\"\n",
    "    # --- a) Embed the query (stubbed as random) ---\n",
    "    #    Replace this with your real query embedding call!\n",
    "    query_emb = np.random.rand(1, embeddings.shape[1])\n",
    "    \n",
    "    # --- b) Compute similarities and pick top_k ---\n",
    "    sims = cosine_similarity(query_emb, embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
    "    return [chunks[i] for i in top_idx]\n",
    "\n",
    "# --- 4) Run retrieval across sizes ---\n",
    "retrieved_chunks_dict = {\n",
    "    size: retrieve_relevant_chunks(\n",
    "        query,\n",
    "        text_chunks_dict[size],\n",
    "        chunk_embeddings_dict[size]\n",
    "    )\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# --- 5) Print results for size=256 ---\n",
    "print(\"Top chunks for chunk_size=256:\")\n",
    "print(retrieved_chunks_dict.get(256, 'No chunks for 256'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks\n",
    "Let's  generate a response based on the retrieved text for chunk size `256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI response (chunk_size=256):\n",
      "Based on the provided context, the key attributes of a helpful AI assistant are being concise, relevant, and accurate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant that strictly answers based on the given context. \"\n",
    "    \"If the answer cannot be derived directly from the provided context, \"\n",
    "    \"respond with: 'I do not have enough information to answer that.'\"\n",
    ")\n",
    "\n",
    "# 2) Define the response generator for Gemini\n",
    "def generate_response(query: str, system_prompt: str, retrieved_chunks: List[str], model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates an AI response based on retrieved chunks using the Gemini API.\n",
    "    \"\"\"\n",
    "    # Combine retrieved chunks into a single context string\n",
    "    context = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" \n",
    "                            for i, chunk in enumerate(retrieved_chunks)])\n",
    "    \n",
    "    # Build the full user prompt, including the context and the question\n",
    "    user_prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    # Create the model instance with the system prompt\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        # Call the generate_content endpoint\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"Error: Could not generate a response.\"\n",
    "\n",
    "# 3) (Re)define your query and retrieved_chunks_dict\n",
    "#    For this example, we'll use simulated data.\n",
    "query = \"What are the key attributes of a helpful AI assistant?\"\n",
    "chunk_sizes = [128, 256, 512]\n",
    "retrieved_chunks_dict = {\n",
    "    128: [\"A helpful assistant is responsive and polite.\", \"It provides concise and relevant answers.\"],\n",
    "    256: [\"A helpful assistant is designed to provide information in a clear and organized manner, demonstrating attributes like being concise, relevant, and accurate based on provided data.\"],\n",
    "    512: [\"A helpful assistant is built on a foundation of ethical principles, ensuring it is fair, unbiased, and respectful. It processes information efficiently and can adapt its communication style to the user's needs. The core attributes include providing concise, relevant, and accurate answers directly from the given context.\"]\n",
    "}\n",
    "\n",
    "# 4) Generate responses for each chunk size\n",
    "ai_responses_dict = {\n",
    "    size: generate_response(query, system_prompt, retrieved_chunks_dict[size])\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# 5) Print the response for chunk size 256\n",
    "print(\"AI response (chunk_size=256):\")\n",
    "print(ai_responses_dict.get(256, \"No response for size 256\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We score responses based on faithfulness and relevancy using powerfull llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation scoring system constants\n",
    "SCORE_FULL = 1.0     # Complete match or fully satisfactory\n",
    "SCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\n",
    "SCORE_NONE = 0.0     # No match or unsatisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strict evaluation prompt templates\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the faithfulness of the AI response compared to the true answer.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "True Answer: {true_answer}\n",
    "\n",
    "Faithfulness measures how well the AI response aligns with facts in the true answer, without hallucinations.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely faithful, no contradictions with true answer\n",
    "    * {partial} = Partially faithful, minor contradictions\n",
    "    * {none} = Not faithful, major contradictions or hallucinations\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the relevancy of the AI response to the user query.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "\n",
    "Relevancy measures how well the response addresses the user's question.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely relevant, directly addresses the query\n",
    "    * {partial} = Partially relevant, addresses some aspects\n",
    "    * {none} = Not relevant, fails to address the query\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness (256): 1.0, Relevancy (256): 0.0\n",
      "Faithfulness (128): 0.5, Relevancy (128): 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from typing import Tuple\n",
    "\n",
    "# --- 0) Initialize Gemini client (make sure GOOGLE_API_KEY is set) ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 1) Load your validation data ---\n",
    "val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/val.json'\n",
    "if not os.path.isfile(val_path):\n",
    "    raise FileNotFoundError(f\"Could not find {val_path!r}\")\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Use simulated data for a runnable example\n",
    "query = data[3]['question']\n",
    "true_answer = data[3]['ideal_answer']\n",
    "\n",
    "# 2) Define your prompts & scoring constants\n",
    "SCORE_FULL = \"1.0\"\n",
    "SCORE_PARTIAL = \"0.5\"\n",
    "SCORE_NONE = \"0.0\"\n",
    "\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the FAITHFULNESS of the assistant’s response given the true answer.\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "True Answer: {true_answer}\n",
    "Return:\n",
    "- {full} if the response is fully supported by the true answer.\n",
    "- {partial} if it’s partially supported.\n",
    "- {none} if it’s unsupported.\n",
    "Just output ONLY the numeric score.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the RELEVANCY of the assistant’s response to the user’s question.\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Return:\n",
    "- {full} if the response directly addresses the question.\n",
    "- {partial} if it somewhat addresses it.\n",
    "- {none} if it does not address it.\n",
    "Just output ONLY the numeric score.\n",
    "\"\"\"\n",
    "\n",
    "# 3) Define the evaluator\n",
    "def evaluate_response(question: str, response: str, true_answer: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates a response using the Gemini API for faithfulness and relevancy.\n",
    "    \"\"\"\n",
    "    # A single prompt is created by combining the system instruction and the user content\n",
    "    # This is a common pattern when using Gemini in this way.\n",
    "    faith_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        response=response,\n",
    "        true_answer=true_answer,\n",
    "        full=SCORE_FULL,\n",
    "        partial=SCORE_PARTIAL,\n",
    "        none=SCORE_NONE\n",
    "    )\n",
    "    rel_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        response=response,\n",
    "        full=SCORE_FULL,\n",
    "        partial=SCORE_PARTIAL,\n",
    "        none=SCORE_NONE\n",
    "    )\n",
    "    \n",
    "    # Use a single Gemini model for both evaluations\n",
    "    eval_model = genai.GenerativeModel('gemini-1.5-flash', system_instruction=\"You are an objective evaluator. Return ONLY the numeric score.\")\n",
    "\n",
    "    # Ask the LLM for faithfulness score\n",
    "    try:\n",
    "        faith_resp = eval_model.generate_content(faith_prompt)\n",
    "        faith_score = float(faith_resp.text.strip())\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(f\"Error parsing faithfulness score: {e}\")\n",
    "        faith_score = 0.0\n",
    "\n",
    "    # Ask the LLM for relevancy score\n",
    "    try:\n",
    "        rel_resp = eval_model.generate_content(rel_prompt)\n",
    "        rel_score = float(rel_resp.text.strip())\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(f\"Error parsing relevancy score: {e}\")\n",
    "        rel_score = 0.0\n",
    "\n",
    "    return faith_score, rel_score\n",
    "\n",
    "# 4) Simulate `ai_responses_dict`\n",
    "ai_responses_dict = {\n",
    "    256: \"The true answer is provided by the validation data for the question.\",\n",
    "    128: \"The answer is partially supported by the true answer.\"\n",
    "}\n",
    "chunk_sizes = [128, 256]\n",
    "\n",
    "# 5) Evaluate for chunk sizes 256 and 128\n",
    "faith256, rel256 = evaluate_response(query, ai_responses_dict[256], true_answer)\n",
    "faith128, rel128 = evaluate_response(query, ai_responses_dict[128], true_answer)\n",
    "\n",
    "print(f\"Faithfulness (256): {faith256}, Relevancy (256): {rel256}\")\n",
    "print(f\"Faithfulness (128): {faith128}, Relevancy (128): {rel128}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
