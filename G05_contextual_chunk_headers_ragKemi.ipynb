{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Contextual Chunk Headers (CCH) in Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) improves the factual accuracy of language models by retrieving relevant external knowledge before generating a response. However, standard chunking often loses important context, making retrieval less effective.\n",
    "\n",
    "Contextual Chunk Headers (CCH) enhance RAG by prepending high-level context (like document titles or section headers) to each chunk before embedding them. This improves retrieval quality and prevents out-of-context responses.\n",
    "\n",
    "## Steps in this Notebook:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking with Contextual Headers**: Extract section titles and prepend them to chunks.\n",
    "3. **Embedding Creation**: Convert context-enhanced chunks into numerical representations.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response from retrieved text.\n",
    "6. **Evaluation**: Assess response accuracy using a scoring system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set. Please set it.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text and Identifying Section Headers\n",
    "We extract text from a PDF while also identifying section titles (potential headers for chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Open the PDF file using a context manager\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page in the PDF\n",
    "            for page in mypdf:\n",
    "                # Extract text from the page\n",
    "                all_text += page.get_text(\"text\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Text with Contextual Headers\n",
    "To improve retrieval, we generate descriptive headers for each chunk using an LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chunk header with Gemini...\n",
      "\n",
      "Original Chunk:\n",
      "Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\n",
      "\n",
      "Generated Header:\n",
      "The Complex Issue of Homelessness\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- 0) Initialize Gemini client (make sure GOOGLE_API_KEY is set) ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1) Define the response generator\n",
    "def generate_chunk_header(chunk: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a title/header for a given text chunk using a Gemini LLM.\n",
    "\n",
    "    Args:\n",
    "    chunk (str): The text chunk to summarize as a header.\n",
    "    model (str): The model to be used for generating the header. Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: Generated header/title.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"Generate a concise and informative title for the given text. Do not include any other text besides the title.\"\n",
    "    \n",
    "    try:\n",
    "        # Create a GenerativeModel instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response\n",
    "        response = gemini_model.generate_content(chunk)\n",
    "\n",
    "        # Return the generated header/title, stripping any leading/trailing whitespace\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during header generation: {e}\")\n",
    "        return \"Failed to generate header.\"\n",
    "\n",
    "# --- Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a text chunk\n",
    "    text_chunk = \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "\n",
    "    print(\"Generating chunk header with Gemini...\")\n",
    "    # Generate a header for the text chunk\n",
    "    header = generate_chunk_header(text_chunk)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"\\nOriginal Chunk:\\n{text_chunk}\")\n",
    "    print(f\"\\nGenerated Header:\\n{header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text and generating headers with Gemini...\n",
      "\n",
      "Generated chunks with headers:\n",
      "Header: Causes of Homelessness\n",
      "Text: \n",
      "    Homelessness is a complex social problem with various contributing factors, including economic,...\n",
      "\n",
      "Header: Factors Contributing to Homelessness\n",
      "Text:  affects low-income families and individuals.\n",
      "    Social factors like family breakdown, domestic vio...\n",
      "\n",
      "Header: Addressing Homelessness: Solutions and Support\n",
      "Text: ob loss, mental health challenges, or substance abuse, are often triggers for losing housing.\n",
      "    Go...\n",
      "\n",
      "Header: Addressing Homelessness: A Holistic Approach\n",
      "Text: elp the homeless.\n",
      "    The best long-term solution is a holistic approach that includes providing aff...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 0) Initialize Gemini client ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1) Define the header generator for Gemini\n",
    "def generate_chunk_header(chunk: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a title/header for a given text chunk using a Gemini LLM.\n",
    "\n",
    "    Args:\n",
    "    chunk (str): The text chunk to summarize as a header.\n",
    "    model (str): The model to be used for generating the header. Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: Generated header/title.\n",
    "    \"\"\"\n",
    "    system_prompt = \"Generate a concise and informative title for the given text. Do not include any other text besides the title.\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(chunk)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during header generation: {e}\")\n",
    "        return \"Failed to generate header.\"\n",
    "\n",
    "# 2) Define the text chunker with headers\n",
    "def chunk_text_with_headers(text: str, n: int, overlap: int) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Chunks text into smaller segments and generates headers for each.\n",
    "\n",
    "    Args:\n",
    "    text (str): The full text to be chunked.\n",
    "    n (int): The chunk size in characters.\n",
    "    overlap (int): Overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: A list of dictionaries with 'header' and 'text' keys.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunk = text[i:i + n]\n",
    "        header = generate_chunk_header(chunk)\n",
    "        chunks.append({\"header\": header, \"text\": chunk})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# --- Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    text_to_chunk = \"\"\"\n",
    "    Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\n",
    "    A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\n",
    "    Social factors like family breakdown, domestic violence, and a lack of social support networks can also lead to homelessness.\n",
    "    Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\n",
    "    Government and non-profit organizations offer services like shelters, food banks, and temporary housing to help the homeless.\n",
    "    The best long-term solution is a holistic approach that includes providing affordable housing, mental health services, and job training.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Chunking text and generating headers with Gemini...\")\n",
    "    chunked_data = chunk_text_with_headers(text_to_chunk, n=256, overlap=50)\n",
    "\n",
    "    print(\"\\nGenerated chunks with headers:\")\n",
    "    for item in chunked_data:\n",
    "        print(f\"Header: {item['header']}\\nText: {item['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text and generating headers with Gemini...\n",
      "\n",
      "Sample Chunk:\n",
      "Header: Artificial Intelligence: An Introduction\n",
      "Content: Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper functions ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            for page in mypdf:\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return all_text.strip()\n",
    "\n",
    "def generate_chunk_header(chunk: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a concise and informative title for a text chunk using a Gemini LLM.\n",
    "    \"\"\"\n",
    "    system_prompt = \"Generate a concise and informative title for the given text. Do not include any other text besides the title.\"\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(chunk)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during header generation: {e}\")\n",
    "        return \"Failed to generate header.\"\n",
    "\n",
    "def chunk_text_with_headers(text: str, n: int, overlap: int) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Chunks text into smaller segments and generates headers for each.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    step_size = n - overlap\n",
    "    for i in range(0, len(text), step_size):\n",
    "        chunk = text[i:i + n]\n",
    "        if chunk.strip():  # Only process non-empty chunks\n",
    "            header = generate_chunk_header(chunk)\n",
    "            chunks.append({\"header\": header, \"text\": chunk})\n",
    "    return chunks\n",
    "\n",
    "# --- 3. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the PDF file path\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'\")\n",
    "        exit()\n",
    "\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Chunk the extracted text with headers\n",
    "    print(\"Chunking text and generating headers with Gemini...\")\n",
    "    text_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
    "\n",
    "    # Check if chunks were created\n",
    "    if text_chunks:\n",
    "        # Print a sample chunk with its generated header\n",
    "        print(\"\\nSample Chunk:\")\n",
    "        print(\"Header:\", text_chunks[0]['header'])\n",
    "        print(\"Content:\", text_chunks[0]['text'])\n",
    "    else:\n",
    "        print(\"Failed to create any text chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Headers and Text\n",
    "We create embeddings for both headers and text to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding with Gemini...\n",
      "\n",
      "Embedding created successfully.\n",
      "Embedding shape: (768,)\n",
      "First 5 values: [ 0.05044351 -0.03356895 -0.06893376 -0.04065947  0.04912253]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str): The embedding model to be used. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embeddings using the specified model and input text\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        # Return the embedding from the response as a NumPy array\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Homelessness is a significant social issue.\"\n",
    "\n",
    "    print(\"Creating embedding with Gemini...\")\n",
    "    embedding = create_embeddings(sample_text)\n",
    "\n",
    "    if embedding.size > 0:\n",
    "        print(\"\\nEmbedding created successfully.\")\n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"First 5 values: {embedding[:5]}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 42/42 [00:21<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []  # Initialize an empty list to store embeddings\n",
    "\n",
    "# Iterate through each text chunk with a progress bar\n",
    "for chunk in tqdm(text_chunks, desc=\"Generating embeddings\"):\n",
    "    # Create an embedding for the chunk's text\n",
    "    text_embedding = create_embeddings(chunk[\"text\"])\n",
    "    # Create an embedding for the chunk's header\n",
    "    header_embedding = create_embeddings(chunk[\"header\"])\n",
    "    # Append the chunk's header, text, and their embeddings to the list\n",
    "    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Comparing 'The cat sat on the mat.' and 'A feline rested on the rug.'...\n",
      "Cosine Similarity: 0.8778\n",
      "\n",
      "Comparing 'The cat sat on the mat.' and 'The car drove on the highway.'...\n",
      "Cosine Similarity: 0.6581\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. Helper function to get embeddings from Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for a given text using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- 3. Your original cosine_similarity function ---\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    # Handle the case where a norm is zero to prevent division by zero\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- 4. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define two sentences with similar meaning\n",
    "    sentence1 = \"The cat sat on the mat.\"\n",
    "    sentence2 = \"A feline rested on the rug.\"\n",
    "    \n",
    "    # Define a sentence with a different meaning\n",
    "    sentence3 = \"The car drove on the highway.\"\n",
    "\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    vec1 = get_embedding(sentence1)\n",
    "    vec2 = get_embedding(sentence2)\n",
    "    vec3 = get_embedding(sentence3)\n",
    "\n",
    "    if vec1.size > 0 and vec2.size > 0 and vec3.size > 0:\n",
    "        # Calculate and print the cosine similarity between the similar sentences\n",
    "        print(f\"\\nComparing '{sentence1}' and '{sentence2}'...\")\n",
    "        similarity_1_2 = cosine_similarity(vec1, vec2)\n",
    "        print(f\"Cosine Similarity: {similarity_1_2:.4f}\")\n",
    "\n",
    "        # Calculate and print the cosine similarity between the dissimilar sentences\n",
    "        print(f\"\\nComparing '{sentence1}' and '{sentence3}'...\")\n",
    "        similarity_1_3 = cosine_similarity(vec1, vec3)\n",
    "        print(f\"Cosine Similarity: {similarity_1_3:.4f}\")\n",
    "\n",
    "        # Expected output: similarity_1_2 should be a high value (close to 1), \n",
    "        # and similarity_1_3 should be a low value (closer to 0).\n",
    "    else:\n",
    "        print(\"\\nFailed to generate embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for relevant chunks for query: 'What are the reasons for someone becoming homeless?'...\n",
      "\n",
      "Top 2 most relevant chunks:\n",
      "[1] Header: Homelessness Causes\n",
      "    Content: Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\n",
      "\n",
      "[2] Header: Mental Health and Job Loss\n",
      "    Content: Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper Functions for Gemini API ---\n",
    "def create_embeddings(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using the Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str): The embedding model to be used.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- 3. Your Semantic Search Function (Revised) ---\n",
    "def semantic_search(query: str, chunks: List[Dict], k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches for the most relevant chunks based on a query.\n",
    "    \n",
    "    Args:\n",
    "    query (str): User query.\n",
    "    chunks (List[dict]): List of text chunks with embeddings.\n",
    "    k (int): Number of top results.\n",
    "    \n",
    "    Returns:\n",
    "    List[dict]: Top-k most relevant chunks.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query using the Gemini API\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    similarities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Compute cosine similarity between query embedding and chunk text embedding\n",
    "        sim_text = cosine_similarity(query_embedding, np.array(chunk[\"embedding\"]))\n",
    "        # Compute cosine similarity between query embedding and chunk header embedding\n",
    "        sim_header = cosine_similarity(query_embedding, np.array(chunk[\"header_embedding\"]))\n",
    "        \n",
    "        # Calculate the average similarity score\n",
    "        avg_similarity = (sim_text + sim_header) / 2\n",
    "        \n",
    "        similarities.append((chunk, avg_similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in similarities[:k]]\n",
    "\n",
    "# --- 4. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a list of chunks with their embeddings\n",
    "    # In a real scenario, these would come from a previous step\n",
    "    # with generate_chunk_header and create_embeddings functions\n",
    "    chunks_with_embeddings = [\n",
    "        {\"header\": \"Homelessness Causes\", \"text\": \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\", \"embedding\": create_embeddings(\"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\"), \"header_embedding\": create_embeddings(\"Homelessness Causes\")},\n",
    "        {\"header\": \"Affordable Housing\", \"text\": \"A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\", \"embedding\": create_embeddings(\"A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"), \"header_embedding\": create_embeddings(\"Affordable Housing\")},\n",
    "        {\"header\": \"Mental Health and Job Loss\", \"text\": \"Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\", \"embedding\": create_embeddings(\"Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\"), \"header_embedding\": create_embeddings(\"Mental Health and Job Loss\")},\n",
    "        {\"header\": \"Solar System Planets\", \"text\": \"The sun is the star at the center of the Solar System. The eight planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\", \"embedding\": create_embeddings(\"The sun is the star at the center of the Solar System. The eight planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\"), \"header_embedding\": create_embeddings(\"Solar System Planets\")}\n",
    "    ]\n",
    "\n",
    "    query = \"What are the reasons for someone becoming homeless?\"\n",
    "    \n",
    "    print(f\"Searching for relevant chunks for query: '{query}'...\")\n",
    "    top_chunks = semantic_search(query, chunks_with_embeddings, k=2)\n",
    "\n",
    "    print(\"\\nTop 2 most relevant chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"[{i+1}] Header: {chunk['header']}\")\n",
    "        print(f\"    Content: {chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the ETHOS typology?\n",
      "Header 1: \"Understanding Homelessness and Housing Exclusion: The ETHOS Typology in the EU\"\n",
      "Content:\n",
      "elessness, while people living in insecure and/or inadequate housing \n",
      "and/or in social isolation might also be affected by exclusion from one or two domains, \n",
      "but their situation is classified under ‘housing exclusion’ rather than ‘homelessness’.\n",
      "On the basis of this conceptional understanding and to try to grasp the varying \n",
      "practices in different EU countries, the ETHOS typology was developed, which \n",
      "relates, in its most recent version, thirteen different operational categories and \n",
      "twenty-four different living situations to the four conceptional categories: roofless, \n",
      "houseless, insecure housing and inadequate housing.4 See Table 1.2.\n",
      "4\t\n",
      "Apart from documenting progress concerning the measurement of homelessness in different \n",
      "EU countries and reporting on the latest available data, the forth and fifth reviews of statistics \n",
      "(Edgar and Meert, 2005, 2006) focused on developing and refining the ETHOS definition and \n",
      "considering the measurement issues involved in greater detail. \n",
      "24\n",
      "Home\n",
      "\n",
      "Header 2: \"Understanding Homelessness: Beyond Rooflessness and the ETHOS Typology\"\n",
      "Content:\n",
      "ple living in unfit housing\n",
      "12.1\n",
      "Occupied dwelling  \n",
      "unfit for habitation \n",
      "13\n",
      "People living  \n",
      "in extreme overcrowding\n",
      "13.1\n",
      "Highest national norm  \n",
      "of overcrowding\n",
      "Source: Edgar, 2009, p.73.\n",
      "25\n",
      "The ETHOS typology provides an extremely useful reference frame and underlines \n",
      "that rooflessness, the category that is least controversial and receiving the greatest \n",
      "attention from the media and the general public, is only the ‘tip of the iceberg’ making \n",
      "visible a much wider phenomenon. There is a broad consensus that the term ‘home-\n",
      "lessness’ covers more living situations than being without a roof over one’s head. \n",
      "However, most definitions of homelessness at national level include either more or \n",
      "(more often) less categories than listed in the houseless category of ETHOS.\n",
      "Edgar et al. (2004, p.5) note that some countries (e.g. Austria, Germany and \n",
      "Luxembourg) make a distinction between those who are homeless at a point in time, \n",
      "those imminently threatened with homelessness and those housed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "with open('/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query = data[0]['question']\n",
    "\n",
    "# Retrieve the top 2 most relevant text chunks\n",
    "top_chunks = semantic_search(query, embeddings, k=2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Query:\", query)\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Header {i+1}: {chunk['header']}\")\n",
    "    print(f\"Content:\\n{chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the ETHOS typology?\n",
      "Header 1: Homelessness Causes\n",
      "Content:\n",
      "Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\n",
      "\n",
      "Header 2: Mental Health and Job Loss\n",
      "Content:\n",
      "Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # Load validation data\n",
    "    val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json'\n",
    "    # The `json.load()` method is a standard Python function and doesn't need to be changed for Gemini.\n",
    "    if not os.path.isfile(val_path):\n",
    "        raise FileNotFoundError(f\"Could not find validation file at: {val_path!r}\")\n",
    "    with open(val_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Use the first question from the validation data\n",
    "    query = data[0]['question']\n",
    "\n",
    "    # Retrieve the top 2 most relevant text chunks\n",
    "    top_chunks = semantic_search(query, chunks_with_embeddings, k=2)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Query:\", query)\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"Header {i+1}: {chunk['header']}\")\n",
    "        print(f\"Content:\\n{chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Social factors that contribute to homelessness include family breakdown, domestic violence, and a lack of social support networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # Define the system prompt for the AI assistant\n",
    "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "    # Create the user prompt based on the top chunks\n",
    "    user_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\n",
    "    user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "    # Generate AI response\n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "    # Print the final AI response\n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation system prompt\n",
    "evaluate_system_prompt = \"\"\"You are an intelligent evaluation system. \n",
    "Assess the AI assistant's response based on the provided context. \n",
    "- Assign a score of 1 if the response is very close to the true answer. \n",
    "- Assign a score of 0.5 if the response is partially correct. \n",
    "- Assign a score of 0 if the response is incorrect.\n",
    "Return only the score (0, 0.5, or 1).\"\"\"\n",
    "\n",
    "# Extract the ground truth answer from validation data\n",
    "true_answer = data[0]['ideal_answer']\n",
    "\n",
    "# Construct evaluation prompt\n",
    "evaluation_prompt = f\"\"\"\n",
    "User Query: {query}\n",
    "AI Response: {ai_response}\n",
    "True Answer: {true_answer}\n",
    "{evaluate_system_prompt}\n",
    "\"\"\"\n",
    "\n",
    "# Generate evaluation score\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation score\n",
    "print(\"Evaluation Score:\", evaluation_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating evaluation score with Gemini...\n",
      "\n",
      "Evaluation Score: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query, AI response, and true answer\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    ai_response_content = \"A lack of affordable housing is a key contributing factor to homelessness.\"\n",
    "    true_answer = \"Homelessness is caused by a lack of affordable housing, job loss, and mental health issues.\"\n",
    "\n",
    "    # Define evaluation system prompt\n",
    "    evaluate_system_prompt = \"\"\"You are an intelligent evaluation system.\n",
    "Assess the AI assistant's response based on the provided context.\n",
    "- Assign a score of 1 if the response is very close to the true answer.\n",
    "- Assign a score of 0.5 if the response is partially correct.\n",
    "- Assign a score of 0 if the response is incorrect.\n",
    "Return only the score (0, 0.5, or 1).\"\"\"\n",
    "\n",
    "    # Construct evaluation prompt\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    User Query: {query}\n",
    "    AI Response: {ai_response_content}\n",
    "    True Answer: {true_answer}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate evaluation score\n",
    "    print(\"Generating evaluation score with Gemini...\")\n",
    "    evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "    \n",
    "    # Print the evaluation score\n",
    "    print(\"\\nEvaluation Score:\", evaluation_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
