{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\n",
    "\n",
    "In a Simple RAG setup, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n",
    "3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieved text.\n",
    "\n",
    "This notebook implements a Simple RAG approach, evaluates the modelâ€™s response, and explores various improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 43\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence  \n",
      "Chapter 1: Introduction to Artificial Intelligence  \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer -controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intelle ctual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the develo pment and deployment of AI.  \n",
      "Historical Context  \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid -20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI r esearch focused on problem -solving \n",
      "and symbolic methods. The 1980s saw a r\n"
     ]
    }
   ],
   "source": [
    "# Import the required library for PDF reading\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        # Loop through each page and extract text\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunks text into segments with a specified overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    # Loop through the text and create overlapping chunks\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        # Move the start position forward by the chunk size minus the overlap\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# --- Your Original Logic ---\n",
    "\n",
    "# Define the path to the PDF file\n",
    "# IMPORTANT: Make sure this path is correct on your system\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/PHD/ISEM 770/Class Code SAT/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file by calling the function we defined\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text by calling the function we defined\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "# Print only the first chunk if it exists\n",
    "if text_chunks:\n",
    "    print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding chunks with Gemini...\n"
     ]
    }
   ],
   "source": [
    "# --- Embeddings (Gemini) ---\n",
    "def _embed_one(text: str, model: str = \"models/embedding-001\", retries: int = 3) -> List[float]:\n",
    "    \"\"\"Embed a single text with retry/backoff.\"\"\"\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = genai.embed_content(model=model, content=text)\n",
    "            return resp[\"embedding\"]\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            # Friendly handling for common auth issues\n",
    "            if \"API key\" in str(e) or \"401\" in str(e):\n",
    "                raise RuntimeError(\"Google API auth failed: check GOOGLE_API_KEY\") from e\n",
    "            time.sleep(1.5 * (i + 1))\n",
    "    raise last_err\n",
    "\n",
    "def embed_texts(texts: List[str], model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Embed a list of strings. Sequential on purpose (robust/simple).\"\"\"\n",
    "    embs = []\n",
    "    for t in texts:\n",
    "        embs.append(_embed_one(t, model=model))\n",
    "    return np.array(embs, dtype=np.float32)\n",
    "\n",
    "# --- Main example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ...\n",
    "    # 3) Embed chunks\n",
    "    print(\"Embedding chunks with Gemini...\")\n",
    "    chunk_embs = embed_texts(chunks, model=\"models/embedding-001\")\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # 5) Example Q&A\n",
    "    example_query = \"What is AI according to this document?\"\n",
    "    q_vec = np.array(_embed_one(example_query, model=\"models/embedding-001\"), dtype=np.float32)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 43\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence  \n",
      "Chapter 1: Introduction to Artificial Intelligence  \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer -controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intelle ctual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the develo pment and deployment of AI.  \n",
      "Historical Context  \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid -20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI r esearch focused on problem -solving \n",
      "and symbolic methods. The 1980s saw a r\n"
     ]
    }
   ],
   "source": [
    "# Import the required library for PDF reading\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        # Loop through each page and extract text\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunks text into segments with a specified overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    # Loop through the text and create overlapping chunks\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        # Move the start position forward by the chunk size minus the overlap\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# --- Your Original Logic ---\n",
    "\n",
    "# Define the path to the PDF file\n",
    "# IMPORTANT: Make sure this path is correct on your system\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/PHD/ISEM 770/Class Code SAT/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file by calling the function we defined\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text by calling the function we defined\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "# Print only the first chunk if it exists\n",
    "if text_chunks:\n",
    "    print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 43\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence  \n",
      "Chapter 1: Introduction to Artificial Intelligence  \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer -controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intelle ctual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the pa ...\n",
      "\n",
      "Asking Gemini: 'What is artificial intelligence?'...\n",
      "\n",
      "Gemini's Answer:\n",
      "Artificial intelligence (AI) is the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings.  This includes processes characteristic of humans, such as reasoning, discovering meaning, generalizing, and learning from past experience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configure Google API ---\n",
    "# Get your API key from an environment variable for security\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please set the environment variable.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunks text into segments with a specified overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def ask_gemini_about_chunks(chunks, query):\n",
    "    \"\"\"\n",
    "    Uses the Gemini API to answer a query based on a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): A list of text strings (chunks).\n",
    "        query (str): The question to ask the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated answer from the Gemini model.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    # Create a prompt that includes all the chunks as context\n",
    "    prompt = f\"Using the following text, answer the question: '{query}'\\n\\nContext:\\n\" + \"\\n\".join(chunks)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# --- Main Logic ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the PDF file\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/770 Google /AI_Information.pdf\"\n",
    "    \n",
    "    # Extract text and chunk it\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "    print(\"Number of text chunks:\", len(text_chunks))\n",
    "    \n",
    "    if text_chunks:\n",
    "        print(\"\\nFirst text chunk:\")\n",
    "        print(text_chunks[0][:500], \"...\")\n",
    "        \n",
    "    # Example usage of the Gemini API\n",
    "    # Define a query to ask the model\n",
    "    user_query = \"What is artificial intelligence?\"\n",
    "    \n",
    "    # Pass the chunks and the query to the Gemini function\n",
    "    print(f\"\\nAsking Gemini: '{user_query}'...\")\n",
    "    gemini_answer = ask_gemini_about_chunks(text_chunks, user_query)\n",
    "    \n",
    "    print(\"\\nGemini's Answer:\")\n",
    "    print(gemini_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key=os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 embeddings.\n",
      "First embedding vector (snippet):\n",
      "[0.016593121, -0.06711799, 0.00490424, -0.004007633, 0.04125118] ...\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- Configure Google API ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please set the environment variable.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def create_embeddings(text_chunks: List[str], model: str = \"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for a list of text chunks using the Google Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text_chunks (List[str]): A list of text strings to embed.\n",
    "        model (str): The name of the embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: A list of embedding vectors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=text_chunks\n",
    "        )\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Logic Example ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a list of text chunks\n",
    "    text_chunks = [\n",
    "        \"This is a test chunk about machine learning.\",\n",
    "        \"Another piece of text discussing neural networks.\",\n",
    "        \"A third chunk about large language models.\"\n",
    "    ]\n",
    "\n",
    "    # Create embeddings for the chunks\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"Created {len(embeddings)} embeddings.\")\n",
    "        print(\"First embedding vector (snippet):\")\n",
    "        # Print a snippet of the first embedding to show it works\n",
    "        print(embeddings[0][:5], \"...\")\n",
    "    else:\n",
    "        print(\"Failed to create embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 3 embeddings.\n",
      "\n",
      "Embedding for the first chunk (first 5 values):\n",
      "[0.04206869, -0.03465894, 0.0016993333, -0.04137134, 0.01707746]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API with your key. It's best to use an environment variable.\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for a list of text chunks using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "        text_chunks (list[str]): The list of input texts for which embeddings are to be created.\n",
    "        model (str): The Gemini model to be used. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of embedding vectors from the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=text_chunks\n",
    "        )\n",
    "        # The embeddings are located directly in the 'embedding' key of the response dictionary\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define 'text_chunks' as a list of strings\n",
    "    text_chunks = [\n",
    "        \"This is the first sentence, which we want to embed.\",\n",
    "        \"This is the second one, containing different information.\",\n",
    "        \"A third text chunk to demonstrate the functionality.\"\n",
    "    ]\n",
    "\n",
    "    # Create embeddings for the text chunks\n",
    "    embeddings_data = create_embeddings(text_chunks)\n",
    "\n",
    "    if embeddings_data:\n",
    "        # Print the number of embeddings created\n",
    "        print(f\"Successfully created {len(embeddings_data)} embeddings.\")\n",
    "\n",
    "        # Print the embedding for the first text chunk (optional)\n",
    "        print(\"\\nEmbedding for the first chunk (first 5 values):\")\n",
    "        print(embeddings_data[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Cosine similarity between 'The cat sat on the mat.' and 'A feline rested on the rug.':\n",
      "0.8778\n",
      "\n",
      "Cosine similarity between 'The cat sat on the mat.' and 'The car drove down the street.':\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates an embedding vector for a given text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to embed.\n",
    "        model (str): The Gemini embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return np.array(response['embedding'], dtype=np.float32)\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray): The first vector.\n",
    "        vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product and divide by the product of their norms\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product\n",
    "\n",
    "# --- Main Logic Example ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get embeddings for two different texts\n",
    "    text1 = \"The cat sat on the mat.\"\n",
    "    text2 = \"A feline rested on the rug.\"\n",
    "    text3 = \"The car drove down the street.\"\n",
    "\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    vec1 = get_embedding(text1)\n",
    "    vec2 = get_embedding(text2)\n",
    "    vec3 = get_embedding(text3)\n",
    "\n",
    "    # Calculate and print the cosine similarity between the vectors\n",
    "    print(f\"\\nCosine similarity between '{text1}' and '{text2}':\")\n",
    "    similarity_1_2 = cosine_similarity(vec1, vec2)\n",
    "    print(f\"{similarity_1_2:.4f}\")\n",
    "\n",
    "    print(f\"\\nCosine similarity between '{text1}' and '{text3}':\")\n",
    "    similarity_1_3 = cosine_similarity(vec1, vec3)\n",
    "    print(f\"{similarity_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query: str, text_chunks: list[str], embeddings: list[list[float]], k: int):\n",
    "    \"\"\"\n",
    "    Performs semantic search using a query, text chunks, and their embeddings.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)[0].embedding\n",
    "\n",
    "    # Calculate similarity scores between the query and each text chunk\n",
    "    similarity_scores = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Get the indices of the top k scores\n",
    "    top_k_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
    "\n",
    "    # Return the corresponding text chunks using the full variable name\n",
    "    #\n",
    "    # OLD, INCORRECT line:\n",
    "    # return [text_chunks[i] for i in top_\n",
    "    #\n",
    "    # NEW, CORRECT line:\n",
    "    return [text_chunks[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response...\n",
      "\n",
      "AI Response:\n",
      "There are more stars in the universe than grains of sand on all the beaches on Earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "# Note: google.generativeai handles its own authentication and exceptions,\n",
    "# so explicit imports for AuthenticationError are not needed.\n",
    "\n",
    "# --- 1. Set up your Gemini client ---\n",
    "# Get your API key from an environment variable\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check for API key\n",
    "if key is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable is not set.\")\n",
    "    print(\"Please set your API key before running the script.\")\n",
    "    exit()\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "genai.configure(api_key=key)\n",
    "\n",
    "# --- 2. Define the generate_response function ---\n",
    "def generate_response(system_prompt: str, user_message: str, model: str = \"gemini-1.5-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the Gemini model based on a system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior. This will be\n",
    "                         prepended to the user's message.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The Gemini model to be used. Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: The generated text response from the AI model.\n",
    "    \"\"\"\n",
    "    # The Gemini API handles system prompts and user messages as a single chat history.\n",
    "    # We combine the system prompt and user message into a single message for simplicity.\n",
    "    full_prompt = f\"{system_prompt}\\n\\nUser: {user_message}\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model)\n",
    "        response = gemini_model.generate_content(full_prompt)\n",
    "        # The text is directly accessible from the response object\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        # A more general exception handler is used for Gemini\n",
    "        print(f\"An error occurred during AI response generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. Define the system_prompt and user_prompt variables ---\n",
    "system_prompt = \"You are a helpful and creative AI assistant. Provide concise and relevant answers.\"\n",
    "user_prompt = \"Tell me a fun fact about the universe.\"\n",
    "\n",
    "# --- 4. Generate AI response using the corrected function ---\n",
    "print(\"Generating AI response...\")\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "# Print the final response content from the AI\n",
    "if ai_response:\n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is the capital of France?\n",
      "AI Assistant's Response: Paris\n",
      "\n",
      "\n",
      "Generating Evaluation Response...\n",
      "Evaluation Score: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- 1. Set up your Gemini client ---\n",
    "# Load environment variables from a .env file (optional)\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# Get your API key from the environment variable\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Basic check for API key\n",
    "if key is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable is not set.\")\n",
    "    print(\"Please set your API key before running the script.\")\n",
    "    exit()\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "genai.configure(api_key=key)\n",
    "\n",
    "# --- 2. Define the generate_response function ---\n",
    "def generate_response(system_prompt, user_message, model=\"gemini-1.5-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the Gemini model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The Gemini model to be used for generating the response.\n",
    "                 Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: The generated text response from the AI model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Gemini handles the system prompt and user message as a single prompt\n",
    "        # when using a single turn. For multi-turn conversations, it uses a chat history.\n",
    "        model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = model.generate_content(user_message)\n",
    "        # The text is directly accessible from the response object\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during AI response generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. Define the variables needed for the AI response and evaluation ---\n",
    "query = \"What is the capital of France?\"\n",
    "data = [\n",
    "    {\"question\": \"What is the capital of France?\", \"ideal_answer\": \"Paris\"},\n",
    "    {\"question\": \"Who painted the Mona Lisa?\", \"ideal_answer\": \"Leonardo da Vinci\"}\n",
    "]\n",
    "ai_assistant_system_prompt = \"You are a helpful and creative AI assistant. Provide concise and relevant answers.\"\n",
    "\n",
    "# --- 4. Generate AI response ---\n",
    "print(f\"User Query: {query}\")\n",
    "try:\n",
    "    ai_response_content = generate_response(ai_assistant_system_prompt, query)\n",
    "    if ai_response_content:\n",
    "        print(f\"AI Assistant's Response: {ai_response_content}\")\n",
    "    else:\n",
    "        ai_response_content = \"Error generating AI response.\"\n",
    "        print(ai_response_content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AI response generation: {e}\")\n",
    "    ai_response_content = \"Error generating AI response.\"\n",
    "\n",
    "\n",
    "# --- 5. Define the evaluation system prompt ---\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system. Score AI responses: 1 for 'very close' to the true answer, 0 for 'incorrect/unsatisfactory', and 0.5 for 'partially aligned'. Provide only the score.\"\n",
    "\n",
    "# --- 6. Create the evaluation prompt ---\n",
    "evaluation_prompt = (\n",
    "    f\"User Query: {query}\\n\"\n",
    "    f\"AI Response:\\n{ai_response_content}\\n\"\n",
    "    f\"True Response: {data[0]['ideal_answer']}\\n\"\n",
    ")\n",
    "\n",
    "# --- 7. Generate the evaluation response ---\n",
    "print(\"\\nGenerating Evaluation Response...\")\n",
    "try:\n",
    "    evaluation_score = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "    if evaluation_score:\n",
    "        print(f\"Evaluation Score: {evaluation_score}\")\n",
    "    else:\n",
    "        print(\"Error generating evaluation score.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation response generation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
