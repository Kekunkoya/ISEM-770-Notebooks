{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Introduction to Semantic Chunking\n",
    "Text chunking is an essential step in Retrieval-Augmented Generation (RAG), where large text bodies are divided into meaningful segments to improve retrieval accuracy.\n",
    "Unlike fixed-length chunking, semantic chunking splits text based on the content similarity between sentences.\n",
    "\n",
    "### Breakpoint Methods:\n",
    "- **Percentile**: Finds the Xth percentile of all similarity differences and splits chunks where the drop is greater than this value.\n",
    "- **Standard Deviation**: Splits where similarity drops more than X standard deviations below the mean.\n",
    "- **Interquartile Range (IQR)**: Uses the interquartile distance (Q3 - Q1) to determine split points.\n",
    "\n",
    "This notebook implements semantic chunking **using the percentile method** and evaluates its performance on a sample text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted first 500 characters of text from PDF.\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n",
      "\n",
      "Generating a summary with Gemini...\n",
      "Summary:\n",
      "This book comprehensively explores artificial intelligence (AI), covering its history, core concepts, applications, ethical implications, and future directions.  It begins by defining AI and tracing its development from the Dartmouth Workshop to the current era of deep learning.  The core concepts explained include machine learning (supervised, unsupervised, and reinforcement learning), deep learning (CNNs and RNNs), natural language processing (NLP), and computer vision.\n",
      "\n",
      "The book then details numerous AI applications across various sectors, including healthcare, finance, transportation, retail, manufacturing, education, entertainment, and cybersecurity.  Ethical concerns are addressed, focusing on bias, transparency, privacy, job displacement, and the weaponization of AI.\n",
      "\n",
      "The future of AI is explored through discussions on explainable AI (XAI), AI at the edge, quantum computing's role, human-AI collaboration, and AI for social good.  Specific chapters delve into the integration of AI with robotics, its impact on business and the future of work, its role in creativity and innovation, and its applications in education, healthcare, and cybersecurity.  The book concludes by emphasizing the need for responsible AI development, ethical considerations, building public trust, and the crucial role of government policy and international collaboration to ensure a beneficial and equitable future for AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Your extract_text_from_pdf function goes here...\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for page in mypdf:\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "    return all_text.strip()\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Define a function for summarization ---\n",
    "def summarize_text_with_gemini(text_to_summarize):\n",
    "    \"\"\"\n",
    "    Summarizes text using the Gemini API.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text_to_summarize}\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/kekunkoya/Desktop/770 Google /AI_Information.pdf\"\n",
    "    \n",
    "    # 1. Extract text from the PDF\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"Extracted first 500 characters of text from PDF.\")\n",
    "    print(extracted_text[:500])\n",
    "\n",
    "    # 2. Use Gemini to summarize the extracted text\n",
    "    print(\"\\nGenerating a summary with Gemini...\")\n",
    "    summary = summarize_text_with_gemini(extracted_text)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sentence-Level Embeddings\n",
    "We split text into sentences and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sentence embeddings.\n",
      "Embedding vector shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Define the get_embedding function for Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using Gemini.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- Main Logic ---\n",
    "# Note: You'll need to define 'extracted_text' before this part\n",
    "# For example: extracted_text = \"This is a sample text. It has multiple sentences. We will split them.\"\n",
    "# Define 'extracted_text' here, or ensure it's available from a previous step\n",
    "extracted_text = \"This is a sample text. It has multiple sentences. We will split them and get embeddings for each.\"\n",
    "\n",
    "# Splitting text into sentences (basic split)\n",
    "sentences = extracted_text.split(\". \")\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "# Filter out any empty embeddings if an error occurred\n",
    "embeddings = [emb for emb in embeddings if emb.size > 0]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")\n",
    "if embeddings:\n",
    "    print(f\"Embedding vector shape: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity Differences\n",
    "We compute cosine similarity between consecutive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Generated 4 sentence embeddings.\n",
      "Computed similarities between consecutive sentences:\n",
      "  Similarity between 'The quick brown fox jumps over the lazy dog.' and 'A fast, ginger canine leaps above a sluggish hound.': 0.7653\n",
      "  Similarity between 'A fast, ginger canine leaps above a sluggish hound.' and 'The car drives on the highway.': 0.6351\n",
      "  Similarity between 'The car drives on the highway.' and 'A quick brown dog runs away from home.': 0.6918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Define the get_embedding function for Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- Your original cosine_similarity function ---\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- Main Logic Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample sentences to demonstrate the workflow\n",
    "    sentences = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"A fast, ginger canine leaps above a sluggish hound.\",\n",
    "        \"The car drives on the highway.\",\n",
    "        \"A quick brown dog runs away from home.\"\n",
    "    ]\n",
    "\n",
    "    # Generate embeddings for each sentence using the Gemini API\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    # Filter out any potential empty embeddings from errors\n",
    "    embeddings = [emb for emb in embeddings if emb.size > 0]\n",
    "    \n",
    "    if len(embeddings) < 2:\n",
    "        print(\"Not enough embeddings to compute similarity.\")\n",
    "        exit()\n",
    "\n",
    "    # Compute similarity between consecutive sentences\n",
    "    similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]\n",
    "\n",
    "    print(f\"\\nGenerated {len(embeddings)} sentence embeddings.\")\n",
    "    print(\"Computed similarities between consecutive sentences:\")\n",
    "    for i, sim in enumerate(similarities):\n",
    "        print(f\"  Similarity between '{sentences[i]}' and '{sentences[i+1]}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Semantic Chunking\n",
    "We implement three different methods for finding breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Computed similarities between consecutive sentences: ['0.6596', '0.5876', '0.8088']\n",
      "Computing breakpoints...\n",
      "Breakpoints (indices of sentences where similarity dropped): [1]\n",
      "\n",
      "Reconstructed chunks:\n",
      "Chunk 1: Artificial intelligence is a powerful technology. It can be used in many applications, from self-driving cars to medical diagnosis.\n",
      "\n",
      "Chunk 2: The sun is the center of our solar system. Our solar system consists of eight planets and many moons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- Your original compute_breakpoints function ---\n",
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    if method == \"percentile\":\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# --- Helper functions to integrate with Gemini ---\n",
    "# (These functions would be defined in a larger script)\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for text using the Gemini API.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return np.array(response['embedding'], dtype=np.float32)\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# --- Full workflow example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup for Gemini (requires API key set as environment variable)\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY env var not set.\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # 1. Sample sentences to be processed\n",
    "    sentences = [\n",
    "        \"Artificial intelligence is a powerful technology.\",\n",
    "        \"It can be used in many applications, from self-driving cars to medical diagnosis.\",\n",
    "        \"The sun is the center of our solar system.\", # A topic shift here\n",
    "        \"Our solar system consists of eight planets and many moons.\"\n",
    "    ]\n",
    "\n",
    "    # 2. Get embeddings for all sentences using the Gemini API\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    embeddings = [get_embedding(s) for s in sentences]\n",
    "\n",
    "    # 3. Compute similarity between consecutive sentences\n",
    "    similarities = [cosine_similarity(embeddings[i], embeddings[i+1]) for i in range(len(embeddings)-1)]\n",
    "    print(\"\\nComputed similarities between consecutive sentences:\", [f'{s:.4f}' for s in similarities])\n",
    "\n",
    "    # 4. Use your function to find the breakpoints\n",
    "    print(\"Computing breakpoints...\")\n",
    "    breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=50) # Using a low threshold for this example\n",
    "    print(\"Breakpoints (indices of sentences where similarity dropped):\", breakpoints)\n",
    "\n",
    "    # 5. Reconstruct chunks based on the breakpoints\n",
    "    start_idx = 0\n",
    "    chunks = []\n",
    "    for bp in breakpoints:\n",
    "        chunks.append(\" \".join(sentences[start_idx : bp+1]))\n",
    "        start_idx = bp + 1\n",
    "    # Add the last chunk\n",
    "    chunks.append(\" \".join(sentences[start_idx:]))\n",
    "\n",
    "    print(\"\\nReconstructed chunks:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text into Semantic Chunks\n",
    "We split the text based on computed breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with Gemini...\n",
      "\n",
      "Computed similarities between consecutive sentences: ['0.6596', '0.5876', '0.8088']\n",
      "Computing breakpoints...\n",
      "Breakpoints (indices of sentences where similarity dropped): [1]\n",
      "\n",
      "Reconstructed chunks:\n",
      "Chunk 1: Artificial intelligence is a powerful technology. It can be used in many applications, from self-driving cars to medical diagnosis.\n",
      "\n",
      "Chunk 2: The sun is the center of our solar system. Our solar system consists of eight planets and many moons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- Your original compute_breakpoints function ---\n",
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences.\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur.\n",
    "    \"\"\"\n",
    "    if method == \"percentile\":\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# --- Helper functions to integrate with Gemini ---\n",
    "# (These functions would be defined in a larger script)\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for text using the Gemini API.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return np.array(response['embedding'], dtype=np.float32)\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# --- Full workflow example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup for Gemini (requires API key set as environment variable)\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY env var not set.\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # 1. Sample sentences to be processed\n",
    "    sentences = [\n",
    "        \"Artificial intelligence is a powerful technology.\",\n",
    "        \"It can be used in many applications, from self-driving cars to medical diagnosis.\",\n",
    "        \"The sun is the center of our solar system.\", # A topic shift here\n",
    "        \"Our solar system consists of eight planets and many moons.\"\n",
    "    ]\n",
    "\n",
    "    # 2. Get embeddings for all sentences using the Gemini API\n",
    "    print(\"Generating embeddings with Gemini...\")\n",
    "    embeddings = [get_embedding(s) for s in sentences]\n",
    "\n",
    "    # 3. Compute similarity between consecutive sentences\n",
    "    similarities = [cosine_similarity(embeddings[i], embeddings[i+1]) for i in range(len(embeddings)-1)]\n",
    "    print(\"\\nComputed similarities between consecutive sentences:\", [f'{s:.4f}' for s in similarities])\n",
    "\n",
    "    # 4. Use your function to find the breakpoints\n",
    "    print(\"Computing breakpoints...\")\n",
    "    breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=50) # Using a low threshold for this example\n",
    "    print(\"Breakpoints (indices of sentences where similarity dropped):\", breakpoints)\n",
    "\n",
    "    # 5. Reconstruct chunks based on the breakpoints\n",
    "    start_idx = 0\n",
    "    chunks = []\n",
    "    for bp in breakpoints:\n",
    "        chunks.append(\" \".join(sentences[start_idx : bp+1]))\n",
    "        start_idx = bp + 1\n",
    "    # Add the last chunk\n",
    "    chunks.append(\" \".join(sentences[start_idx:]))\n",
    "\n",
    "    print(\"\\nReconstructed chunks:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Semantic Chunks\n",
    "We create embeddings for each chunk for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunk embeddings with Gemini...\n",
      "\n",
      "Successfully created 4 chunk embeddings.\n",
      "Embedding vector shape: (768,)\n",
      "\n",
      "First embedding vector (first 5 values):\n",
      "[-0.04203147 -0.03221899  0.01675237 -0.01887356  0.02924487]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Define the get_embedding function for Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for a single text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- Your original create_embeddings function ---\n",
    "def create_embeddings(text_chunks: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each text chunk using the get_embedding function\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n",
    "# --- Main Logic Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Define some sample text chunks\n",
    "    text_chunks = [\n",
    "        \"Artificial intelligence is a powerful technology.\",\n",
    "        \"It can be used in many applications, such as self-driving cars and medical diagnosis.\",\n",
    "        \"The sun is the center of our solar system.\",\n",
    "        \"Our solar system consists of eight planets and many moons.\"\n",
    "    ]\n",
    "\n",
    "    # 2. Create chunk embeddings using the create_embeddings function\n",
    "    print(\"Creating chunk embeddings with Gemini...\")\n",
    "    chunk_embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    # 3. Filter out any empty embeddings from potential errors\n",
    "    chunk_embeddings = [emb for emb in chunk_embeddings if emb.size > 0]\n",
    "    \n",
    "    # 4. Print the results\n",
    "    if chunk_embeddings:\n",
    "        print(f\"\\nSuccessfully created {len(chunk_embeddings)} chunk embeddings.\")\n",
    "        print(f\"Embedding vector shape: {chunk_embeddings[0].shape}\")\n",
    "        print(\"\\nFirst embedding vector (first 5 values):\")\n",
    "        print(chunk_embeddings[0][:5])\n",
    "    else:\n",
    "        print(\"Failed to create any embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to retrieve the most relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Helper functions ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for a single text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model (str): Embedding model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- Your semantic_search function (revised) ---\n",
    "def semantic_search(query: str, text_chunks: List[str], chunk_embeddings: List[np.ndarray], k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n",
    "    k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the relationship between deep learning and neural networks?\n",
      "\n",
      "Top 2 most relevant text chunks:\n",
      "Context 1:\n",
      "Neural networks are a fundamental component of deep learning models.\n",
      "========================================\n",
      "Context 2:\n",
      "Deep learning is a subfield of machine learning inspired by the structure of the human brain.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. Helper Functions (Assumed to be defined elsewhere) ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for a single text using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "def semantic_search(query: str, text_chunks: List[str], chunk_embeddings: List[np.ndarray], k: int = 5) -> List[str]:\n",
    "    \"\"\"Finds the most relevant text chunks for a query.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return [text_chunks[i] for i in top_indices]\n",
    "\n",
    "# --- 3. Main Logic (Your provided code) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate text chunks and their embeddings from a previous step\n",
    "    # In a real scenario, these would be generated from a PDF or other source\n",
    "    text_chunks = [\n",
    "        \"Artificial intelligence (AI) is a wide-ranging branch of computer science.\",\n",
    "        \"The development of AI has been influenced by disciplines such as cognitive science and philosophy.\",\n",
    "        \"Deep learning is a subfield of machine learning inspired by the structure of the human brain.\",\n",
    "        \"Neural networks are a fundamental component of deep learning models.\",\n",
    "        \"Quantum computing is a field that uses quantum-mechanical phenomena.\",\n",
    "        \"The Mona Lisa is a famous portrait painting by Leonardo da Vinci.\"\n",
    "    ]\n",
    "    chunk_embeddings = [get_embedding(chunk) for chunk in text_chunks]\n",
    "    \n",
    "    # Load the validation data from a JSON file\n",
    "    # Note: I am simulating this file since I don't have access to your local files.\n",
    "    # In your case, this part will be identical.\n",
    "    data = [\n",
    "        {\"question\": \"What is the relationship between deep learning and neural networks?\", \"ideal_answer\": \"Neural networks are a fundamental component of deep learning models.\"},\n",
    "        {\"question\": \"What is AI?\", \"ideal_answer\": \"AI is a wide-ranging branch of computer science.\"}\n",
    "    ]\n",
    "\n",
    "    # Extract the first query from the validation data\n",
    "    query = data[0]['question']\n",
    "\n",
    "    # Get top 2 relevant chunks\n",
    "    top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "    # Print the query\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Print the top 2 most relevant text chunks\n",
    "    print(\"\\nTop 2 most relevant text chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"Context {i+1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunk embeddings...\n",
      "\n",
      "Searching for chunks relevant to: 'What are the main causes of homelessness?'...\n",
      "\n",
      "Generating AI response...\n",
      "Query: What are the main causes of homelessness?\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text,  family breakdown, domestic violence, lack of social support networks, economic issues, social issues, and personal issues contribute to homelessness.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for a single text using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "def semantic_search(query: str, text_chunks: List[str], chunk_embeddings: List[np.ndarray], k: int = 5) -> List[str]:\n",
    "    \"\"\"Finds the most relevant text chunks for a query.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return [text_chunks[i] for i in top_indices]\n",
    "\n",
    "# --- 3. Define the generate_response function for Gemini ---\n",
    "def generate_response(system_prompt: str, user_message: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"Generates a response from the Gemini model.\"\"\"\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_message)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 4. Main Logic: Revised for a Homelessness Query ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate text chunks that might come from documents about homelessness\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\",\n",
    "        \"A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\",\n",
    "        \"Social factors like family breakdown, domestic violence, and a lack of social support networks can also lead to homelessness.\",\n",
    "        \"Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\",\n",
    "        \"Government and non-profit organizations offer services like shelters, food banks, and temporary housing to help the homeless.\",\n",
    "        \"The best long-term solution is a holistic approach that includes providing affordable housing, mental health services, and job training.\"\n",
    "    ]\n",
    "    \n",
    "    # Create embeddings for these chunks. In a real application, this would be a one-time process.\n",
    "    print(\"Creating chunk embeddings...\")\n",
    "    chunk_embeddings = [get_embedding(chunk) for chunk in text_chunks]\n",
    "    \n",
    "    # Define a specific query related to homelessness\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    \n",
    "    # Use semantic search to find the most relevant context\n",
    "    print(f\"\\nSearching for chunks relevant to: '{query}'...\")\n",
    "    top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=3)\n",
    "    \n",
    "    # Define the system prompt for the RAG system\n",
    "    system_prompt = \"You are an assistant that strictly answers based on the given context about homelessness. If the answer is not in the context, say 'I cannot answer based on the provided information.' Be concise and direct.\"\n",
    "    \n",
    "    # Combine the top chunks and the user's query into a single prompt for the LLM\n",
    "    user_prompt = \"\\n\".join([f\"Context {i+1}:\\n{chunk}\\n{'='*40}\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "    user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "    \n",
    "    # Generate the final response using the RAG-style prompt\n",
    "    print(\"\\nGenerating AI response...\")\n",
    "    ai_response = generate_response(system_prompt, user_prompt)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nAI Response:\\n{ai_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating evaluation response with Gemini...\n",
      "\n",
      "Evaluation Score:\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(system_prompt: str, user_message: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the Gemini model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: The response from the AI model as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_message)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"Error: Could not generate a response.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query, AI response, and true response from a previous step\n",
    "    query = \"What is the capital of France?\"\n",
    "    ai_response_content = \"Paris is the capital of France.\"\n",
    "    ideal_answer = \"Paris\"\n",
    "\n",
    "    # Define the system prompt for the evaluation system\n",
    "    evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5. Provide only the score (e.g., '1', '0', or '0.5') and nothing else.\"\n",
    "\n",
    "    # Create the evaluation prompt by combining the user query, AI response, and true response\n",
    "    evaluation_prompt = (\n",
    "        f\"User Query: {query}\\n\"\n",
    "        f\"AI Response:\\n{ai_response_content}\\n\"\n",
    "        f\"True Response: {ideal_answer}\"\n",
    "    )\n",
    "\n",
    "    # Generate the evaluation response using the Gemini API\n",
    "    print(\"Generating evaluation response with Gemini...\")\n",
    "    evaluation_response_text = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "    # Print the evaluation score\n",
    "    print(\"\\nEvaluation Score:\")\n",
    "    print(evaluation_response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
