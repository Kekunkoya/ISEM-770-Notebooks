{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Contextual Compression for Enhanced RAG Systems\n",
    "In this notebook, I implement a contextual compression technique to improve our RAG system's efficiency. We'll filter and compress retrieved text chunks to keep only the most relevant parts, reducing noise and improving response quality.\n",
    "\n",
    "When retrieving documents for RAG, we often get chunks containing both relevant and irrelevant information. Contextual compression helps us:\n",
    "\n",
    "- Remove irrelevant sentences and paragraphs\n",
    "- Focus only on query-relevant information\n",
    "- Maximize the useful signal in our context window\n",
    "\n",
    "Let's implement this approach from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyMuPDF (fitz).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "        # Use a context manager to automatically close the document\n",
    "        with fitz.open(pdf_path) as mypdf:\n",
    "            # Iterate through each page to extract text\n",
    "            for page in mypdf:\n",
    "                all_text += page.get_text(\"text\") + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "let's implement a simple vector store since we cannot use FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text corresponding to the index\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata corresponding to the index\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for single text (first 5 values): [0.052571062, -0.03685706, -0.06520665, -0.04034025, 0.038206574]\n",
      "\n",
      "Number of embeddings for list: 2\n",
      "First embedding in list (first 5 values): [0.07521696, -0.034325134, -0.039195377, -0.008227663, 0.10222888]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text or list of texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API can handle both single strings and lists of strings\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=text\n",
    "        )\n",
    "        \n",
    "        # If the input was a single string, the response has a single embedding.\n",
    "        # If the input was a list, the response is a list of embeddings.\n",
    "        return response['embedding']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Create an embedding for a single string\n",
    "    single_text = \"Homelessness is a complex social issue.\"\n",
    "    embedding = create_embeddings(single_text)\n",
    "    print(f\"Embedding for single text (first 5 values): {embedding[:5]}\")\n",
    "    \n",
    "    # Example 2: Create embeddings for a list of strings\n",
    "    list_of_texts = [\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"Social factors also play a role in homelessness.\"\n",
    "    ]\n",
    "    embeddings_list = create_embeddings(list_of_texts)\n",
    "    print(f\"\\nNumber of embeddings for list: {len(embeddings_list)}\")\n",
    "    print(f\"First embedding in list (first 5 values): {embeddings_list[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf('/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf')\n",
    "    \n",
    "    # Chunk the extracted text into smaller segments\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for each text chunk\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize a simple vector store to store the chunks and their embeddings\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its corresponding embedding to the vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Contextual Compression\n",
    "This is the core of our approach - we'll use an LLM to filter and compress retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating compressed chunk using 'selective' compression...\n",
      "\n",
      "Original Length: 318 characters\n",
      "Compressed Length: 213 characters\n",
      "Compression Ratio: 33.02%\n",
      "\n",
      "Compressed Chunk:\n",
      "Homelessness is a complex social problem. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals. Social factors like family breakdown also contribute.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import Tuple\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main compression function (revised for Gemini) ---\n",
    "def compress_chunk(chunk: str, query: str, compression_type: str = \"selective\", model: str = \"gemini-1.5-flash\") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Compress a retrieved chunk by keeping only the parts relevant to the query.\n",
    "    \n",
    "    Args:\n",
    "        chunk (str): Text chunk to compress\n",
    "        query (str): User query\n",
    "        compression_type (str): Type of compression (\"selective\", \"summary\", or \"extraction\")\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, float]: Compressed chunk and compression ratio.\n",
    "    \"\"\"\n",
    "    if compression_type == \"selective\":\n",
    "        system_prompt = \"\"\"You are an expert at information filtering. Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly relevant to the user's query. Remove all irrelevant content. Your output should: 1. ONLY include text that helps answer the query 2. Preserve the exact wording of relevant sentences (do not paraphrase) 3. Maintain the original order of the text 4. Include ALL relevant content, even if it seems redundant 5. EXCLUDE any text that isn't relevant to the query. Format your response as plain text with no additional comments.\"\"\"\n",
    "    elif compression_type == \"summary\":\n",
    "        system_prompt = \"\"\"You are an expert at summarization. Your task is to create a concise summary of the provided chunk that focuses ONLY on information relevant to the user's query. Your output should: 1. Be brief but comprehensive regarding query-relevant information 2. Focus exclusively on information related to the query 3. Omit irrelevant details 4. Be written in a neutral, factual tone. Format your response as plain text with no additional comments.\"\"\"\n",
    "    else:  # extraction\n",
    "        system_prompt = \"\"\"You are an expert at information extraction. Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant to answering the user's query. Your output should: 1. Include ONLY direct quotes of relevant sentences from the original text 2. Preserve the original wording (do not modify the text) 3. Include ONLY sentences that directly relate to the query 4. Separate extracted sentences with newlines 5. Do not add any commentary or additional text. Format your response as plain text with no additional comments.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Query: {query}\\n\\nDocument Chunk:\\n{chunk}\\n\\nExtract only the content relevant to answering this query.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create a Gemini model instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        compressed_chunk = response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during compression: {e}\")\n",
    "        return chunk, 0.0 # Return original chunk with 0 ratio on error\n",
    "\n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100 if original_length > 0 else 0.0\n",
    "    \n",
    "    return compressed_chunk, compression_ratio\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a document chunk and query\n",
    "    sample_chunk = \"\"\"\n",
    "    Homelessness is a complex social problem. A key factor is the lack of affordable housing, which\n",
    "    disproportionately affects low-income families and individuals. The sun is the center of our solar system,\n",
    "    and Pluto is now considered a dwarf planet. Social factors like family breakdown also contribute.\n",
    "    \"\"\"\n",
    "    query_text = \"What factors contribute to homelessness?\"\n",
    "\n",
    "    print(\"Generating compressed chunk using 'selective' compression...\")\n",
    "    compressed, ratio = compress_chunk(sample_chunk, query_text, compression_type=\"selective\")\n",
    "    \n",
    "    print(f\"\\nOriginal Length: {len(sample_chunk)} characters\")\n",
    "    print(f\"Compressed Length: {len(compressed)} characters\")\n",
    "    print(f\"Compression Ratio: {ratio:.2f}%\")\n",
    "    print(\"\\nCompressed Chunk:\")\n",
    "    print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Batch Compression\n",
    "For efficiency, we'll compress multiple chunks in one go when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing 3 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Compressing Chunks: 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall compression ratio: 19.85%\n",
      "\n",
      "Result 1:\n",
      "Compressed Chunk (Ratio 47.19%): A key factor is the lack of affordable housing.\n",
      "\n",
      "Result 2:\n",
      "Compressed Chunk (Ratio 13.33%): There is no information about the causes of homelessness in the provided text.\n",
      "\n",
      "Result 3:\n",
      "Compressed Chunk (Ratio 0.00%): Social factors like family breakdown, and mental health issues can also lead to homelessness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper function (revised for Gemini) ---\n",
    "def compress_chunk(chunk: str, query: str, compression_type: str = \"selective\", model: str = \"gemini-1.5-flash\") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Compress a retrieved chunk using a Gemini LLM.\n",
    "    \"\"\"\n",
    "    if compression_type == \"selective\":\n",
    "        system_prompt = \"You are an expert at information filtering. Your task is to extract ONLY the sentences or paragraphs that are directly relevant to the user's query. Preserve the exact wording.\"\n",
    "    elif compression_type == \"summary\":\n",
    "        system_prompt = \"You are an expert at summarization. Your task is to create a concise summary of the provided chunk that focuses ONLY on information relevant to the user's query.\"\n",
    "    else: # extraction\n",
    "        system_prompt = \"You are an expert at information extraction. Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant to answering the user's query.\"\n",
    "    \n",
    "    user_prompt = f\"Query: {query}\\n\\nDocument Chunk:\\n{chunk}\\n\\nExtract only the content relevant to answering this query.\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        compressed_chunk = response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during compression: {e}\")\n",
    "        return chunk, 0.0\n",
    "\n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100 if original_length > 0 else 0.0\n",
    "    \n",
    "    return compressed_chunk, compression_ratio\n",
    "\n",
    "# --- 3. The main batch compression function (revised for Gemini) ---\n",
    "def batch_compress_chunks(chunks: List[str], query: str, compression_type: str = \"selective\", model: str = \"gemini-1.5-flash\") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Compress multiple chunks individually.\n",
    "    \"\"\"\n",
    "    print(f\"Compressing {len(chunks)} chunks...\")\n",
    "    results = []\n",
    "    total_original_length = 0\n",
    "    total_compressed_length = 0\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Batch Compressing Chunks\"):\n",
    "        compressed_chunk, compression_ratio = compress_chunk(chunk, query, compression_type, model)\n",
    "        results.append((compressed_chunk, compression_ratio))\n",
    "        \n",
    "        total_original_length += len(chunk)\n",
    "        total_compressed_length += len(compressed_chunk)\n",
    "    \n",
    "    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100 if total_original_length > 0 else 0.0\n",
    "    print(f\"Overall compression ratio: {overall_ratio:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- 4. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a list of chunks and a query\n",
    "    sample_chunks = [\n",
    "        \"Homelessness is a complex social problem. A key factor is the lack of affordable housing.\",\n",
    "        \"The sun is the star at the center of the solar system. Pluto is considered a dwarf planet.\",\n",
    "        \"Social factors like family breakdown, and mental health issues can also lead to homelessness.\"\n",
    "    ]\n",
    "    query_text = \"What are the causes of homelessness?\"\n",
    "\n",
    "    # Compress the chunks in a batch\n",
    "    compressed_results = batch_compress_chunks(sample_chunks, query_text, compression_type=\"selective\")\n",
    "    \n",
    "    # Print the results\n",
    "    for i, (compressed_chunk, ratio) in enumerate(compressed_results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Compressed Chunk (Ratio {ratio:.2f}%): {compressed_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, a key factor contributing to homelessness is the lack of affordable housing.  This affects low-income families and individuals disproportionately.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text from compressed chunks\n",
    "        model (str): LLM model to use\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context.\n",
    "If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based only on the context above.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        \n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and context from a previous step\n",
    "    query = \"What are the main causes of homelessness?\"\n",
    "    context = \"Homelessness is a complex social problem. A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\"\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, context)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete RAG Pipeline with Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the main contributing factors to homelessness?\n",
      "Compression type: selective\n",
      "Retrieving top 10 chunks...\n",
      "Generating response based on compressed chunks...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The provided text does not contain information about the main contributing factors to homelessness.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Gemini API Configuration and Helper Functions ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"A placeholder for your PDF extraction function.\"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"A placeholder for your text chunking function.\"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for i in range(0, len(text), step):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(texts: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"Creates embeddings for a list of texts using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        # Gemini returns a list of embeddings, even for a single text.\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"A placeholder for your vector store class.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.documents.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors: return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if np.linalg.norm(query_vector) == 0 or np.linalg.norm(vector) == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append({\"text\": self.documents[i], \"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "def batch_compress_chunks(chunks: List[str], query: str, compression_type: str, model: str) -> List[Tuple[str, float]]:\n",
    "    \"\"\"A placeholder for your batch chunk compression function.\"\"\"\n",
    "    compressed_results = []\n",
    "    for chunk in chunks:\n",
    "        # Here's where the LLM call happens inside your function\n",
    "        # This is the part that would be Gemini-compatible\n",
    "        compressed_text = f\"Compressed: '{chunk[:50]}...'\"\n",
    "        ratio = 50.0\n",
    "        compressed_results.append((compressed_text, ratio))\n",
    "    return compressed_results\n",
    "\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"Generate a response based on the query and context using Gemini.\"\"\"\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If the context doesn't contain the answer, say so clearly.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "def process_document(pdf_path: str, chunk_size: int = 800) -> SimpleVectorStore:\n",
    "    \"\"\"Process a document to create chunks and a vector store.\"\"\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text, chunk_size, 0)\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    metadata = [{\"index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    \n",
    "    for chunk, embedding, meta in zip(chunks, chunk_embeddings, metadata):\n",
    "        vector_store.add_item(chunk, embedding, meta)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# --- 2. Your original `rag_with_compression` function ---\n",
    "def rag_with_compression(pdf_path: str, query: str, k: int = 10, compression_type: str = \"selective\", model: str = \"gemini-1.5-flash\") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with contextual compression.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        k (int): Number of chunks to retrieve initially\n",
    "        compression_type (str): Type of compression\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including query, compressed chunks, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Compression type: {compression_type}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve the top k most similar chunks based on the query embedding\n",
    "    print(f\"Retrieving top {k} chunks...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # Apply compression to the retrieved chunks\n",
    "    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model)\n",
    "    compressed_chunks = [result[0] for result in compressed_results]\n",
    "    compression_ratios = [result[1] for result in compressed_results]\n",
    "    \n",
    "    # Filter out any empty compressed chunks\n",
    "    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n",
    "    \n",
    "    if not filtered_chunks:\n",
    "        print(\"Warning: All chunks were compressed to empty strings. Using original chunks.\")\n",
    "        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n",
    "    \n",
    "    compressed_chunks, compression_ratios = zip(*filtered_chunks)\n",
    "    \n",
    "    # Generate context from the compressed chunks\n",
    "    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n",
    "    \n",
    "    # Generate a response based on the compressed chunks\n",
    "    print(\"Generating response based on compressed chunks...\")\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"original_chunks\": retrieved_chunks,\n",
    "        \"compressed_chunks\": compressed_chunks,\n",
    "        \"compression_ratios\": compression_ratios,\n",
    "        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 3. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf'\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_file_path}'\")\n",
    "        exit()\n",
    "    \n",
    "    user_query = \"What are the main contributing factors to homelessness?\"\n",
    "    rag_with_compression(pdf_file_path, user_query, compression_type=\"selective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing RAG With and Without Compression\n",
    "Let's create a function to compare standard RAG with our compression-enhanced version:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STANDARD RAG ===\n",
      "Query: What are the key contributing factors to homelessness?\n",
      "Creating query embedding and retrieving chunks...\n",
      "Generating response...\n",
      "\n",
      "=== RESPONSE ===\n",
      "This document discusses methods for measuring and understanding homelessness,  including the types of homelessness (long-term, episodic, etc.) and data collection strategies, but it does not identify the key contributing factors to homelessness.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    all_text = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                all_text.append(page.get_text(\"text\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Chunks the given text into segments.\"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for i in range(0, len(text), step):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(texts: str or List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"Creates embeddings for a list of texts using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        if not texts:\n",
    "            return []\n",
    "        # The embed_content endpoint takes a list of strings\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"A simple vector store implementation using NumPy.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_documents(self, documents: List[str], vectors: List[Any], metadata: List[Dict]):\n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(np.array(vec, dtype=np.float32))\n",
    "            self.metadata.append(meta)\n",
    "\n",
    "    def similarity_search(self, query_embedding: Any, top_k: int = 5) -> List[Dict]:\n",
    "        query_array = np.array(query_embedding, dtype=np.float32)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if np.linalg.norm(query_array) == 0 or np.linalg.norm(vector) == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = np.dot(query_array, vector) / (np.linalg.norm(query_array) * np.linalg.norm(vector))\n",
    "            similarities.append({\"document\": self.documents[i], \"score\": similarity, \"metadata\": self.metadata[i]})\n",
    "        similarities.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "def process_document(pdf_path: str, chunk_size: int) -> SimpleVectorStore:\n",
    "    \"\"\"Process a document to create chunks and a vector store.\"\"\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text, chunk_size, 0)\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    if not chunk_embeddings or len(chunks) != len(chunk_embeddings):\n",
    "        raise RuntimeError(\"Failed to create embeddings or embedding count mismatch.\")\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    metadata = [{\"index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def generate_response(query: str, context: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"Generate a response based on the query and context using Gemini.\"\"\"\n",
    "    system_prompt = \"You are a helpful assistant that answers questions based on the provided context. If the context doesn't contain the answer, say so clearly.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 2. Your original `standard_rag` function ---\n",
    "def standard_rag(pdf_path: str, query: str, k: int = 10, model: str = \"gemini-1.5-flash\") -> Dict:\n",
    "    \"\"\"\n",
    "    Standard RAG without compression.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STANDARD RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    vector_store = process_document(pdf_path, chunk_size=800)\n",
    "    \n",
    "    print(\"Creating query embedding and retrieving chunks...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    results = vector_store.similarity_search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"document\"] for result in results]\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 3. Main Logic for a runnable example ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf'\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_file_path}'\")\n",
    "        exit()\n",
    "    \n",
    "    user_query = \"What are the key contributing factors to homelessness?\"\n",
    "    standard_rag(pdf_file_path, user_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Approach\n",
    "Now, let's implement a function to evaluate and compare the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression(pdf_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
    "    \"\"\"\n",
    "    Compare different compression techniques with standard RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        reference_answer (str): Optional reference answer\n",
    "        compression_types (List[str]): Compression types to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EVALUATING CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Run standard RAG without compression\n",
    "    standard_result = standard_rag(pdf_path, query)\n",
    "    \n",
    "    # Dictionary to store results of different compression techniques\n",
    "    compression_results = {}\n",
    "    \n",
    "    # Run RAG with each compression technique\n",
    "    for comp_type in compression_types:\n",
    "        print(f\"\\nTesting {comp_type} compression...\")\n",
    "        compression_results[comp_type] = rag_with_compression(pdf_path, query, compression_type=comp_type)\n",
    "    \n",
    "    # Gather responses for evaluation\n",
    "    responses = {\n",
    "        \"standard\": standard_result[\"response\"]\n",
    "    }\n",
    "    for comp_type in compression_types:\n",
    "        responses[comp_type] = compression_results[comp_type][\"response\"]\n",
    "    \n",
    "    # Evaluate responses if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        evaluation = evaluate_responses(query, responses, reference_answer)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation)\n",
    "    else:\n",
    "        evaluation = \"No reference answer provided for evaluation.\"\n",
    "    \n",
    "    # Calculate metrics for each compression type\n",
    "    metrics = {}\n",
    "    for comp_type in compression_types:\n",
    "        metrics[comp_type] = {\n",
    "            \"avg_compression_ratio\": f\"{sum(compression_results[comp_type]['compression_ratios'])/len(compression_results[comp_type]['compression_ratios']):.2f}%\",\n",
    "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n",
    "            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n",
    "        }\n",
    "    \n",
    "    # Return the evaluation results, responses, and metrics\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"responses\": responses,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"metrics\": metrics,\n",
    "        \"standard_result\": standard_result,\n",
    "        \"compression_results\": compression_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Complete System (Custom Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SELECTIVE EVALUATION ---\n",
      "**Faithfulness:** The RAG answer is completely unfaithful to the provided context.  The context is an introduction to artificial intelligence, containing no information whatsoever about European homelessness strategies or policy objectives. The RAG answer hallucinates information entirely unrelated to the input.\n",
      "\n",
      "**Relevance:** The RAG answer is completely irrelevant to the provided context. The question about European homelessness strategies is unrelated to the text about artificial intelligence.  The generated answer, even if factually correct (which it may not be), is entirely out of scope.\n",
      "\n",
      "\n",
      "**Overall:** The performance of the RAG system is extremely poor in this instance.  It demonstrates a complete failure to appropriately leverage the provided context and instead generates a fabricated response.  The lack of faithfulness and relevance indicates a critical flaw in the system's ability to perform its intended function.\n",
      "\n",
      "\n",
      "--- SUMMARY EVALUATION ---\n",
      "The RAG answer is completely unfaithful and irrelevant to the provided context.  The context describes applications of AI, while the question asks about European homelessness strategies.  The RAG answer provides a fabricated response (\"Dummy RAG answer\") that has no connection to the provided context or the actual policy objectives of European homelessness strategies.  There's no basis in the context for the mentioned elements (subsidized housing listings, predictive AI for prevention, supportive programs).  It's essentially a hallucination.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "* **Faithfulness:**  Extremely low.  The answer is entirely fabricated and bears no resemblance to factual information.\n",
      "* **Relevance:** Extremely low. The answer is unrelated to both the question and the given context.\n",
      "\n",
      "\n",
      "--- EXTRACTION EVALUATION ---\n",
      "**Faithfulness:** The RAG answer is completely unfaithful to the provided context.  The context document discusses AI ethics and responsible AI development; it contains absolutely no information about European homelessness strategies or their policy objectives. The RAG answer is entirely fabricated.\n",
      "\n",
      "**Relevance:** The RAG answer is irrelevant to the question and the provided context. The question asks about specific policy objectives in European homelessness strategies, while the context discusses a completely unrelated topic.  The generated answer is not only wrong but also off-topic.\n",
      "\n",
      "**Overall Assessment:** The RAG answer is both unfaithful and irrelevant.  It demonstrates a complete failure to extract information from the provided context and generate a meaningful response to the question.  The quality is extremely poor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize Gemini client\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks = [page.get_text(\"text\") for page in doc]\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_chunks)\n",
    "\n",
    "def compress_text(full_text: str, method: str) -> str:\n",
    "    \"\"\"Stub for text compression.\"\"\"\n",
    "    length = len(full_text)\n",
    "    if method == \"selective\":\n",
    "        return full_text[:500]\n",
    "    elif method == \"summary\":\n",
    "        start = max(0, (length - 500) // 2)\n",
    "        return full_text[start:start+500]\n",
    "    elif method == \"extraction\":\n",
    "        return full_text[-500:]\n",
    "    else:\n",
    "        return full_text\n",
    "\n",
    "def run_rag_pipeline(context: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Stub for RAG pipeline.\n",
    "    Returns a dummy answer. Replace with actual retrieval and generation steps.\n",
    "    \"\"\"\n",
    "    return f\"Dummy RAG answer for query: '{query}' based on provided context.\"\n",
    "\n",
    "def evaluate_compression(pdf_path: str,\n",
    "                         query: str,\n",
    "                         reference_answer: str,\n",
    "                         compression_types: list[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Applies each compression method, runs a dummy RAG, and evaluates against a reference using Gemini.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    for ctype in compression_types:\n",
    "        compressed_ctx = compress_text(full_text, method=ctype)\n",
    "        rag_answer = run_rag_pipeline(compressed_ctx, query)\n",
    "        \n",
    "        # If no reference, store raw answer\n",
    "        if not reference_answer.strip():\n",
    "            results[ctype] = rag_answer\n",
    "            continue\n",
    "        \n",
    "        eval_prompt = (\n",
    "            f\"Compression type: {ctype}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Context:\\n{compressed_ctx}\\n\\n\"\n",
    "            f\"RAG Answer: {rag_answer}\\n\\n\"\n",
    "            f\"Reference Answer: {reference_answer}\\n\\n\"\n",
    "            \"Evaluate for faithfulness and relevance. Provide details.\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Create a Gemini model instance with the system prompt\n",
    "            resp = genai.GenerativeModel(\n",
    "                \"gemini-1.5-flash\",\n",
    "                system_instruction=\"You are an objective evaluator.\"\n",
    "            ).generate_content(eval_prompt)\n",
    "            results[ctype] = resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation for {ctype}: {e}\")\n",
    "            results[ctype] = \"Evaluation failed due to an error.\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf\"\n",
    "query = \"What are typical policy objectives in European homelessness strategies?\"\n",
    "reference_answer = \"\"\"\n",
    "Section Setting concrete targets\n",
    "listing of subsidized housing in the newspapers\n",
    "prevention reduction of homelessness through predictive AI\n",
    "supportive programs through workplace, hospitals and churches\n",
    "\"\"\"\n",
    "compression_types = [\"selective\", \"summary\", \"extraction\"]\n",
    "\n",
    "results = evaluate_compression(\n",
    "    pdf_path=pdf_path,\n",
    "    query=query,\n",
    "    reference_answer=reference_answer,\n",
    "    compression_types=compression_types\n",
    ")\n",
    "\n",
    "for ctype, eval_text in results.items():\n",
    "    print(f\"\\n--- {ctype.upper()} EVALUATION ---\\n{eval_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Compression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_compression_results(evaluation_results):\n",
    "    \"\"\"\n",
    "    Visualize the results of different compression techniques.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results (Dict): Results from evaluate_compression function\n",
    "    \"\"\"\n",
    "    # Extract the query and standard chunks from the evaluation results\n",
    "    query = evaluation_results[\"query\"]\n",
    "    standard_chunks = evaluation_results[\"standard_result\"][\"chunks\"]\n",
    "    \n",
    "    # Print the query\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Get a sample chunk to visualize (using the first chunk)\n",
    "    original_chunk = standard_chunks[0]\n",
    "    \n",
    "    # Iterate over each compression type and show a comparison\n",
    "    for comp_type in evaluation_results[\"compression_results\"].keys():\n",
    "        compressed_chunks = evaluation_results[\"compression_results\"][comp_type][\"compressed_chunks\"]\n",
    "        compression_ratios = evaluation_results[\"compression_results\"][comp_type][\"compression_ratios\"]\n",
    "        \n",
    "        # Get the corresponding compressed chunk and its compression ratio\n",
    "        compressed_chunk = compressed_chunks[0]\n",
    "        compression_ratio = compression_ratios[0]\n",
    "        \n",
    "        print(f\"\\n=== {comp_type.upper()} COMPRESSION EXAMPLE ===\\n\")\n",
    "        \n",
    "        # Show the original chunk (truncated if too long)\n",
    "        print(\"ORIGINAL CHUNK:\")\n",
    "        print(\"-\" * 40)\n",
    "        if len(original_chunk) > 800:\n",
    "            print(original_chunk[:800] + \"... [truncated]\")\n",
    "        else:\n",
    "            print(original_chunk)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Length: {len(original_chunk)} characters\\n\")\n",
    "        \n",
    "        # Show the compressed chunk\n",
    "        print(\"COMPRESSED CHUNK:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(compressed_chunk)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Length: {len(compressed_chunk)} characters\")\n",
    "        print(f\"Compression ratio: {compression_ratio:.2f}%\\n\")\n",
    "        \n",
    "        # Show overall statistics for this compression type\n",
    "        avg_ratio = sum(compression_ratios) / len(compression_ratios)\n",
    "        print(f\"Average compression across all chunks: {avg_ratio:.2f}%\")\n",
    "        print(f\"Total context length reduction: {evaluation_results['metrics'][comp_type]['avg_compression_ratio']}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Show a summary table of compression techniques\n",
    "    print(\"\\n=== COMPRESSION SUMMARY ===\\n\")\n",
    "    print(f\"{'Technique':<15} {'Avg Ratio':<15} {'Context Length':<15} {'Original Length':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Print the metrics for each compression type\n",
    "    for comp_type, metrics in evaluation_results[\"metrics\"].items():\n",
    "        print(f\"{comp_type:<15} {metrics['avg_compression_ratio']:<15} {metrics['total_context_length']:<15} {metrics['original_context_length']:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SELECTIVE ---\n",
      "**Faithfulness:** The RAG answer is completely unfaithful to the provided context.  The context is an introduction to artificial intelligence, containing no information whatsoever about European homelessness strategies or policy objectives. The RAG answer hallucinates information entirely unrelated to the input.\n",
      "\n",
      "**Relevance:** The RAG answer is completely irrelevant to the provided context. The question about European homelessness strategies is unrelated to the text about artificial intelligence.  The generated answer, even if factually correct (which it may not be), is entirely out of scope.\n",
      "\n",
      "\n",
      "**Overall:** The performance of the RAG system is extremely poor in this instance.  It demonstrates a complete failure to appropriately leverage the provided context and instead generates a fabricated response.  The lack of faithfulness and relevance indicates a critical flaw in the system's ability to perform its intended function.\n",
      "\n",
      "\n",
      "--- SUMMARY ---\n",
      "The RAG answer is completely unfaithful and irrelevant to the provided context.  The context describes applications of AI, while the question asks about European homelessness strategies.  The RAG answer provides a fabricated response (\"Dummy RAG answer\") that has no connection to the provided context or the actual policy objectives of European homelessness strategies.  There's no basis in the context for the mentioned elements (subsidized housing listings, predictive AI for prevention, supportive programs).  It's essentially a hallucination.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "* **Faithfulness:**  Extremely low.  The answer is entirely fabricated and bears no resemblance to factual information.\n",
      "* **Relevance:** Extremely low. The answer is unrelated to both the question and the given context.\n",
      "\n",
      "\n",
      "--- EXTRACTION ---\n",
      "**Faithfulness:** The RAG answer is completely unfaithful to the provided context.  The context document discusses AI ethics and responsible AI development; it contains absolutely no information about European homelessness strategies or their policy objectives. The RAG answer is entirely fabricated.\n",
      "\n",
      "**Relevance:** The RAG answer is irrelevant to the question and the provided context. The question asks about specific policy objectives in European homelessness strategies, while the context discusses a completely unrelated topic.  The generated answer is not only wrong but also off-topic.\n",
      "\n",
      "**Overall Assessment:** The RAG answer is both unfaithful and irrelevant.  It demonstrates a complete failure to extract information from the provided context and generate a meaningful response to the question.  The quality is extremely poor.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_compression_results(evaluation_results: dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Print out each compression type alongside its evaluation.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: dict mapping compression_type -> evaluation text\n",
    "    \"\"\"\n",
    "    for method, evaluation in evaluation_results.items():\n",
    "        print(f\"--- {method.upper()} ---\")\n",
    "        print(evaluation)\n",
    "        print()\n",
    "\n",
    "# Now call it:\n",
    "visualize_compression_results(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
