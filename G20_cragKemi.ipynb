{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Corrective RAG (CRAG) Implementation\n",
    "\n",
    "In this notebook, I implement Corrective RAG - an advanced approach that dynamically evaluates retrieved information and corrects the retrieval process when necessary, using web search as a fallback.\n",
    "\n",
    "CRAG improves on traditional RAG by:\n",
    "\n",
    "- Evaluating retrieved content before using it\n",
    "- Dynamically switching between knowledge sources based on relevance\n",
    "- Correcting the retrieval with web search when local knowledge is insufficient\n",
    "- Combining information from multiple sources when appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enitre Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector stores...\n",
      "\n",
      "=== Response ===\n",
      "Based on the document, key points include:\n",
      "\n",
      "Loading existing vector stores...\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "Avg(Hierarchical)=0.000, Avg(Standard)=0.000 → Tie\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Configure your API key here\n",
    "# genai.configure(api_key=\"YOUR_GOOGLE_API_KEY\")\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings: List[np.ndarray] = []\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add_item(self, *, text: str, embedding: Union[np.ndarray, List[float]], metadata: Optional[Dict[str, Any]] = None) -> None:\n",
    "        doc = {\"text\": text, \"metadata\": (metadata or {})}\n",
    "        self._add_core(embedding, doc)\n",
    "\n",
    "    def add(self, embedding: Union[np.ndarray, List[float]], document: Union[Dict[str, Any], str]) -> None:\n",
    "        if isinstance(document, str):\n",
    "            doc = {\"text\": document, \"metadata\": {}}\n",
    "        else:\n",
    "            doc = {\"text\": document.get(\"text\", \"\"), \"metadata\": document.get(\"metadata\", {})}\n",
    "        self._add_core(embedding, doc)\n",
    "\n",
    "    def similarity_search(self, query_embedding: Union[np.ndarray, List[float]], k: int = 5, filter_func: Optional[Callable[[Dict[str, Any]], bool]] = None, return_scores: bool = False):\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        \n",
    "        q = self._as_unit_vector(query_embedding)\n",
    "        sims: List[Tuple[int, float]] = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            doc = self.documents[i]\n",
    "            if filter_func is not None and not filter_func(doc):\n",
    "                continue\n",
    "            sim = float(np.dot(q, emb))\n",
    "            sims.append((i, sim))\n",
    "\n",
    "        if not sims:\n",
    "            return []\n",
    "        \n",
    "        sims.sort(key=lambda t: t[1], reverse=True)\n",
    "        top = sims[:max(0, int(k))]\n",
    "        \n",
    "        if return_scores:\n",
    "            return [(self.documents[i], score) for i, score in top]\n",
    "        else:\n",
    "            return [self.documents[i] for i, _ in top]\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump({\"embeddings\": self.embeddings, \"documents\": self.documents}, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"SimpleVectorStore\":\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        store = cls()\n",
    "        store.embeddings = [np.asarray(e, dtype=float).reshape(-1) for e in data.get(\"embeddings\", [])]\n",
    "        store.documents = data.get(\"documents\", [])\n",
    "        store.embeddings = [store._as_unit_vector(e) for e in store.embeddings]\n",
    "        return store\n",
    "\n",
    "    def _add_core(self, embedding: Union[np.ndarray, List[float]], doc: Dict[str, Any]) -> None:\n",
    "        emb = self._as_unit_vector(embedding)\n",
    "        self.embeddings.append(emb)\n",
    "        self.documents.append(doc)\n",
    "\n",
    "    def _as_unit_vector(self, x: Union[np.ndarray, List[float]]) -> np.ndarray:\n",
    "        v = np.asarray(x, dtype=float).reshape(-1)\n",
    "        n = np.linalg.norm(v)\n",
    "        if n == 0:\n",
    "            return v\n",
    "        return v / n\n",
    "\n",
    "def _safe_get_page(doc: Dict[str, Any]):\n",
    "    meta = doc.get(\"metadata\") or {}\n",
    "    for k in (\"page\", \"page_num\", \"page_number\", \"pg\"):\n",
    "        if k in meta:\n",
    "            return meta[k]\n",
    "    return None\n",
    "\n",
    "def extract_text_pages(pdf_path: str) -> List[str]:\n",
    "    pages = []\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for p in reader.pages:\n",
    "                pages.append(p.extract_text() or \"\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {pdf_path} was not found.\")\n",
    "        return []\n",
    "    return pages\n",
    "\n",
    "def chunk_text(text: str, chunk_size=1000, overlap=200) -> List[Tuple[str, int]]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append((text[start:end], start))\n",
    "        if end == n: break\n",
    "        start = max(end - overlap, start + 1)\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(texts: Union[str, List[str]], model=\"models/embedding-001\", max_retries=3, sleep_sec=1.5):\n",
    "    if not texts:\n",
    "        return [] if isinstance(texts, list) else None\n",
    "\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    vectors = []\n",
    "    for t in input_texts:\n",
    "        if t is None:\n",
    "            vectors.append([])\n",
    "            continue\n",
    "        text = str(t)\n",
    "        \n",
    "        last_err = None\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                resp = genai.embed_content(model=model, content=text)\n",
    "                if isinstance(resp, dict):\n",
    "                    emb = resp.get(\"embedding\", {})\n",
    "                    values = emb.get(\"values\", [])\n",
    "                else:\n",
    "                    values = getattr(getattr(resp, \"embedding\", None), \"values\", [])\n",
    "                vectors.append(list(values))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if attempt < max_retries:\n",
    "                    time.sleep(sleep_sec * attempt)\n",
    "                else:\n",
    "                    vectors.append([])\n",
    "        if last_err and vectors[-1] == []:\n",
    "            pass\n",
    "\n",
    "    return vectors if isinstance(texts, list) else vectors[0]\n",
    "\n",
    "def generate_page_summary(page_text: str) -> str:\n",
    "    system_prompt = \"You are an AI assistant. Summarize the following text in a concise paragraph.\"\n",
    "    max_chars = 3000\n",
    "    truncated_text = page_text[:max_chars] if len(page_text) > max_chars else page_text\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(\n",
    "        f\"{system_prompt}\\n\\n{truncated_text}\",\n",
    "        generation_config={\"temperature\": 0.3}\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "def process_document_hierarchically(pdf_path: str, chunk_size=1000, chunk_overlap=200):\n",
    "    pages = extract_text_pages(pdf_path)\n",
    "    if not pages:\n",
    "        return SimpleVectorStore(), SimpleVectorStore()\n",
    "    \n",
    "    detailed_store = SimpleVectorStore()\n",
    "    detailed_text_items = []\n",
    "    for page_idx, page_text in enumerate(pages):\n",
    "        page_text = page_text.strip()\n",
    "        if not page_text:\n",
    "            continue\n",
    "        detailed_text_items.append((page_idx, page_text))\n",
    "        for chunk_text_str, start_idx in chunk_text(page_text, chunk_size, chunk_overlap):\n",
    "            emb = create_embeddings(chunk_text_str)\n",
    "            if emb:\n",
    "                detailed_store.add_item(\n",
    "                    text=chunk_text_str,\n",
    "                    embedding=emb,\n",
    "                    metadata={\"page\": page_idx, \"char_start\": start_idx}\n",
    "                )\n",
    "\n",
    "    summary_store = SimpleVectorStore()\n",
    "    for page_idx, full_text in detailed_text_items:\n",
    "        summary_text = full_text[:350] if len(full_text) > 350 else full_text\n",
    "        emb = create_embeddings(summary_text)\n",
    "        if emb:\n",
    "            summary_store.add_item(\n",
    "                text=summary_text,\n",
    "                embedding=emb,\n",
    "                metadata={\"page\": page_idx}\n",
    "            )\n",
    "    return summary_store, detailed_store\n",
    "\n",
    "def retrieve_hierarchically(query: str, summary_store: SimpleVectorStore, detailed_store: SimpleVectorStore, k_summaries=5, k_chunks=10):\n",
    "    query_emb = create_embeddings(query)\n",
    "    if not query_emb:\n",
    "        return []\n",
    "    \n",
    "    summary_docs = summary_store.similarity_search(query_emb, k=k_summaries, return_scores=False)\n",
    "    relevant_pages = {p for p in (_safe_get_page(d) for d in summary_docs) if p is not None and p != -1}\n",
    "\n",
    "    if not relevant_pages:\n",
    "        return detailed_store.similarity_search(query_emb, k=max(1, k_chunks), return_scores=False)\n",
    "\n",
    "    def page_filter(doc):\n",
    "        return _safe_get_page(doc) in relevant_pages\n",
    "\n",
    "    detailed_docs = detailed_store.similarity_search(\n",
    "        query_emb,\n",
    "        k=max(1, k_chunks * max(1, len(relevant_pages))),\n",
    "        filter_func=page_filter,\n",
    "        return_scores=False\n",
    "    )\n",
    "    return detailed_docs\n",
    "\n",
    "def generate_response(query: str, retrieved_docs: List[Dict[str, Any]], max_chars=1200):\n",
    "    context = \"\"\n",
    "    for d in retrieved_docs:\n",
    "        t = d.get(\"text\", \"\")\n",
    "        if not t: continue\n",
    "        space_left = max_chars - len(context)\n",
    "        if space_left <= 0: break\n",
    "        context += (t[:space_left] + \"\\n\")\n",
    "    answer = f\"Based on the document, key points include:\\n{context.strip()}\"\n",
    "    return answer\n",
    "\n",
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k_summaries=5, k_chunks=10, regenerate=False):\n",
    "    base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    summary_pkl = f\"{base}__summary_store.pkl\"\n",
    "    detailed_pkl = f\"{base}__detailed_store.pkl\"\n",
    "\n",
    "    if regenerate or not (os.path.exists(summary_pkl) and os.path.exists(detailed_pkl)):\n",
    "        print(\"Processing document and creating vector stores...\")\n",
    "        summary_store, detailed_store = process_document_hierarchically(pdf_path, chunk_size, chunk_overlap)\n",
    "        with open(summary_pkl, \"wb\") as f: pickle.dump(summary_store, f)\n",
    "        with open(detailed_pkl, \"wb\") as f: pickle.dump(detailed_store, f)\n",
    "    else:\n",
    "        print(\"Loading existing vector stores...\")\n",
    "        with open(summary_pkl, \"rb\") as f: summary_store = pickle.load(f)\n",
    "        with open(detailed_pkl, \"rb\") as f: detailed_store = pickle.load(f)\n",
    "\n",
    "    retrieved = retrieve_hierarchically(query, summary_store, detailed_store, k_summaries, k_chunks)\n",
    "    response = generate_response(query, retrieved)\n",
    "\n",
    "    return {\"response\": response, \"chunks_used\": retrieved}\n",
    "\n",
    "def standard_rag(query: str, pdf_path: str, k=15):\n",
    "    base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    detailed_pkl = f\"{base}__detailed_store.pkl\"\n",
    "    if not os.path.exists(detailed_pkl):\n",
    "        _ = hierarchical_rag(\"warmup\", pdf_path, regenerate=True)\n",
    "    with open(detailed_pkl, \"rb\") as f:\n",
    "        detailed_store = pickle.load(f)\n",
    "\n",
    "    query_emb = create_embeddings(query)\n",
    "    docs = detailed_store.similarity_search(query_emb, k=k, return_scores=False)\n",
    "    return {\"response\": generate_response(query, docs), \"chunks_used\": docs}\n",
    "\n",
    "def _cosine(a, b):\n",
    "    a = np.asarray(a, dtype=float).reshape(-1)\n",
    "    b = np.asarray(b, dtype=float).reshape(-1)\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0: return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "def run_evaluation(pdf_path: str, test_queries: List[str], reference_answers: List[str]):\n",
    "    results = []\n",
    "    for q in test_queries:\n",
    "        hier = hierarchical_rag(q, pdf_path)\n",
    "        base = standard_rag(q, pdf_path)\n",
    "\n",
    "        hier_emb = create_embeddings(hier[\"response\"])\n",
    "        base_emb = create_embeddings(base[\"response\"])\n",
    "\n",
    "        ref_scores_h = []\n",
    "        ref_scores_b = []\n",
    "        for ref in reference_answers:\n",
    "            ref_emb = create_embeddings(ref)\n",
    "            ref_scores_h.append(_cosine(hier_emb, ref_emb))\n",
    "            ref_scores_b.append(_cosine(base_emb, ref_emb))\n",
    "\n",
    "        results.append({\n",
    "            \"query\": q,\n",
    "            \"hierarchical_score\": max(ref_scores_h) if ref_scores_h else 0.0,\n",
    "            \"standard_score\": max(ref_scores_b) if ref_scores_b else 0.0,\n",
    "            \"hierarchical_response\": hier[\"response\"],\n",
    "            \"standard_response\": base[\"response\"],\n",
    "        })\n",
    "\n",
    "    avg_h = sum(r[\"hierarchical_score\"] for r in results) / max(1, len(results))\n",
    "    avg_b = sum(r[\"standard_score\"] for r in results) / max(1, len(results))\n",
    "    verdict = \"Hierarchical > Standard\" if avg_h > avg_b else (\"Standard > Hierarchical\" if avg_b > avg_h else \"Tie\")\n",
    "\n",
    "    return {\n",
    "        \"per_query\": results,\n",
    "        \"overall\": {\"avg_hierarchical\": avg_h, \"avg_standard\": avg_b, \"verdict\": verdict},\n",
    "        \"overall_analysis\": f\"Avg(Hierarchical)={avg_h:.3f}, Avg(Standard)={avg_b:.3f} → {verdict}\"\n",
    "    }\n",
    "\n",
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Example query about AI for testing the hierarchical RAG approach\n",
    "query = \"What have been done to prevent homelessness?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# Test query for formal evaluation (using only one query as requested)\n",
    "test_queries = [\n",
    "     \"What are the strategies to prevent homelessness?\"\n",
    "]\n",
    "\n",
    "# Reference answer for the test query to enable comparison\n",
    "reference_answers = [\n",
    "    \"Prevent new incidences of homelessness through early intervention and support services.\",  \"Address and mitigate the underlying causes of homelessness, such as poverty, unemployment, and lack of affordable housing.\", \"Reduce the overall number of people experiencing homelessness via rapid rehousing and housing-first models.\", \"Minimize the negative social, health, and economic impacts on individuals and families currently experiencing homelessness.\", \"Ensure formerly homeless people maintain permanent, independent housing through ongoing support and follow-up services.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Run the evaluation comparing hierarchical and standard RAG approaches\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis of the comparison\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
