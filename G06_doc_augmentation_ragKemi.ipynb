{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Document Augmentation RAG with Question Generation\n",
    "\n",
    "This notebook implements an enhanced RAG approach using document augmentation through question generation. By generating relevant questions for each text chunk, we improve the retrieval process, leading to better responses from the language model.\n",
    "\n",
    "In this implementation, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Extract text from a PDF file.\n",
    "2. **Chunking**: Split the text into manageable chunks.\n",
    "3. **Question Generation**: Generate relevant questions for each chunk.\n",
    "4. **Embedding Creation**: Create embeddings for both chunks and generated questions.\n",
    "5. **Vector Store Creation**: Build a simple vector store using NumPy.\n",
    "6. **Semantic Search**: Retrieve relevant chunks and questions for user queries.\n",
    "7. **Response Generation**: Generate answers based on retrieved content.\n",
    "8. **Evaluation**: Assess the quality of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set. Please set it.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">> Abstract_ Substantial progress has been made at EU level on defining home-\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this \n",
      "typology. The development and some of th\n"
     ]
    }
   ],
   "source": [
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text[:500])  # Print the first 500 characters to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Text extraction complete.\n",
      "\n",
      "Generating summary with Gemini...\n",
      "\n",
      "Summary:\n",
      "This chapter examines the progress and challenges in defining and measuring homelessness in Europe.  While the European Typology on Homelessness and Housing Exclusion (ETHOS) provides a widely accepted framework, inconsistencies remain in national definitions.  A simplified version, \"ETHOS light,\" aims for greater harmonization, focusing on easily comparable categories.  The chapter argues that a single homelessness figure is insufficient;  multiple indicators (point-in-time, annual prevalence, inflow/outflow) are needed to understand and monitor the issue effectively.  Various data collection methods (surveys, registers, censuses) are discussed, highlighting the need for improved data collection strategies, political commitment, and transnational cooperation to achieve comparable European-wide data.  Future research should focus on clarifying ambiguous categories (long-term homelessness, youth homelessness), improving data on hidden homelessness and homeless migrants, and measuring the cost-effectiveness of interventions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Your original PDF text extraction function ---\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# --- 2. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 3. Function to summarize text using Gemini ---\n",
    "def summarize_with_gemini(text_to_summarize: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Gemini API to generate a summary of the provided text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        prompt = f\"Please provide a concise summary of the following text:\\n\\n{text_to_summarize}\"\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return \"Failed to generate summary.\"\n",
    "\n",
    "# --- 4. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \"/Users/kekunkoya/Desktop/770 Google /Homelessness.pdf\"\n",
    "\n",
    "    if not os.path.exists(pdf_file):\n",
    "        print(f\"Error: PDF file not found at '{pdf_file}'\")\n",
    "        exit()\n",
    "\n",
    "    # Step A: Extract text from the PDF using your function\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    print(\"Text extraction complete.\")\n",
    "\n",
    "    # Step B: Pass the extracted text to Gemini for summarization\n",
    "    if text:\n",
    "        print(\"\\nGenerating summary with Gemini...\")\n",
    "        summary = summarize_with_gemini(text)\n",
    "        print(\"\\nSummary:\")\n",
    "        print(summary)\n",
    "    else:\n",
    "        print(\"No text was extracted from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions for Text Chunks\n",
    "This is the key enhancement over simple RAG. We generate questions that could be answered by each text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions with Gemini...\n",
      "\n",
      "Generated Questions:\n",
      "- What is a key factor contributing to homelessness?\n",
      "- What types of issues contribute to homelessness?\n",
      "- Which groups are disproportionately affected by a lack of affordable housing?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the question generation function for Gemini ---\n",
    "def generate_questions(text_chunk: str, num_questions: int = 5, model: str = \"gemini-1.5-flash\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates relevant questions that can be answered from the given text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunk (str): The text chunk to generate questions from.\n",
    "    num_questions (int): Number of questions to generate.\n",
    "    model (str): The model to use for question generation.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated questions.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    \n",
    "    # Define the user prompt with the text chunk and the number of questions to generate\n",
    "    user_prompt = f\"\"\"\n",
    "Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
    "\n",
    "{text_chunk}\n",
    "\n",
    "Format your response as a numbered list of questions only, with no additional text.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate questions using the Gemini API\n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        \n",
    "        # Extract and clean questions from the response\n",
    "        questions_text = response.text.strip()\n",
    "        questions = []\n",
    "        \n",
    "        # Extract questions using regex pattern matching\n",
    "        for line in questions_text.split('\\n'):\n",
    "            cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "            if cleaned_line and cleaned_line.endswith('?'):\n",
    "                questions.append(cleaned_line)\n",
    "        \n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during question generation: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a text chunk\n",
    "    sample_chunk = \"\"\"\n",
    "    Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues. \n",
    "    A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating questions with Gemini...\")\n",
    "    # Generate questions from the text chunk\n",
    "    generated_questions = generate_questions(sample_chunk, num_questions=3)\n",
    "    \n",
    "    # Print the generated questions\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for q in generated_questions:\n",
    "        print(f\"- {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text\n",
    "We generate embeddings for both text chunks and generated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding with Gemini...\n",
      "\n",
      "Embedding created successfully.\n",
      "Embedding shape: (768,)\n",
      "First 5 values: [ 0.05044351 -0.03356895 -0.06893376 -0.04065947  0.04912253]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Get your API key from an environment variable\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The embedding model to be used. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embeddings using the specified model and input text\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        # The embedding is located in the 'embedding' key of the response dictionary\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Homelessness is a significant social issue.\"\n",
    "\n",
    "    print(\"Creating embedding with Gemini...\")\n",
    "    embedding = create_embeddings(sample_text)\n",
    "\n",
    "    if embedding.size > 0:\n",
    "        print(\"\\nEmbedding created successfully.\")\n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"First 5 values: {embedding[:5]}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "We'll implement a simple vector store using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating vector store with Gemini embeddings...\n",
      "\n",
      "Searching for relevant documents for: 'What causes homelessness?'\n",
      "\n",
      "Top 2 search results:\n",
      "- Similarity: 0.8567, Text: 'Homelessness is a complex social problem.'\n",
      "- Similarity: 0.7409, Text: 'A lack of affordable housing is a key contributing factor.'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# Your SimpleVectorStore class definition goes here...\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Added a check to prevent division by zero\n",
    "            if np.linalg.norm(query_vector) == 0 or np.linalg.norm(vector) == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Helper function to get embedding from Gemini ---\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Creates an embedding for a given text using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 2. Add some sample data to the vector store\n",
    "    docs = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"The sun is the center of our solar system.\" # Unrelated text\n",
    "    ]\n",
    "\n",
    "    print(\"Populating vector store with Gemini embeddings...\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        embedding = get_embedding(doc)\n",
    "        if embedding:\n",
    "            store.add_item(doc, embedding, {\"id\": i})\n",
    "\n",
    "    # 3. Perform a semantic search\n",
    "    query_text = \"What causes homelessness?\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    \n",
    "    if query_embedding:\n",
    "        print(f\"\\nSearching for relevant documents for: '{query_text}'\")\n",
    "        results = store.similarity_search(query_embedding, k=2)\n",
    "\n",
    "        # 4. Print the search results\n",
    "        print(\"\\nTop 2 search results:\")\n",
    "        for res in results:\n",
    "            print(f\"- Similarity: {res['similarity']:.4f}, Text: '{res['text']}'\")\n",
    "    else:\n",
    "        print(\"\\nFailed to get embedding for the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with Question Augmentation\n",
    "Now we'll put everything together to process documents, generate questions, and build our augmented vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Helper functions (assumed to be defined elsewhere) ---\n",
    "# Your original functions for PDF extraction, text chunking, vector store,\n",
    "# and cosine similarity would be defined here. For this example,\n",
    "# I'll provide simplified Gemini-compatible versions.\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = [page.get_text(\"text\") for page in doc]\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text: str, n: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Chunks the given text into segments of n characters with overlap.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "    return chunks\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"A simple vector store implementation using NumPy.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    # ... other methods like similarity_search would be here\n",
    "\n",
    "def create_embeddings(text: str or List[str], model: str = \"models/embedding-001\") -> List[float] or List[List[float]]:\n",
    "    \"\"\"Creates embeddings for text using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_questions(text_chunk: str, num_questions: int = 5, model: str = \"gemini-1.5-flash\") -> List[str]:\n",
    "    \"\"\"Generates relevant questions from a text chunk using Gemini.\"\"\"\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    user_prompt = f\"Based on the following text, generate {num_questions} different questions:\\n\\n{text_chunk}\\n\\nFormat your response as a numbered list.\"\n",
    "    try:\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_prompt)\n",
    "        # Assuming the response is a numbered list, we'll parse it\n",
    "        return [re.sub(r'^\\d+\\.\\s*', '', line.strip()) for line in response.text.split('\\n') if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Question generation error: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. The main processing function (revised) ---\n",
    "def process_document(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200, questions_per_chunk: int = 5) -> Tuple[List[str], SimpleVectorStore]:\n",
    "    \"\"\"\n",
    "    Process a document with question augmentation using Gemini.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(text_chunks)} text chunks\")\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    print(\"Processing chunks and generating questions...\")\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Processing Chunks\")):\n",
    "        # Create embedding for the chunk itself\n",
    "        chunk_embedding = create_embeddings(chunk)\n",
    "        if not chunk_embedding: continue\n",
    "\n",
    "        vector_store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=chunk_embedding,\n",
    "            metadata={\"type\": \"chunk\", \"index\": i}\n",
    "        )\n",
    "        \n",
    "        # Generate questions for this chunk\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        \n",
    "        # Create embeddings for each question and add to vector store\n",
    "        if questions:\n",
    "            question_embeddings = create_embeddings(questions)\n",
    "            if question_embeddings:\n",
    "                for j, question in enumerate(questions):\n",
    "                    vector_store.add_item(\n",
    "                        text=question,\n",
    "                        embedding=question_embeddings[j],\n",
    "                        metadata={\"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk}\n",
    "                    )\n",
    "    \n",
    "    return text_chunks, vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Processing the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 65 text chunks\n",
      "Processing chunks and generating questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 65/65 [01:30<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store contains 260 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "text_chunks, vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    questions_per_chunk=3\n",
    ")\n",
    "\n",
    "print(f\"Vector store contains {len(vector_store.texts)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement a semantic search function similar to the simple RAG implementation but adapted to our augmented vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating vector store with Gemini embeddings...\n",
      "\n",
      "Searching for relevant documents for: 'What causes homelessness?'\n",
      "\n",
      "Top 2 search results:\n",
      "- Similarity: 0.8567, Text: 'Homelessness is a complex social problem.'\n",
      "- Similarity: 0.7409, Text: 'A lack of affordable housing is a key contributing factor.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to be embedded.\n",
    "    model (str): The embedding model to be used. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API takes a single text input for this function call\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- 3. Define the semantic_search function (revised) ---\n",
    "def semantic_search(query: str, vector_store, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query using the Gemini-compatible function\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- 4. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a SimpleVectorStore class and populate it\n",
    "    class SimpleVectorStore:\n",
    "        def __init__(self):\n",
    "            self.vectors = []\n",
    "            self.texts = []\n",
    "            self.metadata = []\n",
    "        \n",
    "        def add_item(self, text, embedding, metadata=None):\n",
    "            self.vectors.append(np.array(embedding))\n",
    "            self.texts.append(text)\n",
    "            self.metadata.append(metadata or {})\n",
    "        \n",
    "        def similarity_search(self, query_embedding, k=5):\n",
    "            if not self.vectors: return []\n",
    "            \n",
    "            query_vector = np.array(query_embedding)\n",
    "            similarities = []\n",
    "            for i, vector in enumerate(self.vectors):\n",
    "                if np.linalg.norm(query_vector) == 0 or np.linalg.norm(vector) == 0:\n",
    "                    similarity = 0\n",
    "                else:\n",
    "                    similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "                similarities.append((i, similarity))\n",
    "            \n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            results = []\n",
    "            for i in range(min(k, len(similarities))):\n",
    "                idx, score = similarities[i]\n",
    "                results.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "            return results\n",
    "\n",
    "    store = SimpleVectorStore()\n",
    "    docs = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"The sun is the center of our solar system.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Populating vector store with Gemini embeddings...\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        embedding = create_embeddings(doc)\n",
    "        if embedding.size > 0:\n",
    "            store.add_item(doc, embedding, {\"id\": i})\n",
    "\n",
    "    query_text = \"What causes homelessness?\"\n",
    "    \n",
    "    print(f\"\\nSearching for relevant documents for: '{query_text}'\")\n",
    "    results = semantic_search(query_text, store, k=2)\n",
    "\n",
    "    print(\"\\nTop 2 search results:\")\n",
    "    for res in results:\n",
    "        print(f\"- Similarity: {res['similarity']:.4f}, Text: '{res['text']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on the Augmented Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main contributing factors to homelessness?\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Relevant Document Chunks:\n",
      "Context 1 (similarity: 0.8682):\n",
      "Homelessness is a complex social problem. A key factor is the lack of affordable housing....\n",
      "=====================================\n",
      "\n",
      "Matched Questions:\n",
      "Question 1 (similarity: 0.9502):\n",
      "What is a key factor in homelessness?\n",
      "From chunk 0\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration and Helper Functions ---\n",
    "# (Assumed to be defined and configured for Gemini in a separate module or above)\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for a text using the Gemini API.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return np.array(response['embedding'], dtype=np.float32)\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors: return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "        return results\n",
    "\n",
    "def semantic_search(query: str, vector_store, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Performs semantic search using the Gemini-compatible vector store.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding.size == 0: return []\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    return results\n",
    "\n",
    "# --- 2. Main Logic: Your provided code ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a populated vector store for the example\n",
    "    vector_store = SimpleVectorStore()\n",
    "    doc_chunk = \"Homelessness is a complex social problem. A key factor is the lack of affordable housing.\"\n",
    "    q_chunk = \"What is a key factor in homelessness?\"\n",
    "    vector_store.add_item(doc_chunk, get_embedding(doc_chunk), {\"type\": \"chunk\", \"index\": 0})\n",
    "    vector_store.add_item(q_chunk, get_embedding(q_chunk), {\"type\": \"question\", \"chunk_index\": 0})\n",
    "\n",
    "    # Load the validation data from a JSON file\n",
    "    # Note: This is a placeholder since I cannot access your local file\n",
    "    data = [{\"question\": \"What are the main contributing factors to homelessness?\"}]\n",
    "\n",
    "    # Extract the first query from the validation data\n",
    "    query = data[0]['question']\n",
    "\n",
    "    # Perform semantic search to find relevant content\n",
    "    search_results = semantic_search(query, vector_store, k=5)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nSearch Results:\")\n",
    "\n",
    "    # Organize results by type\n",
    "    chunk_results = []\n",
    "    question_results = []\n",
    "\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_results.append(result)\n",
    "        else:\n",
    "            question_results.append(result)\n",
    "\n",
    "    # Print chunk results first\n",
    "    print(\"\\nRelevant Document Chunks:\")\n",
    "    for i, result in enumerate(chunk_results):\n",
    "        print(f\"Context {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
    "        print(result[\"text\"][:300] + \"...\")\n",
    "        print(\"=====================================\")\n",
    "\n",
    "    # Then print question matches\n",
    "    print(\"\\nMatched Questions:\")\n",
    "    for i, result in enumerate(question_results):\n",
    "        print(f\"Question {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
    "        print(result[\"text\"])\n",
    "        chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "        print(f\"From chunk {chunk_idx}\")\n",
    "        print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Context for Response\n",
    "Now we prepare the context by combining information from relevant chunks and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(search_results):\n",
    "    \"\"\"\n",
    "    Prepares a unified context from search results for response generation.\n",
    "\n",
    "    Args:\n",
    "    search_results (List[Dict]): Results from semantic search.\n",
    "\n",
    "    Returns:\n",
    "    str: Combined context string.\n",
    "    \"\"\"\n",
    "    # Extract unique chunks referenced in the results\n",
    "    chunk_indices = set()\n",
    "    context_chunks = []\n",
    "    \n",
    "    # First add direct chunk matches\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_indices.add(result[\"metadata\"][\"index\"])\n",
    "            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n",
    "    \n",
    "    # Then add chunks referenced by questions\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"question\":\n",
    "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n",
    "    \n",
    "    # Combine all context chunks\n",
    "    full_context = \"\\n\\n\".join(context_chunks)\n",
    "    return full_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key drivers of homelessness?\n",
      "\n",
      "Prepared context for LLM:\n",
      "Chunk 0:\n",
      "Homelessness is a complex social issue with many contributing factors.\n",
      "\n",
      "Chunk 1:\n",
      "The lack of affordable housing is a primary cause of homelessness.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration and Helper Functions ---\n",
    "# (Assumed to be defined and configured for Gemini in a separate module or above)\n",
    "def get_embedding(text: str, model: str = \"models/embedding-001\") -> np.ndarray:\n",
    "    \"\"\"Creates an embedding for a text using the Gemini API.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return np.array(response['embedding'], dtype=np.float32)\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors: return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "        return results\n",
    "\n",
    "def semantic_search(query: str, vector_store, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Performs semantic search using the Gemini-compatible vector store.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding.size == 0: return []\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    return results\n",
    "\n",
    "# --- 2. Your original prepare_context function ---\n",
    "def prepare_context(search_results: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Prepares a unified context from search results for response generation.\n",
    "    \"\"\"\n",
    "    chunk_indices = set()\n",
    "    context_chunks = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_indices.add(result[\"metadata\"][\"index\"])\n",
    "            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n",
    "    \n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"question\":\n",
    "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n",
    "    \n",
    "    full_context = \"\\n\\n\".join(context_chunks)\n",
    "    return full_context\n",
    "\n",
    "# --- 3. Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a populated vector store with both chunks and questions\n",
    "    vector_store = SimpleVectorStore()\n",
    "    chunk1_text = \"Homelessness is a complex social issue with many contributing factors.\"\n",
    "    chunk2_text = \"The lack of affordable housing is a primary cause of homelessness.\"\n",
    "    q1_text = \"What is a major cause of homelessness?\"\n",
    "    \n",
    "    chunk1_embedding = get_embedding(chunk1_text)\n",
    "    chunk2_embedding = get_embedding(chunk2_text)\n",
    "    q1_embedding = get_embedding(q1_text)\n",
    "\n",
    "    vector_store.add_item(chunk1_text, chunk1_embedding, {\"type\": \"chunk\", \"index\": 0})\n",
    "    vector_store.add_item(chunk2_text, chunk2_embedding, {\"type\": \"chunk\", \"index\": 1})\n",
    "    vector_store.add_item(q1_text, q1_embedding, {\"type\": \"question\", \"chunk_index\": 1, \"original_chunk\": chunk2_text})\n",
    "\n",
    "    query = \"What are the key drivers of homelessness?\"\n",
    "    \n",
    "    # Perform semantic search to find relevant content\n",
    "    search_results = semantic_search(query, vector_store, k=5)\n",
    "    \n",
    "    # Prepare the context using your function\n",
    "    prepared_context = prepare_context(search_results)\n",
    "    \n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nPrepared context for LLM:\")\n",
    "    print(prepared_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and Displaying the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the ETHOS typology?\n",
      "\n",
      "Response:\n",
      "The ETHOS typology, or European Typology on Homelessness and Housing Exclusion, is a conceptual framework developed to define and measure homelessness and housing exclusion in Europe. It categorizes various living situations into four main conceptual categories: roofless, houseless, insecure housing, and inadequate housing. The most recent version includes thirteen operational categories and twenty-four different living situations. It aims to improve the comparability of homelessness definitions and data across different EU countries.\n"
     ]
    }
   ],
   "source": [
    "# Prepare context from search results\n",
    "context = prepare_context(search_results)\n",
    "\n",
    "# Generate response\n",
    "response_text = generate_response(query, context)\n",
    "\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nResponse:\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating evaluation with Gemini...\n",
      "\n",
      "Evaluation Feedback:\n",
      "Score: 0.8\n",
      "\n",
      "Justification:\n",
      "\n",
      "1. **Factual Correctness:** The AI response correctly identifies Paris as the capital of France.  The added information about Paris being a major European city is factually accurate but not strictly necessary to answer the question.\n",
      "\n",
      "2. **Completeness:** The AI response provides the core answer. While it includes extra information, it doesn't detract from the accuracy of the main point.  The reference answer is more concise, but the AI response isn't overly verbose.\n",
      "\n",
      "3. **Relevance:** The AI response directly and completely addresses the user's question. The additional sentence about Paris being a major European city is relevant to the context but not crucial to answering the question itself.\n",
      "\n",
      "\n",
      "The AI response is slightly more verbose than the reference answer, but the added information is accurate and doesn't make the response incorrect or less relevant.  Therefore, a score of 0.8 is appropriate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Define the evaluation function for Gemini ---\n",
    "def evaluate_response(query: str, response: str, reference_answer: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the AI response against a reference answer using Gemini.\n",
    "    \n",
    "    Args:\n",
    "    query (str): The user's question.\n",
    "    response (str): The AI-generated response.\n",
    "    reference_answer (str): The reference/ideal answer.\n",
    "    model (str): Model to use for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "    str: Evaluation feedback.\n",
    "    \"\"\"\n",
    "    # Define the system prompt for the evaluation system\n",
    "    evaluate_system_prompt = \"\"\"You are an intelligent evaluation system tasked with assessing AI responses.\n",
    "    \n",
    "    Compare the AI assistant's response to the true/reference answer, and evaluate based on:\n",
    "    1. Factual correctness - Does the response contain accurate information?\n",
    "    2. Completeness - Does it cover all important aspects from the reference?\n",
    "    3. Relevance - Does it directly address the question?\n",
    "\n",
    "    Assign a score from 0 to 1:\n",
    "    - 1.0: Perfect match in content and meaning\n",
    "    - 0.8: Very good, with minor omissions/differences\n",
    "    - 0.6: Good, covers main points but misses some details\n",
    "    - 0.4: Partial answer with significant omissions\n",
    "    - 0.2: Minimal relevant information\n",
    "    - 0.0: Incorrect or irrelevant\n",
    "\n",
    "    Provide your score with justification.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the evaluation prompt\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    User Query: {query}\n",
    "    \n",
    "    AI Response:\n",
    "    {response}\n",
    "\n",
    "    Reference Answer:\n",
    "    {reference_answer}\n",
    "    \n",
    "    Please evaluate the AI response against the reference answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate evaluation\n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=evaluate_system_prompt)\n",
    "        eval_response = gemini_model.generate_content(evaluation_prompt)\n",
    "        return eval_response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation generation: {e}\")\n",
    "        return \"Evaluation failed due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query, AI response, and reference answer\n",
    "    query = \"What is the capital of France?\"\n",
    "    ai_response = \"The capital of France is Paris, which is also a major European city.\"\n",
    "    reference_answer = \"Paris\"\n",
    "\n",
    "    print(\"Generating evaluation with Gemini...\")\n",
    "    # Generate the evaluation feedback\n",
    "    evaluation_feedback = evaluate_response(query, ai_response, reference_answer)\n",
    "\n",
    "    print(\"\\nEvaluation Feedback:\")\n",
    "    print(evaluation_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "Score: 1.0\n",
      "\n",
      "Justification: The AI response accurately describes the ETHOS typology, including its full name (European Typology on Homelessness and Housing Exclusion) and its purpose as a framework for defining and measuring homelessness and housing exclusion in Europe. It correctly identifies the four main conceptual categories (roofless, houseless, insecure housing, and inadequate housing) and mentions the thirteen operational categories and twenty-four living situations. The response also highlights the aim of improving comparability across EU countries, which aligns perfectly with the reference answer. There are no significant omissions or inaccuracies, making it a perfect match in content and meaning.\n"
     ]
    }
   ],
   "source": [
    "# Get reference answer from validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# Evaluate the response\n",
    "evaluation = evaluate_response(query, response_text, reference_answer)\n",
    "\n",
    "print(\"\\nEvaluation:\")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "Score: 0.0\n",
      "\n",
      "Justification:\n",
      "\n",
      "The AI's response is completely irrelevant to the question.  It discusses homelessness in general terms, but doesn't mention or even allude to the ETHOS typology, which is what the question specifically asks about.  The reference answer accurately defines the ETHOS typology, while the AI response provides entirely unrelated information. Therefore, a score of 0.0 is appropriate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate data from previous steps\n",
    "    with open('/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Assuming 'response_text' is the generated AI response from a previous step\n",
    "    response_text = \"Homelessness is a complex social problem caused by various economic, social, and personal issues.\"\n",
    "    query = data[0]['question']\n",
    "\n",
    "    # Get reference answer from validation data\n",
    "    reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "    # Evaluate the response\n",
    "    evaluation = evaluate_response(query, response_text, reference_answer)\n",
    "\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 65\n",
      "\n",
      "First text chunk:\n",
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">> Abstract_ Substantial progress has been made at EU level on defining home-\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this \n",
      "typology. The development and some of the remaining controversial issues \n",
      "concerning ETHOS and a reduced version of it are discussed in this chapter. \n",
      "Furthermore essential reasons and different approaches to measure home-\n",
      "lessness are presented. It is argued that a single number will not be enough \n",
      "to understand homelessness and monitor progress in tackling it. More \n",
      "research and more work to improve information on homelessness at national \n",
      "levels will be needed before we can achieve comparable numbers at EU level.\n",
      ">> Keywords_ Data,\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings with Gemini...\n",
      "\n",
      "Embeddings created successfully.\n",
      "Number of embeddings: 2\n",
      "Shape of first embedding: (768,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from typing import List, Any # Import Any from the typing module\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(texts: List[str] or str, model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text or list of texts using the specified Gemini model.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str] or str): The input text(s) for which embeddings are to be created.\n",
    "        model (str): The model to be used for creating embeddings. Default is \"models/embedding-001\".\n",
    "        \n",
    "    Returns:\n",
    "        Any: A list of numpy arrays, where each array is an embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API handles both single strings and lists of strings\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=texts\n",
    "        )\n",
    "        # The response is a dictionary with a single key 'embedding'\n",
    "        # The value is a list of embeddings.\n",
    "        return [np.array(emb, dtype=np.float32) for emb in response['embedding']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate text chunks from a previous step\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Creating embeddings with Gemini...\")\n",
    "    # Create embeddings for the text chunks\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"\\nEmbeddings created successfully.\")\n",
    "        print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"Shape of first embedding: {embeddings[0].shape}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings with Gemini...\n",
      "\n",
      "Embeddings created successfully.\n",
      "Number of embeddings: 2\n",
      "Shape of first embedding: (768,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the create_embeddings function for Gemini ---\n",
    "def create_embeddings(texts: List[str] or str, model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text or list of texts using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "    texts (List[str] or str): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Default is \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: A list of numpy arrays, where each array is an embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The Gemini API can handle both single strings and lists of strings\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=texts\n",
    "        )\n",
    "        # The response is a dictionary with a key 'embedding'\n",
    "        # The value is a list of embeddings.\n",
    "        return [np.array(emb, dtype=np.float32) for emb in response['embedding']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate text chunks from a previous step\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Creating embeddings with Gemini...\")\n",
    "    # Create embeddings for the text chunks\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"\\nEmbeddings created successfully.\")\n",
    "        print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"Shape of first embedding: {embeddings[0].shape}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embeddings.\")\n",
    "\n",
    "# The embed_content method from the Google Gemini API is a direct equivalent to the OpenAI function for generating embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "\n",
      "Searching for chunks relevant to: 'What is the ETHOS typology?'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 667 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Perform semantic search to find the top 2 most relevant text chunks\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSearching for chunks relevant to: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m top_chunks = \u001b[43msemantic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuery:\u001b[39m\u001b[33m\"\u001b[39m, query)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36msemantic_search\u001b[39m\u001b[34m(query, text_chunks, embeddings, k)\u001b[39m\n\u001b[32m     42\u001b[39m top_indices = np.argsort(similarities)[-k:][::-\u001b[32m1\u001b[39m]\n\u001b[32m     44\u001b[39m text_chunks_array = np.array(text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtext_chunks_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_indices\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: index 667 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Any\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- Helper functions (assumed to be defined) ---\n",
    "def create_embeddings(texts: List[str] or str, model: str = \"models/embedding-001\") -> Any:\n",
    "    # Your embedding function as defined in previous steps\n",
    "    try:\n",
    "        response = genai.embed_content(model=model, content=texts)\n",
    "        return [np.array(emb, dtype=np.float32) for emb in response['embedding']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    # Your cosine similarity function as defined in previous steps\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0.0\n",
    "\n",
    "# --- Corrected semantic_search function with error checking ---\n",
    "def semantic_search(query: str, text_chunks: List[str], embeddings: List[np.ndarray], k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Performs semantic search to find the most relevant text chunks.\n",
    "    \"\"\"\n",
    "    # ⚠️ CRITICAL CHECK: The number of text chunks and embeddings must be equal.\n",
    "    if len(text_chunks) != len(embeddings):\n",
    "        raise ValueError(\n",
    "            f\"The number of text chunks ({len(text_chunks)}) must be equal to \"\n",
    "            f\"the number of embeddings ({len(embeddings)}).\"\n",
    "        )\n",
    "\n",
    "    query_embedding = create_embeddings(query)[0]\n",
    "    \n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in embeddings]\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    text_chunks_array = np.array(text_chunks)\n",
    "\n",
    "    return list(text_chunks_array[top_indices])\n",
    "\n",
    "# --- Main logic (re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a full pipeline\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        print(\"GOOGLE_API_KEY env var not set. Exiting.\")\n",
    "        exit()\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json'\n",
    "    if not os.path.isfile(val_path):\n",
    "        print(f\"File not found at: {val_path}\")\n",
    "        exit()\n",
    "    with open(val_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    query = data[0]['question']\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\",\n",
    "        \"A key factor is the lack of affordable housing, which disproportionately affects low-income families and individuals.\",\n",
    "        \"Social factors like family breakdown, domestic violence, and a lack of social support networks can also lead to homelessness.\",\n",
    "        \"Personal crises, such as job loss, mental health challenges, or substance abuse, are often triggers for losing housing.\"\n",
    "    ]\n",
    "    \n",
    "    # Create embeddings for the simulated text chunks\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "    if not embeddings:\n",
    "        print(\"Failed to create embeddings.\")\n",
    "        exit()\n",
    "    \n",
    "    # Perform semantic search to find the top 2 most relevant text chunks\n",
    "    print(f\"\\nSearching for chunks relevant to: '{query}'\")\n",
    "    top_chunks = semantic_search(query, text_chunks, embeddings, k=2)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 2 most relevant text chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"Context {i + 1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the ETHOS typology?\n",
      "Context 1:\n",
      "he forth and fifth reviews of statistics \n",
      "(Edgar and Meert, 2005, 2006) focused on developing and refining the ETHOS definition and \n",
      "considering the measurement issues involved in greater detail. \n",
      "\n",
      "24\n",
      "Homelessness Research in Europe\n",
      "Table 1.2 ETHOS – European typology on homelessness and housing exclusion\n",
      "Conceptual \n",
      "category\n",
      "Operational category\n",
      "Living situation\n",
      "ROOFLESS\n",
      "1\n",
      "People living rough\n",
      "1.1\n",
      "Public space or external space\n",
      "2\n",
      "People staying in a night shelter 2.1\n",
      "Night shelter\n",
      "HOUSELESS\n",
      "3\n",
      "People in accommodation  \n",
      "for the homeless\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "Homeless hostel\n",
      "Temporary accommodation\n",
      "Transitional supported \n",
      "accommodation\n",
      "4\n",
      "People in a women’s shelter\n",
      "4.1\n",
      "Women’s shelter accommodation\n",
      "5\n",
      "People in accommodation  \n",
      "for immigrants\n",
      "5.1\n",
      "5.2\n",
      "Temporary accommodation, \n",
      "reception centres \n",
      "Migrant workers’ accommodation\n",
      "6\n",
      "People due to be released  \n",
      "from institutions\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "Penal institutions\n",
      "Medical institutions\n",
      "Children’s institutions/homes\n",
      "7\n",
      "People receiving longer-term \n",
      "support (due to \n",
      "=====================================\n",
      "Context 2:\n",
      " homelessness, while people living in insecure and/or inadequate housing \n",
      "and/or in social isolation might also be affected by exclusion from one or two domains, \n",
      "but their situation is classified under ‘housing exclusion’ rather than ‘homelessness’.\n",
      "On the basis of this conceptional understanding and to try to grasp the varying \n",
      "practices in different EU countries, the ETHOS typology was developed, which \n",
      "relates, in its most recent version, thirteen different operational categories and \n",
      "twenty-four different living situations to the four conceptional categories: roofless, \n",
      "houseless, insecure housing and inadequate housing.4 See Table 1.2.\n",
      "4\t\n",
      "Apart from documenting progress concerning the measurement of homelessness in different \n",
      "EU countries and reporting on the latest available data, the forth and fifth reviews of statistics \n",
      "(Edgar and Meert, 2005, 2006) focused on developing and refining the ETHOS definition and \n",
      "considering the measurement issues involved in greater detail. \n",
      "\n",
      "24\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Perform semantic search to find the top 2 most relevant text chunks for the query\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# Print the query\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected semantic_search function with error checking ---\n",
    "def semantic_search(query: str, text_chunks: list[str], embeddings: list[np.ndarray], k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs semantic search to find the most relevant text chunks.\n",
    "    \"\"\"\n",
    "    #  CRITICAL CHECK: The number of text chunks and embeddings must be equal.\n",
    "    if len(text_chunks) != len(embeddings):\n",
    "        raise ValueError(\n",
    "            f\"The number of text chunks ({len(text_chunks)}) must be equal to \"\n",
    "            f\"the number of embeddings ({len(embeddings)}).\"\n",
    "        )\n",
    "\n",
    "    # ... The rest of your function logic follows ...\n",
    "    query_embedding = create_embeddings(query)[0]\n",
    "    \n",
    "    if query_embedding.size == 0:\n",
    "        return []\n",
    "\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in embeddings]\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    text_chunks_array = np.array(text_chunks)\n",
    "\n",
    "    return list(text_chunks_array[top_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, social factors contributing to homelessness include family breakdown, domestic violence, and a lack of social support networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(system_prompt: str, user_message: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the Gemini model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"gemini-1.5-flash\".\n",
    "\n",
    "    Returns:\n",
    "    str: The response from the AI model as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        response = gemini_model.generate_content(user_message)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and top_chunks from previous steps\n",
    "    query = \"What are the social factors that contribute to homelessness?\"\n",
    "    top_chunks = [\n",
    "        \"Homelessness is a complex social problem with various contributing factors, including economic, social, and personal issues.\",\n",
    "        \"Social factors like family breakdown, domestic violence, and a lack of social support networks can also lead to homelessness.\"\n",
    "    ]\n",
    "\n",
    "    # Define the system prompt for the AI assistant\n",
    "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "    # Create the user prompt based on the top chunks\n",
    "    user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "    user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "    # Generate AI response\n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "    # Print the final AI response\n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
