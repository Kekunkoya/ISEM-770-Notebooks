{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Multi-Modal RAG with Image Captioning\n",
    "\n",
    "In this notebook, I implement a Multi-Modal RAG system that extracts both text and images from documents, generates captions for images, and uses both content types to respond to queries. This approach enhances traditional RAG by incorporating visual information into the knowledge base.\n",
    "\n",
    "Traditional RAG systems only work with text, but many documents contain crucial information in images, charts, and tables. By captioning these visual elements and incorporating them into our retrieval system, we can:\n",
    "\n",
    "- Access information locked in figures and diagrams\n",
    "- Understand tables and charts that complement the text\n",
    "- Create a more comprehensive knowledge base\n",
    "- Answer questions that rely on visual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "import base64\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# --- Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set. Please set it.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract both text and images from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str, optional): Directory to save extracted images\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for images if not provided\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    text_data = []  # List to store extracted text data\n",
    "    image_paths = []  # List to store paths of extracted images\n",
    "    \n",
    "    print(f\"Extracting content from {pdf_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Loop through every page in the PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # Extract text from the page\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # Extract images from the page\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # XREF of the image\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    \n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save the image to the output directory\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "                        \n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n",
    "        return text_data, image_paths\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text data into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text_data (List[Dict]): Text data extracted from PDF\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Chunked text data\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize an empty list to store chunked data\n",
    "    \n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # Extract the text content\n",
    "        metadata = item[\"metadata\"]  # Extract the metadata\n",
    "        \n",
    "        # Skip if text is too short\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Add each chunk with updated metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # Copy the original metadata\n",
    "            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # The chunk text\n",
    "                \"metadata\": chunk_metadata  # The updated metadata\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n",
    "    return chunked_data  # Return the list of chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning with OpenAI Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file as base64.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded image\n",
    "    \"\"\"\n",
    "    # Open the image file in binary read mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the image file and encode it to base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # Decode the base64 bytes to a string and return\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image caption with Gemini...\n",
      "\n",
      "Generated Caption:\n",
      "Here's a caption describing the image's academic content:\n",
      "\n",
      "**Figure: Reward History During Reinforcement Learning (RL) Training**\n",
      "\n",
      "This line graph illustrates the reward history observed during the training of a reinforcement learning agent across five episodes.  The x-axis represents the episode number (0 through 4), and the y-axis represents the reward obtained by the agent in each episode, ranging from 0 to approximately 0.9. The graph shows a fluctuating reward pattern.  The reward increases sharply in the first episode then peaks in the second episode before dropping to a near-zero value. Subsequently, there is a rapid increase in the reward again by episode 3, followed by a more gradual improvement in the final episode. This suggests an RL agent learning process that's neither consistently smooth nor consistently poor, instead displaying periods of rapid learning interspersed with dips.  The overall trend shows an improvement in the agent's performance over the training period.  The figure could be used to evaluate the learning efficiency and stability of the reinforcement learning algorithm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main image caption generation function (revised for Gemini) ---\n",
    "def generate_image_caption(image_path: str, model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using Gemini's multi-modal capabilities.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        model (str): The Gemini multi-modal model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image file not found\"\n",
    "        \n",
    "    try:\n",
    "        # Define the system prompt to guide the AI's behavior\n",
    "        system_prompt = (\n",
    "            \"You are an assistant specialized in describing images from academic papers. \"\n",
    "            \"Provide detailed captions for the image that capture key information. \"\n",
    "            \"If the image contains charts, tables, or diagrams, describe their content and purpose clearly. \"\n",
    "            \"Your caption should be optimized for future retrieval when people ask questions about this content.\"\n",
    "        )\n",
    "        \n",
    "        # Open the image file using PIL\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Create the Gemini model instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # The prompt is a list of parts: a text string and the image object\n",
    "        prompt_parts = [\n",
    "            \"Describe this image in detail, focusing on its academic content:\",\n",
    "            img\n",
    "        ]\n",
    "        \n",
    "        # Generate the caption using the specified model\n",
    "        response = gemini_model.generate_content(prompt_parts, stream=False)\n",
    "        \n",
    "        # Return the generated caption\n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating caption: {str(e)}\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a local image file. Replace with a valid path for a real run.\n",
    "    image_file = \"/Users/kekunkoya/Desktop/770 Google /reward_plot.png\"\n",
    "    \n",
    "    print(\"Generating image caption with Gemini...\")\n",
    "    caption = generate_image_caption(image_file)\n",
    "    \n",
    "    print(\"\\nGenerated Caption:\")\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    Process all images and generate captions.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (List[Dict]): Paths to extracted images\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Image data with captions\n",
    "    \"\"\"\n",
    "    image_data = []  # Initialize an empty list to store image data with captions\n",
    "    \n",
    "    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n",
    "        img_path = img_item[\"path\"]  # Get the image path\n",
    "        metadata = img_item[\"metadata\"]  # Get the image metadata\n",
    "        \n",
    "        # Generate caption for the image\n",
    "        caption = generate_image_caption(img_path)\n",
    "        \n",
    "        # Add the image data with caption to the list\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # The generated caption\n",
    "            \"metadata\": metadata,  # The image metadata\n",
    "            \"image_path\": img_path  # The path to the image\n",
    "        })\n",
    "    \n",
    "    return image_data  # Return the list of image data with captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation for multi-modal content.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, contents, and metadata\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            content (str): The content (text or image caption)\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding vector, content, and metadata to their respective lists\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of content items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Loop through items and embeddings and add each to the vector store\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings with Gemini...\n",
      "\n",
      "Embeddings created successfully.\n",
      "Number of embeddings: 3\n",
      "Embedding dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main embedding function (revised for Gemini) ---\n",
    "def create_embeddings(texts: List[str], model: str = \"models/embedding-001\") -> Any:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given texts using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # The Gemini API can handle a list of texts directly for batching\n",
    "    # The batch size is often handled more efficiently by the API itself.\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=model,\n",
    "            content=texts\n",
    "        )\n",
    "        # The embedding list is directly under the 'embedding' key\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a list of text chunks\n",
    "    text_chunks = [\n",
    "        \"Homelessness is a complex social problem.\",\n",
    "        \"A lack of affordable housing is a key contributing factor.\",\n",
    "        \"Social factors like family breakdown can also lead to homelessness.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Creating embeddings with Gemini...\")\n",
    "    # Create embeddings for the text chunks\n",
    "    embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"\\nEmbeddings created successfully.\")\n",
    "        print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"Embedding dimensions: {len(embeddings[0])}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for multi-modal RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[MultiModalVectorStore, Dict]: Vector store and document info\n",
    "    \"\"\"\n",
    "    # Create a directory for extracted images\n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract text and images from the PDF\n",
    "    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Process the extracted images to generate captions\n",
    "    image_data = process_images(image_paths)\n",
    "    \n",
    "    # Combine all content items (text chunks and image captions)\n",
    "    all_items = chunked_text + image_data\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    \n",
    "    # Create embeddings for all content\n",
    "    print(\"Creating embeddings for all content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build the vector store and add items with their embeddings\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "    \n",
    "    # Prepare document info with counts of text chunks and image captions\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "    \n",
    "    # Print summary of added items\n",
    "    print(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n",
    "    \n",
    "    # Return the vector store and document info\n",
    "    return vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Query the multi-modal RAG system.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (MultiModalVectorStore): Vector store with document content\n",
    "        k (int): Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results and generated response\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query: {query} ===\\n\")\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve relevant content from the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # Separate text and image results\n",
    "    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n",
    "    \n",
    "    # Generate a response using the retrieved content\n",
    "    response = generate_response(query, results)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"response\": response,\n",
    "        \"text_results_count\": len(text_results),\n",
    "        \"image_results_count\": len(image_results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response with Gemini...\n",
      "\n",
      "AI Response:\n",
      "Based on the provided text, Paris is the capital of France.  The image caption further supports this by mentioning that a map of France shows Paris marked on it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Define the response generator for Gemini ---\n",
    "def generate_response(query: str, results: List[Dict], model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved results using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved content\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Format the context from the retrieved results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        # Determine the type of content (text or image caption)\n",
    "        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n",
    "        # Get the page number from the metadata\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n",
    "        \n",
    "        # Append the content type and page number to the context\n",
    "        context += f\"[{content_type} from page {page_num}]\\n\"\n",
    "        # Append the actual content to the context\n",
    "        context += result[\"content\"]\n",
    "        context += \"\\n\\n\"\n",
    "        \n",
    "    # System message to guide the AI assistant\n",
    "    system_message = \"\"\"You are an AI assistant specializing in answering questions about documents\n",
    "that contain both text and images. You have been given relevant text passages and image captions\n",
    "from the document. Use this information to provide a comprehensive, accurate response to the query.\n",
    "If information comes from an image or chart, mention this in your answer.\n",
    "If the retrieved information doesn't fully answer the query, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # User message containing the query and the formatted context\n",
    "    user_message = f\"\"\"Query: {query}\n",
    "\n",
    "Retrieved content:\n",
    "{context}\n",
    "\n",
    "Please answer the query based on the retrieved content.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Pass the system message to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_message)\n",
    "        \n",
    "        # Generate the response using the specified model\n",
    "        response = gemini_model.generate_content(user_message, generation_config={\"temperature\": 0.1})\n",
    "        \n",
    "        # Return the generated response\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "        return \"I could not generate a response due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a query and results from a previous step\n",
    "    query = \"What is the capital of France?\"\n",
    "    results = [\n",
    "        {\"content\": \"Paris is the capital of France.\", \"metadata\": {\"type\": \"text\", \"page\": 1}},\n",
    "        {\"content\": \"This is a map of France with Paris marked on it.\", \"metadata\": {\"type\": \"image_caption\", \"page\": 2}}\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating AI response with Gemini...\")\n",
    "    ai_response = generate_response(query, results)\n",
    "    \n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Against Text-Only RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Build a text-only vector store for comparison.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        MultiModalVectorStore: Text-only vector store\n",
    "    \"\"\"\n",
    "    # Extract text from PDF (reuse function but ignore images)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "    \n",
    "    # Chunk text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for text-only content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build vector store\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"Added {len(chunked_text)} text items to text-only vector store\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare multi-modal RAG with text-only RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): Test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\\n\")\n",
    "    \n",
    "    # Process document for multi-modal RAG\n",
    "    print(\"\\nProcessing document for multi-modal RAG...\")\n",
    "    mm_vector_store, mm_doc_info = process_document(pdf_path)\n",
    "    \n",
    "    # Build text-only store\n",
    "    print(\"\\nProcessing document for text-only RAG...\")\n",
    "    text_vector_store = build_text_only_store(pdf_path)\n",
    "    \n",
    "    # Run evaluation for each query\n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}: {query} ===\")\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Run multi-modal RAG\n",
    "        print(\"\\nRunning multi-modal RAG...\")\n",
    "        mm_result = query_multimodal_rag(query, mm_vector_store)\n",
    "        \n",
    "        # Run text-only RAG\n",
    "        print(\"\\nRunning text-only RAG...\")\n",
    "        text_result = query_multimodal_rag(query, text_vector_store)\n",
    "        \n",
    "        # Compare responses\n",
    "        comparison = compare_responses(query, mm_result[\"response\"], text_result[\"response\"], reference)\n",
    "        \n",
    "        # Add to results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"multimodal_response\": mm_result[\"response\"],\n",
    "            \"textonly_response\": text_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison\n",
    "        })\n",
    "    \n",
    "    # Generate overall analysis\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis,\n",
    "        \"multimodal_doc_info\": mm_doc_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing responses with Gemini...\n",
      "\n",
      "Comparison Analysis:\n",
      "Both responses correctly identify Paris as the capital of France.  However, the **Text-only RAG response is slightly better** for this specific query. Here's why:\n",
      "\n",
      "* **Accuracy and Correctness:** Both are accurate.\n",
      "\n",
      "* **Completeness of Information:** Both provide the same core information.  The additional sentence about Paris being a major European city is relevant but not crucial to answering the question about the capital.\n",
      "\n",
      "* **Relevance to the Query:** Both are highly relevant.\n",
      "\n",
      "* **Unique Information from Visual Elements (Multi-modal):** The multi-modal response mentions a map showing Paris' location. While this is helpful context, it's not *necessary* to answer the question \"What is the capital of France?\".  The inclusion of the map is more of a tangential addition than a crucial piece of information directly answering the query.  In fact, the reference to a specific page (\"page 2\") implies a reliance on a particular document layout, making the response less robust and potentially less reproducible.  A better multi-modal response would have integrated the map information more seamlessly, perhaps by stating \"Paris is located in northern France, as shown on this map [link/image].\"\n",
      "\n",
      "In summary, for this simple, factual query, the extra information provided by the multi-modal system is not beneficial and even slightly detracts from the conciseness and directness of the answer.  The text-only response provides the core information efficiently and accurately.  The multi-modal system's advantage would be more apparent with queries requiring visual interpretation or where visual information is crucial to a complete answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import Optional\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main comparison function (revised for Gemini) ---\n",
    "def compare_responses(query: str, mm_response: str, text_response: str, reference: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Compare multi-modal and text-only responses using Gemini.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        mm_response (str): Multi-modal response\n",
    "        text_response (str): Text-only response\n",
    "        reference (str, optional): Reference answer\n",
    "\n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are an expert evaluator comparing two RAG systems:\n",
    "1. Multi-modal RAG: Retrieves from both text and image captions\n",
    "2. Text-only RAG: Retrieves only from text\n",
    "\n",
    "Evaluate which response better answers the query based on:\n",
    "- Accuracy and correctness\n",
    "- Completeness of information\n",
    "- Relevance to the query\n",
    "- Unique information from visual elements (for multi-modal)\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Multi-modal RAG Response:\n",
    "{mm_response}\n",
    "\n",
    "Text-only RAG Response:\n",
    "{text_response}\n",
    "\"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "Reference Answer:\n",
    "{reference}\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "Compare these responses and explain which one better answers the query and why.\n",
    "Note any specific information that came from images in the multi-modal response.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Pass the system prompt to the GenerativeModel's system_instruction parameter\n",
    "        gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate the comparison using the specified model\n",
    "        response = gemini_model.generate_content(user_prompt, generation_config={\"temperature\": 0.0})\n",
    "        \n",
    "        # Return the generated response content\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during comparison: {e}\")\n",
    "        return \"Comparison failed due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate responses and a query\n",
    "    query = \"What is the capital of France?\"\n",
    "    mm_response = \"Paris is the capital of France, which is also a major European city. A map on page 2 shows its location.\"\n",
    "    text_response = \"Paris is the capital of France, which is a major European city.\"\n",
    "\n",
    "    print(\"Comparing responses with Gemini...\")\n",
    "    analysis = compare_responses(query, mm_response, text_response)\n",
    "    \n",
    "    print(\"\\nComparison Analysis:\")\n",
    "    print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating overall analysis with Gemini...\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "## Multi-modal vs. Text-only RAG: A Comparative Analysis\n",
      "\n",
      "Based on the provided evaluations of two queries, a clear picture emerges regarding the strengths and weaknesses of multi-modal RAG compared to its text-only counterpart.  The analysis reveals that multi-modal RAG's effectiveness is highly dependent on the query's nature and the availability of relevant and informative visual data.\n",
      "\n",
      "**1. Types of Queries Where Multi-modal RAG Outperforms Text-only:**\n",
      "\n",
      "Multi-modal RAG significantly outperforms text-only RAG when the query involves:\n",
      "\n",
      "* **Data visualization:**  As demonstrated in Query 1, queries requiring the interpretation of data presented visually (charts, graphs, diagrams) benefit immensely from multi-modal retrieval.  Text alone might describe the data, but the visual representation provides immediate understanding and context, leading to a richer and more complete answer.  This is particularly true for complex datasets where visual summaries are crucial for comprehension.\n",
      "* **Spatial or visual information:** Queries focusing on geographical locations, object identification, or visual characteristics of events or phenomena are better addressed by multi-modal RAG.  For example, a query about the layout of a historical site or the damage caused by a natural disaster would benefit from image analysis.\n",
      "* **Queries requiring contextual understanding from images:**  Images can provide crucial context that text alone might miss. For instance, a query about a specific historical event might benefit from an image showing the event's setting or participants, enriching the textual information.\n",
      "\n",
      "**2. Specific Advantages of Incorporating Image Information:**\n",
      "\n",
      "* **Enhanced comprehension:** Images provide a quick and intuitive understanding of complex information, complementing and enhancing textual descriptions.\n",
      "* **Improved accuracy:** Visual data can corroborate or contradict textual information, leading to a more accurate and nuanced answer.\n",
      "* **Increased completeness:** Images can fill gaps in textual information, providing a more holistic understanding of the topic.\n",
      "* **Faster information processing:** Visual information is often processed faster than text, leading to quicker response generation.\n",
      "\n",
      "**3. Disadvantages and Limitations of the Multi-modal Approach:**\n",
      "\n",
      "* **Dependence on image quality and relevance:** The effectiveness of multi-modal RAG is heavily reliant on the quality and relevance of the retrieved images. Poor quality images or irrelevant images can hinder the system's performance and even lead to inaccurate or misleading answers.\n",
      "* **Increased complexity and computational cost:** Processing and analyzing images adds significant complexity and computational cost compared to text-only processing. This can lead to slower response times and higher infrastructure requirements.\n",
      "* **Bias and fairness concerns:**  Images can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes.  Careful consideration of bias mitigation strategies is crucial.\n",
      "* **Limited scalability:**  Building and maintaining a robust multi-modal RAG system requires significant resources and expertise, making it less scalable than text-only systems in certain contexts.\n",
      "* **Lack of universally applicable image understanding:** Current image processing techniques may struggle with nuanced visual interpretations or abstract concepts.\n",
      "\n",
      "\n",
      "**4. Overall Recommendation on When to Use Each Approach:**\n",
      "\n",
      "* **Use text-only RAG when:** The query focuses primarily on textual information, and no significant benefit is expected from incorporating visual data. This is often the case for queries involving abstract concepts, opinions, or detailed textual analysis.  Text-only RAG is generally simpler, faster, and more cost-effective.\n",
      "\n",
      "* **Use multi-modal RAG when:** The query involves data visualization, spatial information, or visual characteristics that are crucial for a complete and accurate answer.  The availability of high-quality, relevant images is essential for the success of this approach.  Consider the potential for bias and the increased computational cost.\n",
      "\n",
      "\n",
      "In conclusion, multi-modal RAG offers significant advantages over text-only RAG for specific types of queries, particularly those involving visual data. However, its effectiveness is highly dependent on the quality and relevance of the visual information, and its increased complexity and cost must be carefully weighed against the potential benefits.  A hybrid approach, where the system intelligently decides whether to incorporate visual information based on the query's nature, might be the most effective strategy in many scenarios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- 1. Gemini API Configuration ---\n",
    "# Your GOOGLE_API_KEY should be set in your environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. The main analysis function (revised for Gemini) ---\n",
    "def generate_overall_analysis(results: List[Dict], model: str = \"gemini-1.5-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of multi-modal vs text-only RAG using Gemini.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Evaluation results for each query\n",
    "        model (str): The model to be used for the analysis.\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Provide an overall analysis comparing \n",
    "multi-modal RAG (text + images) versus text-only RAG based on multiple test queries.\n",
    "\n",
    "Focus on:\n",
    "1. Types of queries where multi-modal RAG outperforms text-only\n",
    "2. Specific advantages of incorporating image information\n",
    "3. Any disadvantages or limitations of the multi-modal approach\n",
    "4. Overall recommendation on when to use each approach\"\"\"\n",
    "\n",
    "    # Create summary of evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Multi-modal retrieved {result['multimodal_results']['text_count']} text chunks and {result['multimodal_results']['image_count']} image captions\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt with evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of multi-modal vs text-only RAG across {len(results)} queries, \n",
    "provide an overall analysis comparing these two approaches:\n",
    "\n",
    "{evaluations_summary}\n",
    "\n",
    "Please provide a comprehensive analysis of the relative strengths and weaknesses of multi-modal RAG \n",
    "compared to text-only RAG, with specific attention to how image information contributed (or didn't contribute) to response quality.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Gemini model instance with the system prompt\n",
    "        gemini_model = genai.GenerativeModel(model, system_instruction=system_prompt)\n",
    "        \n",
    "        # Generate overall analysis\n",
    "        response = gemini_model.generate_content(user_prompt, generation_config={\"temperature\": 0.0})\n",
    "        \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis generation: {e}\")\n",
    "        return \"Analysis failed due to an error.\"\n",
    "\n",
    "# --- 3. Main Logic (Re-implemented for a runnable example) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate a results list from a previous evaluation pipeline\n",
    "    simulated_results = [\n",
    "        {\n",
    "            \"query\": \"What are the key differences in homelessness data collection methods?\",\n",
    "            \"multimodal_results\": {\"text_count\": 2, \"image_count\": 1},\n",
    "            \"comparison\": \"Multi-modal RAG was superior. It retrieved text on data collection methods and an image caption of a chart that visually represented the different methodologies, leading to a more complete answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Describe the main causes of homelessness.\",\n",
    "            \"multimodal_results\": {\"text_count\": 3, \"image_count\": 0},\n",
    "            \"comparison\": \"Both RAG systems performed similarly. Since no relevant images were retrieved, the multi-modal approach offered no additional value. Both systems provided a good text-based answer.\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"Generating overall analysis with Gemini...\")\n",
    "    overall_analysis = generate_overall_analysis(simulated_results)\n",
    "    \n",
    "    print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "    print(overall_analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Multi-Modal RAG vs Text-Only RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\n",
      "\n",
      "\n",
      "Processing document for multi-modal RAG...\n",
      "Extracting content from /Users/kekunkoya/Desktop/ISEM 770 Class Project/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Generating captions for 3 images...\n",
      "Processing image 1/3...\n",
      "Processing image 2/3...\n",
      "Processing image 3/3...\n",
      "Creating embeddings for all content...\n",
      "Added 59 items to vector store (56 text chunks, 3 image captions)\n",
      "\n",
      "Processing document for text-only RAG...\n",
      "Extracting content from /Users/kekunkoya/Desktop/ISEM 770 Class Project/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Creating embeddings for text-only content...\n",
      "Added 56 text items to text-only vector store\n",
      "\n",
      "\n",
      "=== Evaluating Query 1: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n",
      "Running multi-modal RAG...\n",
      "\n",
      "=== Processing query: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n",
      "Retrieved 5 relevant items (5 text, 0 image captions)\n",
      "\n",
      "Running text-only RAG...\n",
      "\n",
      "=== Processing query: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n",
      "Retrieved 5 relevant items (5 text, 0 image captions)\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "The provided evaluation data, consisting of only one query, is insufficient to draw robust conclusions about the comparative performance of multi-modal and text-only RAG systems.  A single query, especially one where both systems perform identically, offers no insight into the situations where multi-modal RAG might excel or falter.  To provide a comprehensive analysis, we need a diverse set of queries spanning various complexities and data types.\n",
      "\n",
      "However, based on general knowledge and the nature of multi-modal RAG, we can anticipate the following:\n",
      "\n",
      "**1. Types of Queries Where Multi-modal RAG Outperforms Text-Only:**\n",
      "\n",
      "* **Visually-rich queries:** Multi-modal RAG significantly outperforms text-only when the query involves visual information.  Examples include:\n",
      "    * Identifying objects or scenes in an image (\"What is this plant?\", \"Describe the architecture of this building\").\n",
      "    * Comparing visual features across different images (\"What are the differences between these two microscopic images?\").\n",
      "    * Answering questions about diagrams, charts, or graphs (\"What is the trend shown in this line graph?\").\n",
      "    * Queries related to image captions or alt text.\n",
      "* **Queries requiring contextual understanding from images:**  Even if the query is text-based, the presence of relevant images can provide crucial context that text alone might miss. For example, a query about a historical event might be better answered if accompanied by images from that period.\n",
      "* **Queries involving complex visual relationships:**  Understanding spatial relationships, object interactions, or visual patterns often requires image processing capabilities beyond the scope of text-only RAG.\n",
      "\n",
      "**2. Specific Advantages of Incorporating Image Information:**\n",
      "\n",
      "* **Enhanced context and understanding:** Images provide rich contextual information that can disambiguate queries or provide additional details not present in text.\n",
      "* **Improved accuracy and completeness:**  Visual information can fill gaps in textual data, leading to more accurate and comprehensive answers.\n",
      "* **Ability to handle non-textual data:** Multi-modal RAG can process and leverage information from various sources, including images, videos, and audio, expanding the scope of answerable queries.\n",
      "* **Improved user experience:**  Visual elements in the response can make the information more engaging and easier to understand.\n",
      "\n",
      "**3. Disadvantages and Limitations of the Multi-modal Approach:**\n",
      "\n",
      "* **Increased complexity:** Building and maintaining a multi-modal RAG system is significantly more complex than a text-only system. It requires specialized infrastructure for image processing, feature extraction, and multimodal fusion.\n",
      "* **Higher computational cost:** Processing images and integrating visual information adds to the computational burden, potentially increasing latency and resource consumption.\n",
      "* **Data bias and fairness concerns:**  Image data can be biased, reflecting societal biases and potentially leading to unfair or inaccurate responses.  Careful curation and bias mitigation strategies are crucial.\n",
      "* **Challenges in multimodal fusion:** Effectively combining textual and visual information requires sophisticated algorithms and careful consideration of how different modalities contribute to the overall understanding.\n",
      "* **Limited availability of multi-modal datasets:**  High-quality datasets containing both text and relevant images are not always readily available, hindering the development and evaluation of multi-modal RAG systems.\n",
      "\n",
      "\n",
      "**4. Overall Recommendation:**\n",
      "\n",
      "* **Use text-only RAG when:** The query is purely text-based, and relevant information is readily available in text format.  This approach is simpler, faster, and less resource-intensive.\n",
      "* **Use multi-modal RAG when:** The query involves visual information, requires contextual understanding from images, or benefits from the integration of multiple data modalities.  The potential benefits of improved accuracy and richer context outweigh the increased complexity and computational cost.\n",
      "\n",
      "\n",
      "In conclusion, the single query example provides no evidence to support a preference for either approach.  A thorough evaluation requires a much larger and more diverse set of queries to assess the strengths and weaknesses of each method under various conditions.  The choice between multi-modal and text-only RAG should be driven by the specific needs of the application and the nature of the data available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF document\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Define test queries targeting both text and visual content\n",
    "test_queries = [\n",
    "    \"What is the BLEU score of the Transformer (base model)?\",\n",
    "]\n",
    "\n",
    "# Optional reference answers for evaluation\n",
    "reference_answers = [\n",
    "    \"The Transformer (base model) achieves a BLEU score of 27.3 on the WMT 2014 English-to-German translation task and 38.1 on the WMT 2014 English-to-French translation task.\",\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
